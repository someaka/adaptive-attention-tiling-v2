============================= test session starts ==============================
platform linux -- Python 3.12.7, pytest-8.3.4, pluggy-1.5.0 -- /home/d/Desktop/adaptive-attention-tiling-v2/venv/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/home/d/Desktop/adaptive-attention-tiling-v2/.hypothesis/examples'))
benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/d/Desktop/adaptive-attention-tiling-v2
configfile: pytest.ini
testpaths: tests
plugins: hydra-core-1.3.2, hypothesis-6.122.3, benchmark-5.1.0
collecting ... collected 432 items

tests/core/attention/test_geometric.py::test_minkowski_inner_product PASSED
tests/core/attention/test_geometric.py::test_project_to_hyperboloid PASSED
tests/core/attention/test_geometric.py::test_exp_log_inverse 
Original vector v: tensor([[-0.0002,  0.0126],
        [-0.0110,  0.1460]])
Point after exp map y: tensor([[ 1.0000, -0.0022],
        [ 1.0025,  0.0703]])
Recovered vector v: tensor([[-1.6083e-05,  1.0869e-03],
        [-1.0906e-02,  1.4504e-01]])
Difference: tensor([[1.7088e-04, 1.1548e-02],
        [7.1747e-05, 9.5420e-04]])
FAILED
tests/core/attention/test_geometric.py::test_parallel_transport PASSED
tests/core/attention/test_geometric.py::test_geodesic_distance Forward distance: tensor([1.2758, 1.8989])
Reverse distance: tensor([1.2758, 1.8989])
Difference: tensor([0., 0.])
Identity distance: tensor([0., 0.])
PASSED
tests/core/tiling/test_strategies.py::test_random_seed_consistency PASSED
tests/core/tiling/test_strategies.py::test_device_support[cpu] PASSED
tests/core/tiling/test_strategies.py::test_dtype_support[dtype0] PASSED
tests/core/tiling/test_strategies.py::test_dtype_support[dtype1] PASSED
tests/core/tiling/test_strategies.py::test_memory_tracking PASSED
tests/core/tiling/test_strategies.py::test_performance_benchmarking FAILED
tests/core/tiling/test_strategies.py::test_attention_tile_initialization PASSED
tests/core/tiling/test_strategies.py::test_attention_tile_process PASSED
tests/core/tiling/test_strategies.py::test_attention_tile_adapt_resolution PASSED
tests/core/tiling/test_strategies.py::test_attention_tile_neighbors PASSED
tests/core/tiling/test_strategies.py::test_state_management FAILED
tests/core/tiling/test_strategies.py::test_advanced_resolution_adaptation PASSED
tests/core/tiling/test_strategies.py::test_information_flow PASSED
tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_initialization FAILED
tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_after_processing FAILED
tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_history FAILED
tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_with_resolution_change FAILED
tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_with_neighbors FAILED
tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_during_adaptation FAILED
tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_load_balancing_metrics PASSED
tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_stress_conditions PASSED
tests/performance/benchmarks/test_core.py::TestCoreOperations::test_attention_computation FAILED
tests/performance/benchmarks/test_core.py::TestCoreOperations::test_pattern_formation FAILED
tests/performance/benchmarks/test_core.py::TestCoreOperations::test_flow_evolution FAILED
tests/performance/benchmarks/test_core.py::TestCoreOperations::test_memory_patterns FAILED
tests/performance/benchmarks/test_core.py::TestCoreOperations::test_scaling_characteristics FAILED
tests/performance/benchmarks/test_vulkan.py::TestVulkanBenchmarks::test_pool_efficiency ERROR
tests/performance/benchmarks/test_vulkan.py::TestVulkanBenchmarks::test_pool_efficiency ERROR
tests/performance/benchmarks/test_vulkan.py::TestVulkanBenchmarks::test_memory_operations ERROR
tests/performance/benchmarks/test_vulkan.py::TestVulkanBenchmarks::test_memory_operations ERROR
tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.1-matrix_size0] FAILED
tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.1-matrix_size1] FAILED
tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.5-matrix_size0] FAILED
tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.5-matrix_size1] FAILED
tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.9-matrix_size0] FAILED
tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.9-matrix_size1] FAILED
tests/performance/cpu/test_algorithms.py::test_branch_prediction[matrix_size0-1] FAILED
tests/performance/cpu/test_algorithms.py::test_branch_prediction[matrix_size0-16] FAILED
tests/performance/cpu/test_algorithms.py::test_branch_prediction[matrix_size1-1] FAILED
tests/performance/cpu/test_algorithms.py::test_branch_prediction[matrix_size1-16] FAILED
tests/performance/cpu/test_algorithms.py::test_loop_optimization[O0] FAILED
tests/performance/cpu/test_algorithms.py::test_loop_optimization[O1] FAILED
tests/performance/cpu/test_algorithms.py::test_loop_optimization[O2] FAILED
tests/performance/cpu/test_algorithms.py::test_loop_optimization[O3] FAILED
tests/performance/cpu/test_algorithms.py::test_numerical_stability[matrix_size0] FAILED
tests/performance/cpu/test_algorithms.py::test_numerical_stability[matrix_size1] FAILED
tests/performance/cpu/test_algorithms.py::test_optimization_overhead FAILED
tests/performance/cpu/test_memory.py::test_memory_pool_efficiency[32-1024] FAILED
tests/performance/cpu/test_memory.py::test_memory_pool_efficiency[32-4096] FAILED
tests/performance/cpu/test_memory.py::test_memory_pool_efficiency[128-1024] FAILED
tests/performance/cpu/test_memory.py::test_memory_pool_efficiency[128-4096] FAILED
tests/performance/cpu/test_memory.py::test_allocation_pattern_impact[1024-sequential] PASSED
tests/performance/cpu/test_memory.py::test_allocation_pattern_impact[1024-random] PASSED
tests/performance/cpu/test_memory.py::test_allocation_pattern_impact[1024-interleaved] PASSED
tests/performance/cpu/test_memory.py::test_allocation_pattern_impact[4096-sequential] PASSED
tests/performance/cpu/test_memory.py::test_allocation_pattern_impact[4096-random] PASSED
tests/performance/cpu/test_memory.py::test_allocation_pattern_impact[4096-interleaved] PASSED
tests/performance/cpu/test_memory.py::test_cache_utilization[32] PASSED
tests/performance/cpu/test_memory.py::test_cache_utilization[256] PASSED
tests/performance/cpu/test_memory.py::test_memory_bandwidth PASSED
tests/performance/cpu/test_memory.py::test_resource_cleanup FAILED
tests/performance/cpu/test_memory_management.py::test_memory_allocation_deallocation PASSED
tests/performance/cpu/test_memory_management.py::test_memory_fragmentation PASSED
tests/performance/cpu/test_memory_management.py::test_memory_efficient_operations PASSED
tests/performance/cpu/test_memory_management.py::test_memory_peak_tracking PASSED
tests/performance/cpu/test_memory_management.py::test_memory_optimization_strategies PASSED
tests/performance/cpu/test_memory_management.py::test_memory_scaling[size0] PASSED
tests/performance/cpu/test_memory_management.py::test_memory_scaling[size1] PASSED
tests/performance/cpu/test_memory_management.py::test_memory_scaling[size2] PASSED
tests/performance/cpu/test_vectorization.py::test_attention_vectorization_performance[32-32] Attention Performance (batch=32, dim=32):
  Execution time: 0.00ms
  Memory usage: 0.38MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_attention_vectorization_performance[32-128] Attention Performance (batch=128, dim=32):
  Execution time: 0.00ms
  Memory usage: 1.50MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_attention_vectorization_performance[64-32] Attention Performance (batch=32, dim=64):
  Execution time: 0.01ms
  Memory usage: 1.50MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_attention_vectorization_performance[64-128] Attention Performance (batch=128, dim=64):
  Execution time: 0.01ms
  Memory usage: 6.00MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_pattern_dynamics_vectorization[32-32] Pattern Dynamics (batch=32, seq_len=32):
  Execution time: 0.00ms
  Memory usage: 0.13MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_pattern_dynamics_vectorization[32-128] Pattern Dynamics (batch=128, seq_len=32):
  Execution time: 0.00ms
  Memory usage: 0.52MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_pattern_dynamics_vectorization[64-32] Pattern Dynamics (batch=32, seq_len=64):
  Execution time: 0.00ms
  Memory usage: 0.51MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_pattern_dynamics_vectorization[64-128] Pattern Dynamics (batch=128, seq_len=64):
  Execution time: 0.01ms
  Memory usage: 2.03MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_geometric_flow_vectorization[32-32] Geometric Flow (batch=32, dim=32):
  Execution time: 0.01ms
  Memory usage: 4.12MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_geometric_flow_vectorization[32-128] Geometric Flow (batch=128, dim=32):
  Execution time: 0.06ms
  Memory usage: 16.50MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_geometric_flow_vectorization[64-32] Geometric Flow (batch=32, dim=64):
  Execution time: 0.11ms
  Memory usage: 32.50MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_geometric_flow_vectorization[64-128] Geometric Flow (batch=128, dim=64):
  Execution time: 0.28ms
  Memory usage: 130.00MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_chunk_size_impact[32] Chunk Size Impact (size=32):
  Execution time: 13.54ms
  Memory usage: 3.00MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_chunk_size_impact[64] Chunk Size Impact (size=64):
  Execution time: 1.56ms
  Memory usage: 3.00MB
  Vectorization efficiency: 100.00%
PASSED
tests/performance/cpu/test_vectorization.py::test_memory_layout_optimization Memory Layout Optimization Results:
  Contiguous tensors:
    Execution time: 0.01ms
    Memory usage: 3.00MB
    Vectorization efficiency: 100.00%
  Non-contiguous tensors:
    Execution time: 0.01ms
    Memory usage: 3.00MB
    Vectorization efficiency: 100.00%
PASSED
tests/performance/vulkan/gpu/test_memory_management.py::test_tensor_allocation ERROR
tests/performance/vulkan/gpu/test_memory_management.py::test_data_transfer ERROR
tests/performance/vulkan/gpu/test_memory_management.py::test_memory_tracking ERROR
tests/performance/vulkan/gpu/test_memory_management.py::test_error_handling ERROR
tests/performance/vulkan/gpu/test_memory_management.py::test_buffer_pool_cleanup ERROR
tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size0-pattern] ERROR
tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size0-flow] ERROR
tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size0-attention] ERROR
tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size1-pattern] ERROR
tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size1-flow] ERROR
tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size1-attention] ERROR
tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size2-pattern] ERROR
tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size2-flow] ERROR
tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size2-attention] ERROR
tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size0-workgroup_size0] ERROR
tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size0-workgroup_size1] ERROR
tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size0-workgroup_size2] ERROR
tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size1-workgroup_size0] ERROR
tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size1-workgroup_size1] ERROR
tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size1-workgroup_size2] ERROR
tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size2-workgroup_size0] ERROR
tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size2-workgroup_size1] ERROR
tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size2-workgroup_size2] ERROR
tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size0-1] ERROR
tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size0-8] ERROR
tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size0-32] ERROR
tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size1-1] ERROR
tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size1-8] ERROR
tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size1-32] ERROR
tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size2-1] ERROR
tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size2-8] ERROR
tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size2-32] ERROR
tests/performance/vulkan/test_compute.py::test_resource_management[pattern] ERROR
tests/performance/vulkan/test_compute.py::test_resource_management[flow] ERROR
tests/performance/vulkan/test_compute.py::test_resource_management[attention] ERROR
tests/performance/vulkan/test_compute.py::test_descriptor_set_optimization ERROR
tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_pattern_evolution_performance ERROR
tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_flow_computation_performance ERROR
tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_workgroup_impact ERROR
tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_push_constant_performance ERROR
tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_batch_processing_efficiency ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_fence_performance ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_fence_performance ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_semaphore_performance ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_semaphore_performance ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_event_performance ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_event_performance ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_barrier_overhead ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_barrier_overhead ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_queue_sync ERROR
tests/performance/vulkan/test_sync.py::TestVulkanSync::test_queue_sync ERROR
tests/test_core/test_crystal/test_refraction.py::test_placeholder PASSED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_scale_connection FAILED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_renormalization_flow FAILED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_fixed_points FAILED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_anomaly_polynomial FAILED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_scale_invariants FAILED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_callan_symanzik FAILED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_operator_expansion FAILED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_conformal_symmetry FAILED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_holographic_scaling FAILED
tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_entanglement_scaling FAILED
tests/test_core/test_patterns/test_cohomology.py::test_arithmetic_form_creation PASSED
tests/test_core/test_patterns/test_cohomology.py::test_height_computation PASSED
tests/test_core/test_patterns/test_cohomology.py::test_cohomology_class_computation PASSED
tests/test_core/test_patterns/test_cohomology.py::test_curvature_to_cohomology PASSED
tests/test_core/test_patterns/test_cohomology.py::test_height_theory_integration PASSED
tests/test_core/test_patterns/test_cohomology.py::test_boundary_cases PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_bundle_projection[base_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_bundle_projection[pattern_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_local_trivialization[base_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_local_trivialization[pattern_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_transition_functions[base_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_transition_functions[pattern_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_connection_form[base_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_connection_form[pattern_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_parallel_transport[base_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestFiberBundleProtocol::test_parallel_transport[pattern_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestBaseFiberBundle::test_holonomy_computation PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestBaseFiberBundle::test_holonomy_algebra PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_device_handling PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_parameter_gradients PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_batch_operations PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_connection_form_components PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_parallel_transport_components PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_holonomy_properties PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::TestConnectionFormProperties::test_base_direction_independence PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::TestConnectionFormProperties::test_levi_civita_symmetry PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::TestConnectionFormProperties::test_fiber_skew_symmetry_preservation PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_connection_vertical_preservation ERROR
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_connection_horizontal_projection ERROR
tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_connection_levi_civita_compatibility ERROR
tests/test_core/test_patterns/test_fiber_bundle.py::TestConnectionFormHypothesis::test_vertical_preservation_property PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestConnectionFormHypothesis::test_horizontal_skew_symmetry_property PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestConnectionFormHypothesis::test_linearity_property FAILED
tests/test_core/test_patterns/test_fiber_bundle.py::TestConnectionFormHypothesis::test_levi_civita_symmetry_property FAILED
tests/test_core/test_patterns/test_fiber_bundle.py::TestConnectionFormHypothesis::test_metric_compatibility_property PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestConnectionFormValidation::test_connection_form_validation[base_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestConnectionFormValidation::test_connection_form_validation[pattern_bundle] PASSED
tests/test_core/test_patterns/test_fiber_bundle.py::TestGeometricComponents::test_metric_derivatives 
Initial metric:
tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  2.2921,  3.4032,  0.5606],
        [ 0.0000,  0.0000,  3.4032,  8.7199, -0.0236],
        [ 0.0000,  0.0000,  0.5606, -0.0236,  0.5526]],
       grad_fn=<SelectBackward0>)

Derivatives for i=0:
Original:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]], grad_fn=<SelectBackward0>)
Transposed:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]], grad_fn=<TransposeBackward0>)
Difference:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)

Derivatives for i=1:
Original:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]], grad_fn=<SelectBackward0>)
Transposed:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]], grad_fn=<TransposeBackward0>)
Difference:
tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]], grad_fn=<SubBackward0>)

Derivatives for i=2:
Original:
tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.1667,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.1755, -0.0833,  0.0000],
        [ 0.0000,  0.0000,  0.0188,  0.0000, -0.0833]],
       grad_fn=<SelectBackward0>)
Transposed:
tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.1667,  0.1755,  0.0188],
        [ 0.0000,  0.0000,  0.0000, -0.0833,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0833]],
       grad_fn=<TransposeBackward0>)
Difference:
tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000, -0.1755, -0.0188],
        [ 0.0000,  0.0000,  0.1755,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0188,  0.0000,  0.0000]], grad_fn=<SubBackward0>)
FAILED
tests/test_core/test_patterns/test_riemannian.py::test_metric_properties PASSED
tests/test_core/test_patterns/test_riemannian.py::test_christoffel_properties FAILED
tests/test_core/test_patterns/test_riemannian.py::test_parallel_transport FAILED
tests/test_core/test_patterns/test_riemannian.py::test_curvature_identities FAILED
tests/test_core/test_patterns/test_riemannian.py::test_geodesic_equation FAILED
tests/test_core/test_patterns/test_riemannian.py::test_sectional_curvature FAILED
tests/test_core/test_patterns/test_riemannian.py::test_lie_derivative FAILED
tests/test_core/test_quantum/test_path_integral.py::test_placeholder PASSED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_state_preparation PASSED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_state_evolution PASSED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_measurement PASSED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_entropy_computation PASSED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_geometric_structure PASSED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_entanglement PASSED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_quantum_channels PASSED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_state_tomography FAILED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_decoherence PASSED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_geometric_phase 
Berry phase: 0.0
Expected phase: 3.141592653589793
Relative error: 1.0
FAILED
tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_advanced_entanglement PASSED
tests/test_infrastructure/test_infrastructure.py::TestInfrastructure::test_cpu_optimization PASSED
tests/test_infrastructure/test_infrastructure.py::TestInfrastructure::test_memory_management PASSED
tests/test_infrastructure/test_infrastructure.py::TestInfrastructure::test_vulkan_integration FAILED
tests/test_infrastructure/test_infrastructure.py::TestInfrastructure::test_parallel_processing PASSED
tests/test_infrastructure/test_infrastructure.py::TestInfrastructure::test_resource_allocation FAILED
tests/test_infrastructure/test_infrastructure.py::TestInfrastructure::test_infrastructure_integration FAILED
tests/test_integration/test_cross_validation.py::TestCrossValidation::test_pattern_quantum_interaction FAILED
tests/test_integration/test_cross_validation.py::TestCrossValidation::test_geometric_pattern_coupling Warning: Error checking curvature for layer input: 'ModelGeometry' object has no attribute 'sectional_curvature'
Warning: Error checking curvature for layer hidden: 'ModelGeometry' object has no attribute 'sectional_curvature'
Warning: Error checking curvature for layer output: 'ModelGeometry' object has no attribute 'sectional_curvature'
Warning: Error checking curvature for layer input: 'ModelGeometry' object has no attribute 'sectional_curvature'
Warning: Error checking curvature for layer input: 'ModelGeometry' object has no attribute 'sectional_curvature'
Warning: Error checking global energy: 'ModelGeometry' object has no attribute 'metric'
FAILED
tests/test_integration/test_cross_validation.py::TestCrossValidation::test_infrastructure_framework FAILED
tests/test_integration/test_cross_validation.py::TestCrossValidation::test_end_to_end_validation FAILED
tests/test_integration/test_cross_validation.py::TestCrossValidation::test_validation_stability FAILED
tests/test_memory/test_memory_management.py::test_tensor_lifecycle PASSED
tests/test_memory/test_memory_management.py::test_operation_cleanup PASSED
tests/test_memory/test_memory_management.py::test_hyperbolic_operations PASSED
tests/test_memory/test_memory_management.py::test_nested_operations PASSED
tests/test_memory/test_memory_management.py::test_memory_stress PASSED
tests/test_neural/test_attention/test_exponential.py::test_exponential_map FAILED
tests/test_neural/test_attention/test_exponential.py::test_exponential_map_properties 
Initial point x: tensor([1.1180, 0.5000, 0.0000])
x on hyperboloid: -1.0
Projected v: tensor([0.0056, 0.0125, 0.0100])
v in tangent space: 0.0

v1 norm: 0.014999998733401299
v2 norm: 0.029999997466802597

result1: tensor([1.1238, 0.5126, 0.0100])
result2: tensor([1.1297, 0.5252, 0.0200])

inner1: 1.000112533569336
inner2: 1.0004500150680542
dist1: 0.015002097003161907
dist2: 0.029999377205967903
2*dist1: 0.030004194006323814
diff: 4.816800355911255e-06
PASSED
tests/test_neural/test_attention/test_geometric.py::TestGeometricStructures::test_metric_initialization 
[Before config load]

[After config load]

[Before creating geometric structures]

[After creating geometric structures]

[Start metric test]

Metric tensor shape: torch.Size([16, 16])
Metric tensor dtype: torch.float32

[End metric test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestGeometricStructures::test_connection_initialization 
[Before creating geometric structures]

[After creating geometric structures]

[Start connection test]

Connection tensor shape: torch.Size([16, 16, 16])
Connection tensor dtype: torch.float32

[End connection test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestGeometricStructures::test_curvature_tensor 
[Before creating geometric structures]

[After creating geometric structures]

[Start curvature test]

Initial metric shape: torch.Size([16, 16])

[End curvature test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestGeometricStructures::test_sectional_curvature 
[Before creating geometric structures]

[After creating geometric structures]

[Start sectional test]

[End sectional test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestGeometricStructures::test_geodesic_distance 
[Before creating geometric structures]

[After creating geometric structures]

[Start geodesic test]

[End geodesic test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestHyperbolicOperations::test_hyperbolic_distance_formula 
[Start hyperbolic distance test]

[End hyperbolic distance test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestHyperbolicOperations::test_exp_map_properties 
[Start exp map test]

[End exp map test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestHyperbolicOperations::test_log_map_properties 
[Start log map test]
Distance: 0.029120, Vector norm: 0.029120, Diff: 0.000000
Orthogonality: -0.000000

[End log map test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestHyperbolicOperations::test_exp_log_inverse 
[Start exp-log inverse test]

Testing with scale 0.001:
Base point x: tensor([ 1.0253, -0.0473, -0.0598,  0.2132])
Initial vector v: tensor([ 8.7439e-05,  1.7649e-04, -7.1470e-05,  4.3958e-04])
Exp map result y: tensor([ 1.0254, -0.0471, -0.0599,  0.2137])
Recovered vector v: tensor([ 8.9126e-05,  1.7988e-04, -7.2849e-05,  4.4806e-04])
Difference: 9.389864317199681e-06

Testing with scale 0.01:
Base point x: tensor([ 1.0262, -0.0860,  0.2135, -0.0117])
Initial vector v: tensor([-0.0022,  0.0027, -0.0087,  0.0148])
Exp map result y: tensor([ 1.0242, -0.0833,  0.2049,  0.0030])
Recovered vector v: tensor([-0.0022,  0.0027, -0.0086,  0.0146])
Difference: 0.00014150100469123572

[End exp-log inverse test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestHyperbolicOperations::test_exp_log_consistency 
[Start exp-log consistency test]

Testing with norm 0.001:
Base point x: tensor([ 1.0103, -0.1069, -0.0743, -0.0616])
Initial vector v: tensor([ 2.7094e-08, -5.2619e-04,  8.4378e-04, -1.0563e-04])
Exp map result y: tensor([ 1.0103, -0.1074, -0.0735, -0.0618])
Recovered vector v: tensor([ 2.3403e-08, -4.4502e-04,  7.1361e-04, -8.9336e-05])
Original norm: 0.0010000000474974513
Recovered norm: 0.0008457278599962592
Difference: 0.0001542721875011921

Testing with norm 0.01:
Base point x: tensor([ 1.0216, -0.1866, -0.0805, -0.0493])
Initial vector v: tensor([-0.0003,  0.0027,  0.0031, -0.0091])
Exp map result y: tensor([ 1.0214, -0.1839, -0.0774, -0.0584])
Recovered vector v: tensor([-0.0003,  0.0027,  0.0031, -0.0091])
Original norm: 0.009999998845160007
Recovered norm: 0.009994826279580593
Difference: 5.172565579414368e-06

[End exp-log consistency test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestEuclideanOperations::test_exp_log_inverse 
[Start Euclidean exp-log test]

[End Euclidean exp-log test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestEuclideanOperations::test_exp_zero_vector 
[Start Euclidean zero vector test]

[End Euclidean zero vector test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestEuclideanOperations::test_log_same_point 
[Start Euclidean same point test]

[End Euclidean same point test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestParallelTransport::test_schild_ladder 
[Start Schild test]

[End Schild test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestParallelTransport::test_pole_ladder 
[Start pole test]

[End pole test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestParallelTransport::test_transport_zero_vector 
[Start zero vector transport test]

[End zero vector transport test]
PASSED
tests/test_neural/test_attention/test_geometric.py::TestParallelTransport::test_transport_same_point 
[Start same point transport test]

[End same point transport test]
PASSED
tests/test_neural/test_attention/test_hyperboloid.py::test_project_to_hyperboloid PASSED
tests/test_neural/test_attention/test_hyperboloid.py::test_hyperboloid_projection_properties PASSED
tests/test_neural/test_attention/test_logarithm.py::test_logarithm_map FAILED
tests/test_neural/test_attention/test_logarithm.py::test_logarithm_map_properties FAILED
tests/test_neural/test_attention/test_logarithm.py::test_exp_log_inverse FAILED
tests/test_neural/test_attention/test_minkowski.py::test_minkowski_inner_product PASSED
tests/test_neural/test_attention/test_minkowski.py::test_minkowski_inner_properties PASSED
tests/test_neural/test_attention/test_minkowski.py::test_minkowski_inner_fundamental_properties PASSED
tests/test_neural/test_attention/test_minkowski.py::test_advanced_minkowski_properties FAILED
tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_bifurcation_analysis FAILED
tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_bifurcation_detection_threshold FAILED
tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_stability_regions PASSED
tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_solution_branches FAILED
tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_stability_computation ERROR
tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_state_evolution ERROR
tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_bifurcation_detection_components ERROR
tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_convergence_at_bifurcation ERROR
tests/test_neural/test_attention/test_pattern/test_control.py::test_pattern_control FAILED
tests/test_neural/test_attention/test_pattern/test_control.py::test_spatiotemporal_evolution FAILED
tests/test_neural/test_attention/test_pattern/test_diffusion_properties.py::test_mass_conservation PASSED
tests/test_neural/test_attention/test_pattern/test_diffusion_properties.py::test_positivity_preservation PASSED
tests/test_neural/test_attention/test_pattern/test_diffusion_properties.py::test_maximum_principle PASSED
tests/test_neural/test_attention/test_pattern/test_diffusion_properties.py::test_symmetry_preservation PASSED
tests/test_neural/test_attention/test_pattern/test_diffusion_properties.py::test_convergence_to_steady_state FAILED
tests/test_neural/test_attention/test_pattern/test_quantum.py::TestQuantumPatterns::test_quantum_state_conversion PASSED
tests/test_neural/test_attention/test_pattern/test_quantum.py::TestQuantumPatterns::test_quantum_evolution FAILED
tests/test_neural/test_attention/test_pattern/test_quantum.py::TestQuantumPatterns::test_quantum_geometric_tensor PASSED
tests/test_neural/test_attention/test_pattern/test_quantum.py::TestQuantumPatterns::test_berry_phase PASSED
tests/test_neural/test_attention/test_pattern/test_quantum.py::TestQuantumPatterns::test_quantum_potential FAILED
tests/test_neural/test_attention/test_pattern/test_quantum.py::TestQuantumPatterns::test_quantum_disabled PASSED
tests/test_neural/test_attention/test_pattern/test_quantum.py::TestQuantumPatterns::test_parallel_transport PASSED
tests/test_neural/test_attention/test_pattern/test_reaction_diffusion.py::test_reaction_diffusion FAILED
tests/test_neural/test_attention/test_pattern/test_stability.py::test_stability_analysis FAILED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_stability_analysis_basic FAILED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_stability_analysis_advanced FAILED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_pattern_formation FAILED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_forward_pass FAILED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_pattern_control PASSED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_lyapunov_spectrum PASSED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_detect_pattern_formation PASSED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestReactionDiffusion::test_reaction_term PASSED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestReactionDiffusion::test_diffusion_term PASSED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestReactionDiffusion::test_combined_dynamics PASSED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestDiffusionProperties::test_diffusion_conservation PASSED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestDiffusionProperties::test_diffusion_smoothing PASSED
tests/test_neural/test_attention/test_pattern_dynamics.py::TestDiffusionProperties::test_boundary_conditions PASSED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_attention_state_preparation FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_attention_pattern_computation FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_geometric_attention_flow FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_quantum_classical_interface FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_multi_head_integration FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_geometric_phases FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_manifold_curvature FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_attention_entanglement FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_error_correction FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_topological_features FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_attention_patterns FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_geometric_structures FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_pattern_dynamics FAILED
tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_controllability PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_metric_computation PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_ricci_tensor PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_flow_computation PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_flow_step PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_flow_normalization PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_singularity_detection PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_geometric_invariants FAILED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_energy_conservation PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_flow_stability PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_flow_convergence FAILED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_metric_conditioning PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_flow_magnitude PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_ricci_flow_stability PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_ricci_flow PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_mean_curvature_flow PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_singularity_analysis PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestFlowStability::test_metric_conditioning PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestFlowStability::test_flow_magnitude PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestFlowStability::test_volume_preservation PASSED
tests/test_neural/test_flow/test_geometric_flow.py::TestFlowStability::test_ricci_flow_stability PASSED
tests/test_neural/test_flow/test_hamiltonian.py::TestHamiltonianSystem::test_hamiltonian_computation FAILED
tests/test_neural/test_flow/test_hamiltonian.py::TestHamiltonianSystem::test_evolution FAILED
tests/test_neural/test_flow/test_hamiltonian.py::TestHamiltonianSystem::test_canonical_transformations FAILED
tests/test_neural/test_flow/test_hamiltonian.py::TestHamiltonianSystem::test_symplectic_integration FAILED
tests/test_neural/test_flow/test_tensor_shapes.py::TestTensorShapes::test_geometric_flow_shapes PASSED
tests/test_neural/test_flow/test_tensor_shapes.py::TestTensorShapes::test_hamiltonian_shapes PASSED
tests/test_neural/test_flow/test_tensor_shapes.py::TestTensorShapes::test_hamiltonian_evolution_shapes PASSED
tests/test_neural/test_flow/test_tensor_shapes.py::TestTensorShapes::test_validation_shapes FAILED
tests/test_neural/test_flow/test_tensor_shapes.py::TestTensorShapes::test_convergence_shapes FAILED
tests/test_neural/test_flow/test_tensor_shapes.py::TestTensorShapes::test_energy_validation_shapes PASSED
tests/test_utils/test_helpers.py::test_tensor_assertions FAILED
tests/test_utils/test_helpers.py::test_numerical_stability FAILED
tests/test_utils/test_helpers.py::test_performance_benchmark test_operation: 0.0001 seconds
FAILED
tests/test_utils/test_helpers.py::test_data_generator PASSED
tests/test_validation/test_debug_flow.py::TestStabilityValidation::test_validate_stability_return_type[is_valid] FAILED
tests/test_validation/test_debug_flow.py::TestStabilityValidation::test_validate_stability_return_type[message] FAILED
tests/test_validation/test_debug_flow.py::TestStabilityValidation::test_validate_stability_return_type[data] FAILED
tests/test_validation/test_debug_flow.py::TestStabilityValidation::test_stability_spectrum_computation FAILED
tests/test_validation/test_debug_flow.py::TestEnergyValidation::test_energy_computation[check0] FAILED
tests/test_validation/test_debug_flow.py::TestEnergyValidation::test_energy_computation[check1] FAILED
tests/test_validation/test_debug_flow.py::TestEnergyValidation::test_energy_computation[check2] FAILED
tests/test_validation/test_debug_flow.py::TestEnergyValidation::test_energy_conservation_validation FAILED
tests/test_validation/test_debug_flow.py::TestFlowProperties::test_flow_properties_construction FAILED
tests/test_validation/test_debug_flow.py::TestFlowProperties::test_flow_properties_with_energy FAILED
tests/test_validation/test_flow_validation.py::TestFlowValidation::test_energy_conservation FAILED
tests/test_validation/test_flow_validation.py::TestFlowValidation::test_flow_monotonicity FAILED
tests/test_validation/test_flow_validation.py::TestFlowValidation::test_long_time_existence FAILED
tests/test_validation/test_flow_validation.py::TestFlowValidation::test_singularity_detection FAILED
tests/test_validation/test_flow_validation.py::TestFlowValidation::test_validation_integration FAILED
tests/test_validation/test_framework.py::TestValidationFramework::test_geometric_validation FAILED
tests/test_validation/test_framework.py::TestValidationFramework::test_quantum_validation FAILED
tests/test_validation/test_framework.py::TestValidationFramework::test_pattern_validation FAILED
tests/test_validation/test_framework.py::TestValidationFramework::test_integrated_validation FAILED
tests/test_validation/test_framework.py::TestValidationFramework::test_error_handling FAILED
tests/test_validation/test_framework.py::TestValidationFramework::test_validation_metrics FAILED
tests/test_validation/test_framework.py::TestValidationFramework::test_full_integration FAILED
tests/test_validation/test_framework.py::TestValidationFramework::test_validate_all FAILED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_framework_with_motivic PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_framework_merge PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_framework_to_dict PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_framework_from_dict PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_framework_str_representation PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_invalid_motivic_result PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_framework_with_invalid_motivic PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_framework_without_motivic PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_partial_motivic_validation[True-True-False] PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_partial_motivic_validation[True-False-True] PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_partial_motivic_validation[False-True-True] PASSED
tests/test_validation/test_framework_motivic.py::TestFrameworkMotivicIntegration::test_partial_motivic_validation[False-False-False] PASSED
tests/test_validation/test_metric_validation.py::TestMetricValidation::test_positive_definite PASSED
tests/test_validation/test_metric_validation.py::TestMetricValidation::test_compatibility PASSED
tests/test_validation/test_metric_validation.py::TestMetricValidation::test_fisher_rao_metric FAILED
tests/test_validation/test_metric_validation.py::TestMetricValidation::test_curvature_validation FAILED
tests/test_validation/test_metric_validation.py::TestMetricValidation::test_metric_family_validation PASSED
tests/test_validation/test_metric_validation.py::TestMetricValidation::test_metric_properties FAILED
tests/test_validation/test_metric_validation.py::TestMetricValidation::test_error_handling PASSED
tests/test_validation/test_model_geometric.py::TestModelGeometricValidator::test_validate_layer_geometry FAILED
tests/test_validation/test_model_geometric.py::TestModelGeometricValidator::test_validate_attention_geometry 
Distance Statistics:
Query distances:
FAILED
tests/test_validation/test_model_geometric.py::TestModelGeometricValidator::test_geometric_preservation 
Base Metric Properties:
Shape: torch.Size([16, 16])
Symmetric: True
Eigenvalue range: [0.010000, 0.010000]

Attention Scores Properties:
Shape: torch.Size([16, 16])
Range: [0.061361, 0.064082]
Row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       grad_fn=<SumBackward1>)


Distance Statistics:
Query distances:
  Range: [0.000000, 1.000000]
  Mean: 0.627569
  Std: 0.215219

Key distances:
  Range: [0.000000, 1.000000]
  Mean: 0.627569
  Std: 0.215219

Score distances:
  Range: [0.000000, 0.999687]
  Mean: 0.373608
  Std: 0.210234
FAILED
tests/test_validation/test_model_geometric.py::TestModelGeometricValidator::test_geometric_preservation_distances PASSED
tests/test_validation/test_motivic.py::TestHeightValidation::test_basic_height_properties PASSED
tests/test_validation/test_motivic.py::TestHeightValidation::test_strictly_increasing_heights PASSED
tests/test_validation/test_motivic.py::TestHeightValidation::test_height_edge_cases PASSED
tests/test_validation/test_motivic.py::TestHeightValidation::test_invalid_height_cases PASSED
tests/test_validation/test_motivic.py::TestDynamicsValidation::test_basic_dynamics_properties PASSED
tests/test_validation/test_motivic.py::TestDynamicsValidation::test_dynamics_consistency PASSED
tests/test_validation/test_motivic.py::TestDynamicsValidation::test_dynamics_edge_cases PASSED
tests/test_validation/test_motivic.py::TestDynamicsValidation::test_invalid_dynamics_cases PASSED
tests/test_validation/test_motivic.py::TestCohomologyValidation::test_basic_cohomology_properties PASSED
tests/test_validation/test_motivic.py::TestCohomologyValidation::test_curvature_bounds PASSED
tests/test_validation/test_motivic.py::TestCohomologyValidation::test_cohomology_edge_cases PASSED
tests/test_validation/test_motivic.py::TestCohomologyValidation::test_invalid_cohomology_cases PASSED
tests/test_validation/test_motivic.py::TestMotivicRiemannianValidator::test_full_validation PASSED
tests/test_validation/test_motivic.py::TestMotivicRiemannianValidator::test_validation_with_perturbation PASSED
tests/test_validation/test_motivic.py::TestMotivicRiemannianValidator::test_boundary_cases PASSED
tests/test_validation/test_motivic.py::TestMotivicRiemannianValidator::test_different_dimensions[2-3] PASSED
tests/test_validation/test_motivic.py::TestMotivicRiemannianValidator::test_different_dimensions[4-6] PASSED
tests/test_validation/test_motivic.py::TestMotivicRiemannianValidator::test_different_dimensions[5-8] PASSED
tests/test_validation/test_pattern_flow.py::test_pattern_creation PASSED
tests/test_validation/test_pattern_flow.py::test_complex_pattern PASSED
tests/test_validation/test_pattern_flow.py::test_complex_pattern_creation PASSED
tests/test_validation/test_pattern_flow.py::test_pattern_wavelength_scaling PASSED
tests/test_validation/test_pattern_flow.py::test_frequency_grid PASSED
tests/test_validation/test_pattern_flow.py::test_power_spectrum PASSED
tests/test_validation/test_pattern_flow.py::test_wavelength_computation PASSED
tests/test_validation/test_pattern_flow.py::test_wavelength_batch PASSED
tests/test_validation/test_pattern_flow.py::test_wavelength_noise PASSED
tests/test_validation/test_pattern_flow.py::test_wavelength_high_frequency PASSED
tests/test_validation/test_pattern_flow.py::test_wavelength_low_frequency PASSED
tests/test_validation/test_pattern_flow.py::test_wavelength_computation_diagnostic PASSED
tests/test_validation/test_pattern_flow.py::test_pattern_flow_stability FAILED
tests/test_validation/test_pattern_flow.py::test_pattern_flow_energy FAILED
tests/test_validation/test_pattern_flow.py::test_wavelength_diagnostic PASSED
tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_pattern_emergence FAILED
tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_spatial_organization FAILED
tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_temporal_evolution PASSED
tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_bifurcation_analysis PASSED
tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_mode_decomposition PASSED
tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_validation_integration FAILED
tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_reaction_diffusion FAILED
tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_symmetry_breaking FAILED
tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_pattern_stability FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_linear_stability FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_nonlinear_stability FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_perturbation_response FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_lyapunov_analysis FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_mode_stability FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_stability_metrics FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_validation_integration FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_dynamical_system FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_bifurcation_theory FAILED
tests/test_validation/test_pattern_stability.py::TestPatternStability::test_stability_analysis FAILED
tests/test_validation/test_state_validation.py::TestStateValidation::test_state_preparation FAILED
tests/test_validation/test_state_validation.py::TestStateValidation::test_density_matrix_properties FAILED
tests/test_validation/test_state_validation.py::TestStateValidation::test_state_tomography FAILED
tests/test_validation/test_state_validation.py::TestStateValidation::test_validation_integration FAILED

==================================== ERRORS ====================================
_________ ERROR at setup of TestVulkanBenchmarks.test_pool_efficiency __________

self = <test_vulkan.TestVulkanBenchmarks object at 0x705544129880>

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup test environment."""
        # Create instance
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName=b"VulkanBenchmarks",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName=b"No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        self.instance = c_void_p()
>       result = vk.vkCreateInstance(byref(create_info), None, byref(self.instance))
E       TypeError: byref() argument must be a ctypes instance, not '_cffi_backend.__CDataOwn'

tests/performance/benchmarks/test_vulkan.py:40: TypeError
________ ERROR at teardown of TestVulkanBenchmarks.test_pool_efficiency ________

self = <test_vulkan.TestVulkanBenchmarks object at 0x705544129880>

    def teardown_method(self):
        """Cleanup after each test method."""
>       self.memory.cleanup()
E       AttributeError: 'TestVulkanBenchmarks' object has no attribute 'memory'

tests/performance/benchmarks/test_vulkan.py:193: AttributeError
________ ERROR at setup of TestVulkanBenchmarks.test_memory_operations _________

self = <test_vulkan.TestVulkanBenchmarks object at 0x705544129a60>

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup test environment."""
        # Create instance
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName=b"VulkanBenchmarks",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName=b"No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        self.instance = c_void_p()
>       result = vk.vkCreateInstance(byref(create_info), None, byref(self.instance))
E       TypeError: byref() argument must be a ctypes instance, not '_cffi_backend.__CDataOwn'

tests/performance/benchmarks/test_vulkan.py:40: TypeError
_______ ERROR at teardown of TestVulkanBenchmarks.test_memory_operations _______

self = <test_vulkan.TestVulkanBenchmarks object at 0x705544129a60>

    def teardown_method(self):
        """Cleanup after each test method."""
>       self.memory.cleanup()
E       AttributeError: 'TestVulkanBenchmarks' object has no attribute 'memory'

tests/performance/benchmarks/test_vulkan.py:193: AttributeError
___________________ ERROR at setup of test_tensor_allocation ___________________

request = <SubRequest 'memory_manager' for <Function test_tensor_allocation>>

    @pytest.fixture
    def memory_manager(request):
        """Fixture to create and cleanup VulkanMemoryManager."""
        manager = None
        try:
            # Initialize Vulkan instance and device first
            import vulkan as vk
            app_info = vk.VkApplicationInfo(
                sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
                pApplicationName="AAT Tests",
                applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                pEngineName="No Engine",
                engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                apiVersion=vk.VK_API_VERSION_1_0
            )
    
            create_info = vk.VkInstanceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
                pApplicationInfo=app_info
            )
    
            instance = vk.vkCreateInstance(create_info, None)
            physical_devices = vk.vkEnumeratePhysicalDevices(instance)
            if not physical_devices:
                pytest.skip("No Vulkan devices found")
    
            physical_device = physical_devices[0]
    
            queue_create_info = vk.VkDeviceQueueCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
                queueFamilyIndex=0,
                queueCount=1,
                pQueuePriorities=[1.0]
            )
    
            device_create_info = vk.VkDeviceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
                pQueueCreateInfos=[queue_create_info],
                queueCreateInfoCount=1
            )
    
            device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
            # Convert device handles to c_void_p
>           device_handle = c_void_p(int(device))
E           TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/gpu/test_memory_management.py:73: TypeError
_____________________ ERROR at setup of test_data_transfer _____________________

request = <SubRequest 'memory_manager' for <Function test_data_transfer>>

    @pytest.fixture
    def memory_manager(request):
        """Fixture to create and cleanup VulkanMemoryManager."""
        manager = None
        try:
            # Initialize Vulkan instance and device first
            import vulkan as vk
            app_info = vk.VkApplicationInfo(
                sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
                pApplicationName="AAT Tests",
                applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                pEngineName="No Engine",
                engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                apiVersion=vk.VK_API_VERSION_1_0
            )
    
            create_info = vk.VkInstanceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
                pApplicationInfo=app_info
            )
    
            instance = vk.vkCreateInstance(create_info, None)
            physical_devices = vk.vkEnumeratePhysicalDevices(instance)
            if not physical_devices:
                pytest.skip("No Vulkan devices found")
    
            physical_device = physical_devices[0]
    
            queue_create_info = vk.VkDeviceQueueCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
                queueFamilyIndex=0,
                queueCount=1,
                pQueuePriorities=[1.0]
            )
    
            device_create_info = vk.VkDeviceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
                pQueueCreateInfos=[queue_create_info],
                queueCreateInfoCount=1
            )
    
            device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
            # Convert device handles to c_void_p
>           device_handle = c_void_p(int(device))
E           TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/gpu/test_memory_management.py:73: TypeError
____________________ ERROR at setup of test_memory_tracking ____________________

request = <SubRequest 'memory_manager' for <Function test_memory_tracking>>

    @pytest.fixture
    def memory_manager(request):
        """Fixture to create and cleanup VulkanMemoryManager."""
        manager = None
        try:
            # Initialize Vulkan instance and device first
            import vulkan as vk
            app_info = vk.VkApplicationInfo(
                sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
                pApplicationName="AAT Tests",
                applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                pEngineName="No Engine",
                engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                apiVersion=vk.VK_API_VERSION_1_0
            )
    
            create_info = vk.VkInstanceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
                pApplicationInfo=app_info
            )
    
            instance = vk.vkCreateInstance(create_info, None)
            physical_devices = vk.vkEnumeratePhysicalDevices(instance)
            if not physical_devices:
                pytest.skip("No Vulkan devices found")
    
            physical_device = physical_devices[0]
    
            queue_create_info = vk.VkDeviceQueueCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
                queueFamilyIndex=0,
                queueCount=1,
                pQueuePriorities=[1.0]
            )
    
            device_create_info = vk.VkDeviceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
                pQueueCreateInfos=[queue_create_info],
                queueCreateInfoCount=1
            )
    
            device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
            # Convert device handles to c_void_p
>           device_handle = c_void_p(int(device))
E           TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/gpu/test_memory_management.py:73: TypeError
____________________ ERROR at setup of test_error_handling _____________________

request = <SubRequest 'memory_manager' for <Function test_error_handling>>

    @pytest.fixture
    def memory_manager(request):
        """Fixture to create and cleanup VulkanMemoryManager."""
        manager = None
        try:
            # Initialize Vulkan instance and device first
            import vulkan as vk
            app_info = vk.VkApplicationInfo(
                sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
                pApplicationName="AAT Tests",
                applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                pEngineName="No Engine",
                engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                apiVersion=vk.VK_API_VERSION_1_0
            )
    
            create_info = vk.VkInstanceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
                pApplicationInfo=app_info
            )
    
            instance = vk.vkCreateInstance(create_info, None)
            physical_devices = vk.vkEnumeratePhysicalDevices(instance)
            if not physical_devices:
                pytest.skip("No Vulkan devices found")
    
            physical_device = physical_devices[0]
    
            queue_create_info = vk.VkDeviceQueueCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
                queueFamilyIndex=0,
                queueCount=1,
                pQueuePriorities=[1.0]
            )
    
            device_create_info = vk.VkDeviceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
                pQueueCreateInfos=[queue_create_info],
                queueCreateInfoCount=1
            )
    
            device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
            # Convert device handles to c_void_p
>           device_handle = c_void_p(int(device))
E           TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/gpu/test_memory_management.py:73: TypeError
__________________ ERROR at setup of test_buffer_pool_cleanup __________________

request = <SubRequest 'memory_manager' for <Function test_buffer_pool_cleanup>>

    @pytest.fixture
    def memory_manager(request):
        """Fixture to create and cleanup VulkanMemoryManager."""
        manager = None
        try:
            # Initialize Vulkan instance and device first
            import vulkan as vk
            app_info = vk.VkApplicationInfo(
                sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
                pApplicationName="AAT Tests",
                applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                pEngineName="No Engine",
                engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
                apiVersion=vk.VK_API_VERSION_1_0
            )
    
            create_info = vk.VkInstanceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
                pApplicationInfo=app_info
            )
    
            instance = vk.vkCreateInstance(create_info, None)
            physical_devices = vk.vkEnumeratePhysicalDevices(instance)
            if not physical_devices:
                pytest.skip("No Vulkan devices found")
    
            physical_device = physical_devices[0]
    
            queue_create_info = vk.VkDeviceQueueCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
                queueFamilyIndex=0,
                queueCount=1,
                pQueuePriorities=[1.0]
            )
    
            device_create_info = vk.VkDeviceCreateInfo(
                sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
                pQueueCreateInfos=[queue_create_info],
                queueCreateInfoCount=1
            )
    
            device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
            # Convert device handles to c_void_p
>           device_handle = c_void_p(int(device))
E           TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/gpu/test_memory_management.py:73: TypeError
_______ ERROR at setup of test_shader_compilation[matrix_size0-pattern] ________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_________ ERROR at setup of test_shader_compilation[matrix_size0-flow] _________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
______ ERROR at setup of test_shader_compilation[matrix_size0-attention] _______

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_______ ERROR at setup of test_shader_compilation[matrix_size1-pattern] ________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_________ ERROR at setup of test_shader_compilation[matrix_size1-flow] _________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
______ ERROR at setup of test_shader_compilation[matrix_size1-attention] _______

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_______ ERROR at setup of test_shader_compilation[matrix_size2-pattern] ________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_________ ERROR at setup of test_shader_compilation[matrix_size2-flow] _________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
______ ERROR at setup of test_shader_compilation[matrix_size2-attention] _______

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_ ERROR at setup of test_workgroup_optimization[matrix_size0-workgroup_size0] __

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_ ERROR at setup of test_workgroup_optimization[matrix_size0-workgroup_size1] __

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_ ERROR at setup of test_workgroup_optimization[matrix_size0-workgroup_size2] __

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_ ERROR at setup of test_workgroup_optimization[matrix_size1-workgroup_size0] __

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_ ERROR at setup of test_workgroup_optimization[matrix_size1-workgroup_size1] __

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_ ERROR at setup of test_workgroup_optimization[matrix_size1-workgroup_size2] __

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_ ERROR at setup of test_workgroup_optimization[matrix_size2-workgroup_size0] __

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_ ERROR at setup of test_workgroup_optimization[matrix_size2-workgroup_size1] __

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_ ERROR at setup of test_workgroup_optimization[matrix_size2-workgroup_size2] __

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
____________ ERROR at setup of test_memory_transfer[matrix_size0-1] ____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
____________ ERROR at setup of test_memory_transfer[matrix_size0-8] ____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
___________ ERROR at setup of test_memory_transfer[matrix_size0-32] ____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
____________ ERROR at setup of test_memory_transfer[matrix_size1-1] ____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
____________ ERROR at setup of test_memory_transfer[matrix_size1-8] ____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
___________ ERROR at setup of test_memory_transfer[matrix_size1-32] ____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
____________ ERROR at setup of test_memory_transfer[matrix_size2-1] ____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
____________ ERROR at setup of test_memory_transfer[matrix_size2-8] ____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
___________ ERROR at setup of test_memory_transfer[matrix_size2-32] ____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_____________ ERROR at setup of test_resource_management[pattern] ______________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
_______________ ERROR at setup of test_resource_management[flow] _______________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
____________ ERROR at setup of test_resource_management[attention] _____________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
______________ ERROR at setup of test_descriptor_set_optimization ______________

    @pytest.fixture
    def vulkan_compute():
        """Create a VulkanCompute instance for testing."""
        # Ensure all shaders are compiled
        for shader_path in SHADER_PATHS.values():
>           compile_shader(shader_path)

tests/performance/vulkan/test_compute.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

shader_path = 'src/core/backends/vulkan/shaders/pattern_compute.comp'

    def compile_shader(shader_path: str) -> str:
        """Compile GLSL shader to SPIR-V."""
        spv_path = shader_path + ".spv"
    
        # Only recompile if source is newer than SPIR-V
        if os.path.exists(spv_path):
            src_time = os.path.getmtime(shader_path)
            spv_time = os.path.getmtime(spv_path)
            if src_time <= spv_time:
                return spv_path
    
        result = subprocess.run(
            ["glslc", shader_path, "-o", spv_path],
            capture_output=True,
            text=True
        )
    
        if result.returncode != 0:
>           raise RuntimeError(f"Shader compilation failed: {result.stderr}")
E           RuntimeError: Shader compilation failed: glslc: error: cannot open input file: 'src/core/backends/vulkan/shaders/pattern_compute.comp': No such file or directory

tests/performance/vulkan/test_compute.py:53: RuntimeError
____ ERROR at setup of TestVulkanShaders.test_pattern_evolution_performance ____

self = <test_shaders.TestVulkanShaders object at 0x7055441deae0>

    @pytest.fixture(autouse=True)
    def setup(self) -> None:
        """Set up test parameters."""
        self.batch_sizes = [1, 4, 16]
        self.pattern_sizes = [128, 256, 512]
        self.manifold_dims = [16, 32, 64]
        self.iterations = 3
>       self.device, self.physical_device = create_vulkan_device()

tests/performance/vulkan/test_shaders.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def create_vulkan_device():
        """Create Vulkan instance and device for testing."""
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName="AAT Tests",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName="No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        instance = vk.vkCreateInstance(create_info, None)
        physical_devices = vk.vkEnumeratePhysicalDevices(instance)
        if not physical_devices:
            pytest.skip("No Vulkan devices found")
    
        physical_device = physical_devices[0]
    
        queue_create_info = vk.VkDeviceQueueCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
            queueFamilyIndex=0,
            queueCount=1,
            pQueuePriorities=[1.0]
        )
    
        device_create_info = vk.VkDeviceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
            pQueueCreateInfos=[queue_create_info],
            queueCreateInfoCount=1
        )
    
        device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
        # Convert device handles to c_void_p
>       device_handle = c_void_p(int(device))
E       TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/test_shaders.py:51: TypeError
____ ERROR at setup of TestVulkanShaders.test_flow_computation_performance _____

self = <test_shaders.TestVulkanShaders object at 0x70554400c890>

    @pytest.fixture(autouse=True)
    def setup(self) -> None:
        """Set up test parameters."""
        self.batch_sizes = [1, 4, 16]
        self.pattern_sizes = [128, 256, 512]
        self.manifold_dims = [16, 32, 64]
        self.iterations = 3
>       self.device, self.physical_device = create_vulkan_device()

tests/performance/vulkan/test_shaders.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def create_vulkan_device():
        """Create Vulkan instance and device for testing."""
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName="AAT Tests",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName="No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        instance = vk.vkCreateInstance(create_info, None)
        physical_devices = vk.vkEnumeratePhysicalDevices(instance)
        if not physical_devices:
            pytest.skip("No Vulkan devices found")
    
        physical_device = physical_devices[0]
    
        queue_create_info = vk.VkDeviceQueueCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
            queueFamilyIndex=0,
            queueCount=1,
            pQueuePriorities=[1.0]
        )
    
        device_create_info = vk.VkDeviceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
            pQueueCreateInfos=[queue_create_info],
            queueCreateInfoCount=1
        )
    
        device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
        # Convert device handles to c_void_p
>       device_handle = c_void_p(int(device))
E       TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/test_shaders.py:51: TypeError
__________ ERROR at setup of TestVulkanShaders.test_workgroup_impact ___________

self = <test_shaders.TestVulkanShaders object at 0x70554400ca40>

    @pytest.fixture(autouse=True)
    def setup(self) -> None:
        """Set up test parameters."""
        self.batch_sizes = [1, 4, 16]
        self.pattern_sizes = [128, 256, 512]
        self.manifold_dims = [16, 32, 64]
        self.iterations = 3
>       self.device, self.physical_device = create_vulkan_device()

tests/performance/vulkan/test_shaders.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def create_vulkan_device():
        """Create Vulkan instance and device for testing."""
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName="AAT Tests",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName="No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        instance = vk.vkCreateInstance(create_info, None)
        physical_devices = vk.vkEnumeratePhysicalDevices(instance)
        if not physical_devices:
            pytest.skip("No Vulkan devices found")
    
        physical_device = physical_devices[0]
    
        queue_create_info = vk.VkDeviceQueueCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
            queueFamilyIndex=0,
            queueCount=1,
            pQueuePriorities=[1.0]
        )
    
        device_create_info = vk.VkDeviceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
            pQueueCreateInfos=[queue_create_info],
            queueCreateInfoCount=1
        )
    
        device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
        # Convert device handles to c_void_p
>       device_handle = c_void_p(int(device))
E       TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/test_shaders.py:51: TypeError
______ ERROR at setup of TestVulkanShaders.test_push_constant_performance ______

self = <test_shaders.TestVulkanShaders object at 0x70554400cbf0>

    @pytest.fixture(autouse=True)
    def setup(self) -> None:
        """Set up test parameters."""
        self.batch_sizes = [1, 4, 16]
        self.pattern_sizes = [128, 256, 512]
        self.manifold_dims = [16, 32, 64]
        self.iterations = 3
>       self.device, self.physical_device = create_vulkan_device()

tests/performance/vulkan/test_shaders.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def create_vulkan_device():
        """Create Vulkan instance and device for testing."""
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName="AAT Tests",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName="No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        instance = vk.vkCreateInstance(create_info, None)
        physical_devices = vk.vkEnumeratePhysicalDevices(instance)
        if not physical_devices:
            pytest.skip("No Vulkan devices found")
    
        physical_device = physical_devices[0]
    
        queue_create_info = vk.VkDeviceQueueCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
            queueFamilyIndex=0,
            queueCount=1,
            pQueuePriorities=[1.0]
        )
    
        device_create_info = vk.VkDeviceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
            pQueueCreateInfos=[queue_create_info],
            queueCreateInfoCount=1
        )
    
        device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
        # Convert device handles to c_void_p
>       device_handle = c_void_p(int(device))
E       TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/test_shaders.py:51: TypeError
_____ ERROR at setup of TestVulkanShaders.test_batch_processing_efficiency _____

self = <test_shaders.TestVulkanShaders object at 0x70554400cda0>

    @pytest.fixture(autouse=True)
    def setup(self) -> None:
        """Set up test parameters."""
        self.batch_sizes = [1, 4, 16]
        self.pattern_sizes = [128, 256, 512]
        self.manifold_dims = [16, 32, 64]
        self.iterations = 3
>       self.device, self.physical_device = create_vulkan_device()

tests/performance/vulkan/test_shaders.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def create_vulkan_device():
        """Create Vulkan instance and device for testing."""
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName="AAT Tests",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName="No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        instance = vk.vkCreateInstance(create_info, None)
        physical_devices = vk.vkEnumeratePhysicalDevices(instance)
        if not physical_devices:
            pytest.skip("No Vulkan devices found")
    
        physical_device = physical_devices[0]
    
        queue_create_info = vk.VkDeviceQueueCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
            queueFamilyIndex=0,
            queueCount=1,
            pQueuePriorities=[1.0]
        )
    
        device_create_info = vk.VkDeviceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
            pQueueCreateInfos=[queue_create_info],
            queueCreateInfoCount=1
        )
    
        device = vk.vkCreateDevice(physical_device, device_create_info, None)
    
        # Convert device handles to c_void_p
>       device_handle = c_void_p(int(device))
E       TypeError: int() not supported on cdata 'struct VkDevice_T *'

tests/performance/vulkan/test_shaders.py:51: TypeError
___________ ERROR at setup of TestVulkanSync.test_fence_performance ____________

self = <test_sync.TestVulkanSync object at 0x70554400d790>

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup test environment."""
        assert torch.is_vulkan_available(), "Vulkan is not available"
    
        # Create instance
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName=b"VulkanSyncTest",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName=b"No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        self.instance = c_void_p()
>       result = vk.vkCreateInstance(byref(create_info), None, byref(self.instance))
E       TypeError: byref() argument must be a ctypes instance, not '_cffi_backend.__CDataOwn'

tests/performance/vulkan/test_sync.py:46: TypeError
__________ ERROR at teardown of TestVulkanSync.test_fence_performance __________

self = <test_sync.TestVulkanSync object at 0x70554400d790>

    def teardown_method(self):
        """Cleanup after each test."""
        if hasattr(self, 'sync'):
            self.sync.cleanup()
        if hasattr(self, 'device'):
            vk.vkDestroyDevice(self.device, None)
        if hasattr(self, 'instance'):
>           vk.vkDestroyInstance(self.instance, None)

tests/performance/vulkan/test_sync.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11058: in vkDestroyInstance
    result = _callApi(lib.vkDestroyInstance, instance,pAllocator)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11023: in _callApi
    fn_args = [_auto_handle(i, j) for i, j in zip(args, ffi.typeof(fn).args)]
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11017: in _auto_handle
    ptr, _ = _cast_ptr(x, _type)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:96: in _cast_ptr3
    return _cast_ptr2(x, _type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = c_void_p(None), _type = <ctype 'struct VkInstance_T *'>

    def _cast_ptr2(x, _type):
        if isinstance(x, ffi.CData):
            if (_type.item == ffi.typeof(x) or
                (_type.item.cname == 'void' and ffi.typeof(x).kind in
                 ['struct', 'union'])):
                return ffi.addressof(x), x
            return x, x
    
        if isinstance(x, Iterable):
            if _type.item.kind == 'pointer':
                ptrs = [_cast_ptr(i, _type.item) for i in x]
                ret = ffi.new(_type.item.cname+'[]', [i for i, _ in ptrs])
                _weakkey_dict[ret] = tuple(i for _, i in ptrs if i != ffi.NULL)
            else:
                ret = ffi.new(_type.item.cname+'[]', x)
    
            return ret, ret
    
>       return ffi.cast(_type, x), x
E       TypeError: an integer is required

venv/lib/python3.12/site-packages/vulkan/_vulkan.py:87: TypeError
_________ ERROR at setup of TestVulkanSync.test_semaphore_performance __________

self = <test_sync.TestVulkanSync object at 0x70554400d970>

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup test environment."""
        assert torch.is_vulkan_available(), "Vulkan is not available"
    
        # Create instance
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName=b"VulkanSyncTest",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName=b"No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        self.instance = c_void_p()
>       result = vk.vkCreateInstance(byref(create_info), None, byref(self.instance))
E       TypeError: byref() argument must be a ctypes instance, not '_cffi_backend.__CDataOwn'

tests/performance/vulkan/test_sync.py:46: TypeError
________ ERROR at teardown of TestVulkanSync.test_semaphore_performance ________

self = <test_sync.TestVulkanSync object at 0x70554400d970>

    def teardown_method(self):
        """Cleanup after each test."""
        if hasattr(self, 'sync'):
            self.sync.cleanup()
        if hasattr(self, 'device'):
            vk.vkDestroyDevice(self.device, None)
        if hasattr(self, 'instance'):
>           vk.vkDestroyInstance(self.instance, None)

tests/performance/vulkan/test_sync.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11058: in vkDestroyInstance
    result = _callApi(lib.vkDestroyInstance, instance,pAllocator)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11023: in _callApi
    fn_args = [_auto_handle(i, j) for i, j in zip(args, ffi.typeof(fn).args)]
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11017: in _auto_handle
    ptr, _ = _cast_ptr(x, _type)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:96: in _cast_ptr3
    return _cast_ptr2(x, _type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = c_void_p(None), _type = <ctype 'struct VkInstance_T *'>

    def _cast_ptr2(x, _type):
        if isinstance(x, ffi.CData):
            if (_type.item == ffi.typeof(x) or
                (_type.item.cname == 'void' and ffi.typeof(x).kind in
                 ['struct', 'union'])):
                return ffi.addressof(x), x
            return x, x
    
        if isinstance(x, Iterable):
            if _type.item.kind == 'pointer':
                ptrs = [_cast_ptr(i, _type.item) for i in x]
                ret = ffi.new(_type.item.cname+'[]', [i for i, _ in ptrs])
                _weakkey_dict[ret] = tuple(i for _, i in ptrs if i != ffi.NULL)
            else:
                ret = ffi.new(_type.item.cname+'[]', x)
    
            return ret, ret
    
>       return ffi.cast(_type, x), x
E       TypeError: an integer is required

venv/lib/python3.12/site-packages/vulkan/_vulkan.py:87: TypeError
___________ ERROR at setup of TestVulkanSync.test_event_performance ____________

self = <test_sync.TestVulkanSync object at 0x70554400db50>

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup test environment."""
        assert torch.is_vulkan_available(), "Vulkan is not available"
    
        # Create instance
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName=b"VulkanSyncTest",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName=b"No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        self.instance = c_void_p()
>       result = vk.vkCreateInstance(byref(create_info), None, byref(self.instance))
E       TypeError: byref() argument must be a ctypes instance, not '_cffi_backend.__CDataOwn'

tests/performance/vulkan/test_sync.py:46: TypeError
__________ ERROR at teardown of TestVulkanSync.test_event_performance __________

self = <test_sync.TestVulkanSync object at 0x70554400db50>

    def teardown_method(self):
        """Cleanup after each test."""
        if hasattr(self, 'sync'):
            self.sync.cleanup()
        if hasattr(self, 'device'):
            vk.vkDestroyDevice(self.device, None)
        if hasattr(self, 'instance'):
>           vk.vkDestroyInstance(self.instance, None)

tests/performance/vulkan/test_sync.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11058: in vkDestroyInstance
    result = _callApi(lib.vkDestroyInstance, instance,pAllocator)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11023: in _callApi
    fn_args = [_auto_handle(i, j) for i, j in zip(args, ffi.typeof(fn).args)]
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11017: in _auto_handle
    ptr, _ = _cast_ptr(x, _type)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:96: in _cast_ptr3
    return _cast_ptr2(x, _type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = c_void_p(None), _type = <ctype 'struct VkInstance_T *'>

    def _cast_ptr2(x, _type):
        if isinstance(x, ffi.CData):
            if (_type.item == ffi.typeof(x) or
                (_type.item.cname == 'void' and ffi.typeof(x).kind in
                 ['struct', 'union'])):
                return ffi.addressof(x), x
            return x, x
    
        if isinstance(x, Iterable):
            if _type.item.kind == 'pointer':
                ptrs = [_cast_ptr(i, _type.item) for i in x]
                ret = ffi.new(_type.item.cname+'[]', [i for i, _ in ptrs])
                _weakkey_dict[ret] = tuple(i for _, i in ptrs if i != ffi.NULL)
            else:
                ret = ffi.new(_type.item.cname+'[]', x)
    
            return ret, ret
    
>       return ffi.cast(_type, x), x
E       TypeError: an integer is required

venv/lib/python3.12/site-packages/vulkan/_vulkan.py:87: TypeError
____________ ERROR at setup of TestVulkanSync.test_barrier_overhead ____________

self = <test_sync.TestVulkanSync object at 0x70554400dd30>

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup test environment."""
        assert torch.is_vulkan_available(), "Vulkan is not available"
    
        # Create instance
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName=b"VulkanSyncTest",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName=b"No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        self.instance = c_void_p()
>       result = vk.vkCreateInstance(byref(create_info), None, byref(self.instance))
E       TypeError: byref() argument must be a ctypes instance, not '_cffi_backend.__CDataOwn'

tests/performance/vulkan/test_sync.py:46: TypeError
__________ ERROR at teardown of TestVulkanSync.test_barrier_overhead ___________

self = <test_sync.TestVulkanSync object at 0x70554400dd30>

    def teardown_method(self):
        """Cleanup after each test."""
        if hasattr(self, 'sync'):
            self.sync.cleanup()
        if hasattr(self, 'device'):
            vk.vkDestroyDevice(self.device, None)
        if hasattr(self, 'instance'):
>           vk.vkDestroyInstance(self.instance, None)

tests/performance/vulkan/test_sync.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11058: in vkDestroyInstance
    result = _callApi(lib.vkDestroyInstance, instance,pAllocator)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11023: in _callApi
    fn_args = [_auto_handle(i, j) for i, j in zip(args, ffi.typeof(fn).args)]
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11017: in _auto_handle
    ptr, _ = _cast_ptr(x, _type)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:96: in _cast_ptr3
    return _cast_ptr2(x, _type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = c_void_p(None), _type = <ctype 'struct VkInstance_T *'>

    def _cast_ptr2(x, _type):
        if isinstance(x, ffi.CData):
            if (_type.item == ffi.typeof(x) or
                (_type.item.cname == 'void' and ffi.typeof(x).kind in
                 ['struct', 'union'])):
                return ffi.addressof(x), x
            return x, x
    
        if isinstance(x, Iterable):
            if _type.item.kind == 'pointer':
                ptrs = [_cast_ptr(i, _type.item) for i in x]
                ret = ffi.new(_type.item.cname+'[]', [i for i, _ in ptrs])
                _weakkey_dict[ret] = tuple(i for _, i in ptrs if i != ffi.NULL)
            else:
                ret = ffi.new(_type.item.cname+'[]', x)
    
            return ret, ret
    
>       return ffi.cast(_type, x), x
E       TypeError: an integer is required

venv/lib/python3.12/site-packages/vulkan/_vulkan.py:87: TypeError
_______________ ERROR at setup of TestVulkanSync.test_queue_sync _______________

self = <test_sync.TestVulkanSync object at 0x70554400df10>

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup test environment."""
        assert torch.is_vulkan_available(), "Vulkan is not available"
    
        # Create instance
        app_info = vk.VkApplicationInfo(
            sType=vk.VK_STRUCTURE_TYPE_APPLICATION_INFO,
            pApplicationName=b"VulkanSyncTest",
            applicationVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            pEngineName=b"No Engine",
            engineVersion=vk.VK_MAKE_VERSION(1, 0, 0),
            apiVersion=vk.VK_API_VERSION_1_0
        )
    
        create_info = vk.VkInstanceCreateInfo(
            sType=vk.VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
            pApplicationInfo=app_info
        )
    
        self.instance = c_void_p()
>       result = vk.vkCreateInstance(byref(create_info), None, byref(self.instance))
E       TypeError: byref() argument must be a ctypes instance, not '_cffi_backend.__CDataOwn'

tests/performance/vulkan/test_sync.py:46: TypeError
_____________ ERROR at teardown of TestVulkanSync.test_queue_sync ______________

self = <test_sync.TestVulkanSync object at 0x70554400df10>

    def teardown_method(self):
        """Cleanup after each test."""
        if hasattr(self, 'sync'):
            self.sync.cleanup()
        if hasattr(self, 'device'):
            vk.vkDestroyDevice(self.device, None)
        if hasattr(self, 'instance'):
>           vk.vkDestroyInstance(self.instance, None)

tests/performance/vulkan/test_sync.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11058: in vkDestroyInstance
    result = _callApi(lib.vkDestroyInstance, instance,pAllocator)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11023: in _callApi
    fn_args = [_auto_handle(i, j) for i, j in zip(args, ffi.typeof(fn).args)]
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:11017: in _auto_handle
    ptr, _ = _cast_ptr(x, _type)
venv/lib/python3.12/site-packages/vulkan/_vulkan.py:96: in _cast_ptr3
    return _cast_ptr2(x, _type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = c_void_p(None), _type = <ctype 'struct VkInstance_T *'>

    def _cast_ptr2(x, _type):
        if isinstance(x, ffi.CData):
            if (_type.item == ffi.typeof(x) or
                (_type.item.cname == 'void' and ffi.typeof(x).kind in
                 ['struct', 'union'])):
                return ffi.addressof(x), x
            return x, x
    
        if isinstance(x, Iterable):
            if _type.item.kind == 'pointer':
                ptrs = [_cast_ptr(i, _type.item) for i in x]
                ret = ffi.new(_type.item.cname+'[]', [i for i, _ in ptrs])
                _weakkey_dict[ret] = tuple(i for _, i in ptrs if i != ffi.NULL)
            else:
                ret = ffi.new(_type.item.cname+'[]', x)
    
            return ret, ret
    
>       return ffi.cast(_type, x), x
E       TypeError: an integer is required

venv/lib/python3.12/site-packages/vulkan/_vulkan.py:87: TypeError
_ ERROR at setup of TestPatternFiberBundle.test_connection_vertical_preservation _
file /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_core/test_patterns/test_fiber_bundle.py, line 667
      def test_connection_vertical_preservation(self, bundle, request, test_config):
E       fixture 'bundle' not found
>       available fixtures: base_bundle, base_manifold, batch_size, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, dtype, fiber_dim, hydra_restore_singletons, hydra_sweep_runner, hydra_task_runner, monkeypatch, pattern_bundle, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, structure_group, tangent_vector, test_config, test_path, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, total_space
>       use 'pytest --fixtures [testpath]' for help on them.

/home/d/Desktop/adaptive-attention-tiling-v2/tests/test_core/test_patterns/test_fiber_bundle.py:667
_ ERROR at setup of TestPatternFiberBundle.test_connection_horizontal_projection _
file /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_core/test_patterns/test_fiber_bundle.py, line 696
      def test_connection_horizontal_projection(self, bundle, request, test_config):
E       fixture 'bundle' not found
>       available fixtures: base_bundle, base_manifold, batch_size, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, dtype, fiber_dim, hydra_restore_singletons, hydra_sweep_runner, hydra_task_runner, monkeypatch, pattern_bundle, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, structure_group, tangent_vector, test_config, test_path, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, total_space
>       use 'pytest --fixtures [testpath]' for help on them.

/home/d/Desktop/adaptive-attention-tiling-v2/tests/test_core/test_patterns/test_fiber_bundle.py:696
_ ERROR at setup of TestPatternFiberBundle.test_connection_levi_civita_compatibility _
file /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_core/test_patterns/test_fiber_bundle.py, line 726
      def test_connection_levi_civita_compatibility(self, bundle, request, test_config):
E       fixture 'bundle' not found
>       available fixtures: base_bundle, base_manifold, batch_size, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, dtype, fiber_dim, hydra_restore_singletons, hydra_sweep_runner, hydra_task_runner, monkeypatch, pattern_bundle, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, structure_group, tangent_vector, test_config, test_path, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, total_space
>       use 'pytest --fixtures [testpath]' for help on them.

/home/d/Desktop/adaptive-attention-tiling-v2/tests/test_core/test_patterns/test_fiber_bundle.py:726
_________________ ERROR at setup of test_stability_computation _________________
file /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_attention/test_pattern/test_bifurcation.py, line 168
  def test_stability_computation(pattern_dynamics, simple_parameterized_reaction):
E       fixture 'pattern_dynamics' not found
>       available fixtures: batch_size, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, grid_size, hydra_restore_singletons, hydra_sweep_runner, hydra_task_runner, monkeypatch, pattern_system, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, simple_parameterized_reaction, space_dim, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_attention/test_pattern/test_bifurcation.py:168
____________________ ERROR at setup of test_state_evolution ____________________
file /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_attention/test_pattern/test_bifurcation.py, line 185
  def test_state_evolution(pattern_dynamics, simple_parameterized_reaction):
E       fixture 'pattern_dynamics' not found
>       available fixtures: batch_size, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, grid_size, hydra_restore_singletons, hydra_sweep_runner, hydra_task_runner, monkeypatch, pattern_system, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, simple_parameterized_reaction, space_dim, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_attention/test_pattern/test_bifurcation.py:185
___________ ERROR at setup of test_bifurcation_detection_components ____________
file /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_attention/test_pattern/test_bifurcation.py, line 209
  def test_bifurcation_detection_components(pattern_dynamics, simple_parameterized_reaction):
E       fixture 'pattern_dynamics' not found
>       available fixtures: batch_size, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, grid_size, hydra_restore_singletons, hydra_sweep_runner, hydra_task_runner, monkeypatch, pattern_system, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, simple_parameterized_reaction, space_dim, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_attention/test_pattern/test_bifurcation.py:209
______________ ERROR at setup of test_convergence_at_bifurcation _______________
file /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_attention/test_pattern/test_bifurcation.py, line 252
  def test_convergence_at_bifurcation(pattern_dynamics, simple_parameterized_reaction):
E       fixture 'pattern_dynamics' not found
>       available fixtures: batch_size, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, grid_size, hydra_restore_singletons, hydra_sweep_runner, hydra_task_runner, monkeypatch, pattern_system, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, simple_parameterized_reaction, space_dim, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_attention/test_pattern/test_bifurcation.py:252
=================================== FAILURES ===================================
_____________________________ test_exp_log_inverse _____________________________

hyperbolic_exp = HyperbolicExponential(), hyperbolic_log = HyperbolicLogarithm()
dim = 2, batch_size = 2

    def test_exp_log_inverse(hyperbolic_exp, hyperbolic_log, dim, batch_size):
        """Test that exp and log are inverse operations."""
        # Create test point and tangent vector with smaller magnitudes
        x = torch.randn(batch_size, dim) * 0.1  # Scale down the base point
        v = torch.randn(batch_size, dim) * 0.1  # Scale down the tangent vector
    
        # Project x to hyperboloid and v to tangent space
        x = hyperbolic_exp.project_to_hyperboloid(x)
        v = hyperbolic_exp.project_to_tangent(x, v)
    
        # Print intermediate values for debugging
        print(f"\nOriginal vector v: {v}")
    
        # Apply exp then log
        y = hyperbolic_exp(x, v)
        print(f"Point after exp map y: {y}")
    
        v_recovered = hyperbolic_log(x, y)
        print(f"Recovered vector v: {v_recovered}")
        print(f"Difference: {torch.abs(v - v_recovered)}")
    
        # Test recovery of tangent vector with looser tolerances
>       assert torch.allclose(v, v_recovered, rtol=1e-3, atol=1e-3)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x70560ced8b20>(tensor([[-0.0002,  0.0126],\n        [-0.0110,  0.1460]]), tensor([[-1.6083e-05,  1.0869e-03],\n        [-1.0906e-02,  1.4504e-01]]), rtol=0.001, atol=0.001)
E        +    where <built-in method allclose of type object at 0x70560ced8b20> = torch.allclose

tests/core/attention/test_geometric.py:134: AssertionError
________________________ test_performance_benchmarking _________________________

    @pytest.mark.benchmark
    def test_performance_benchmarking() -> None:
        """Test performance benchmarking capabilities."""
        tile = TestAttentionTile(input_dim=128, hidden_dim=128)
        inputs = torch.randn(1, 64, 128, requires_grad=True)
        target = torch.randn(1, 64, 128)
    
>       forward_time, backward_time = benchmark_forward_backward(
            model=tile,
            input_data=inputs,
            target=target,
            loss_fn=torch.nn.functional.mse_loss
        )

tests/core/tiling/test_strategies.py:217: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/utils/test_helpers.py:75: in benchmark_forward_backward
    loss.backward()
venv/lib/python3.12/site-packages/torch/_tensor.py:648: in backward
    torch.autograd.backward(
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347: in backward
    _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(1.9665),), args = ((None,), False, False, ())
kwargs = {'accumulate_grad': True, 'allow_unreachable': True}
attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
____________________________ test_state_management _____________________________

    def test_state_management() -> None:
        """Test state compression, expansion, and transfer."""
        tile = TestAttentionTile(input_dim=128, hidden_dim=128)
    
        # Create sample state
        original_state = torch.randn(1, 32, 64)  # [batch, seq, state_dim]
        tile.state = original_state
    
        # Test compression
        compressed = tile.compress_state()
>       assert compressed.shape[1] < original_state.shape[1]
E       assert 32 < 32

tests/core/tiling/test_strategies.py:316: AssertionError
______________ TestMetricsIntegration.test_metrics_initialization ______________

self = <test_metrics_integration.TestMetricsIntegration object at 0x705544fec920>
attention_tile = QuantumMotivicTile(
  (query): Linear(in_features=32, out_features=32, bias=True)
  (key): Linear(in_features=32, out_...Linear(in_features=32, out_features=32, bias=True)
  (height_proj): Linear(in_features=32, out_features=1, bias=True)
)

    def test_metrics_initialization(self, attention_tile):
        """Test that advanced metrics are properly initialized."""
        metrics = attention_tile.get_metrics()
        assert "ifq" in metrics
        assert "cer" in metrics
        assert "ae" in metrics
>       assert "quantum_entropy" in metrics
E       AssertionError: assert 'quantum_entropy' in {'ae': 1.0, 'cer': 1.0, 'density': 1.0, 'flow': 0.0, ...}

tests/metrics/test_metrics_integration.py:37: AssertionError
_____________ TestMetricsIntegration.test_metrics_after_processing _____________

self = <test_metrics_integration.TestMetricsIntegration object at 0x705544fecad0>
attention_tile = QuantumMotivicTile(
  (query): Linear(in_features=32, out_features=32, bias=True)
  (key): Linear(in_features=32, out_...Linear(in_features=32, out_features=32, bias=True)
  (height_proj): Linear(in_features=32, out_features=1, bias=True)
)
input_tensor = tensor([[[ 0.1795,  1.3871, -0.8838,  ...,  0.1585, -1.8285, -0.2253],
         [-0.8166, -0.0224,  1.0265,  ...,  0.0...07,  0.2936,  ..., -0.2430,  0.2969, -1.2162],
         [ 0.4119,  0.5320, -0.6331,  ...,  0.9421, -1.9100, -0.3084]]])

    def test_metrics_after_processing(self, attention_tile, input_tensor):
        """Test metrics update after processing input."""
        # Process input
        output = attention_tile._process_impl(input_tensor, update_metrics=True)
        metrics = attention_tile.get_metrics()
    
        # Check metrics were updated and are within valid ranges
        assert 0.0 <= metrics["ifq"] <= 1.0
        assert metrics["cer"] >= 0.0
        assert 0.0 <= metrics["ae"] <= 1.0
>       assert 0.1 <= metrics["quantum_entropy"] <= 5.0
E       KeyError: 'quantum_entropy'

tests/metrics/test_metrics_integration.py:53: KeyError
_________________ TestMetricsIntegration.test_metrics_history __________________

self = <test_metrics_integration.TestMetricsIntegration object at 0x705544fecc80>
attention_tile = QuantumMotivicTile(
  (query): Linear(in_features=32, out_features=32, bias=True)
  (key): Linear(in_features=32, out_...Linear(in_features=32, out_features=32, bias=True)
  (height_proj): Linear(in_features=32, out_features=1, bias=True)
)
input_tensor = tensor([[[-0.2113, -1.7063, -0.6057,  ..., -0.5147,  1.0641, -0.5210],
         [-0.5270, -0.5241,  1.2796,  ...,  0.3...63, -0.1948,  ..., -1.1335, -1.2517, -0.2536],
         [ 0.6771, -0.8262, -0.6877,  ...,  0.4797,  0.9654,  1.3793]]])

    def test_metrics_history(self, attention_tile, input_tensor):
        """Test metrics history accumulation."""
        # Process multiple times
        for _ in range(3):
            attention_tile._process_impl(input_tensor, update_metrics=True)
            metrics = attention_tile.get_metrics()
            attention_tile._metrics_log.append(metrics)
    
        # Check metrics log
        assert len(attention_tile._metrics_log) >= 3
        assert all("ifq" in m for m in attention_tile._metrics_log[-3:])
        assert all("cer" in m for m in attention_tile._metrics_log[-3:])
>       assert all("ae" in m for m in attention_tile._metrics_log[-3:])
E       assert False
E        +  where False = all(<generator object TestMetricsIntegration.test_metrics_history.<locals>.<genexpr> at 0x70551616e740>)

tests/metrics/test_metrics_integration.py:71: AssertionError
__________ TestMetricsIntegration.test_metrics_with_resolution_change __________

self = <test_metrics_integration.TestMetricsIntegration object at 0x705544fece30>
attention_tile = QuantumMotivicTile(
  (query): Linear(in_features=32, out_features=32, bias=True)
  (key): Linear(in_features=32, out_...Linear(in_features=32, out_features=32, bias=True)
  (height_proj): Linear(in_features=32, out_features=1, bias=True)
)
input_tensor = tensor([[[-0.4949, -0.0826, -0.7906,  ...,  0.1960, -0.5636, -0.9238],
         [ 0.3016,  1.0787,  1.1053,  ...,  0.2...62,  0.6113,  ..., -0.3635, -2.2053, -0.0285],
         [-1.0065,  0.3451,  1.3556,  ..., -1.2037, -0.5292, -1.0470]]])

    def test_metrics_with_resolution_change(self, attention_tile, input_tensor):
        """Test metrics behavior with resolution changes."""
        # Initial processing
        attention_tile._process_impl(input_tensor, update_metrics=True)
        initial_metrics = attention_tile.get_metrics()
        attention_tile._metrics["resolution_history"] = [1.0]
    
        # Change resolution
        attention_tile.resolution = 0.5
        attention_tile._metrics["resolution_history"].append(0.5)
        attention_tile._process_impl(input_tensor, update_metrics=True)
        new_metrics = attention_tile.get_metrics()
    
        # Metrics should reflect the change
        assert len(attention_tile._metrics["resolution_history"]) >= 2
        assert new_metrics["cer"] != initial_metrics["cer"]
>       assert new_metrics["quantum_entropy"] != initial_metrics["quantum_entropy"]
E       KeyError: 'quantum_entropy'

tests/metrics/test_metrics_integration.py:91: KeyError
______________ TestMetricsIntegration.test_metrics_with_neighbors ______________

self = <test_metrics_integration.TestMetricsIntegration object at 0x705544fecfe0>
attention_tile = QuantumMotivicTile(
  (query): Linear(in_features=32, out_features=32, bias=True)
  (key): Linear(in_features=32, out_...Linear(in_features=32, out_features=32, bias=True)
  (height_proj): Linear(in_features=32, out_features=1, bias=True)
)
input_tensor = tensor([[[-1.5297e+00,  1.0289e+00, -7.0466e-01,  ..., -7.1190e-01,
          -1.8906e+00,  3.7796e-01],
         [-6....6.7338e-01],
         [-2.3414e-01, -2.8727e-01, -4.4618e-01,  ...,  1.6366e+00,
          -3.0277e-04, -6.1986e-01]]])

    def test_metrics_with_neighbors(self, attention_tile, input_tensor):
        """Test metrics with neighboring tiles."""
        # Create neighbor tile
        neighbor = QuantumMotivicTile(size=16, hidden_dim=32)
        neighbor._initialize_quantum_structure()
        attention_tile.add_neighbor(neighbor)
    
        # Process both tiles
        attention_tile._process_impl(input_tensor, update_metrics=True)
        neighbor._process_impl(input_tensor, update_metrics=True)
    
        # Check information flow metrics
        metrics = attention_tile.get_metrics()
        assert metrics["flow"] >= 0.0
        assert 0.0 <= metrics["ifq"] <= 1.0
>       assert metrics["l_function_value"] >= 0.01
E       KeyError: 'l_function_value'

tests/metrics/test_metrics_integration.py:108: KeyError
____________ TestMetricsIntegration.test_metrics_during_adaptation _____________

self = <test_metrics_integration.TestMetricsIntegration object at 0x705544fed190>
attention_tile = QuantumMotivicTile(
  (query): Linear(in_features=32, out_features=32, bias=True)
  (key): Linear(in_features=32, out_...Linear(in_features=32, out_features=32, bias=True)
  (height_proj): Linear(in_features=32, out_features=1, bias=True)
)
input_tensor = tensor([[[ 0.6657, -1.1638,  0.8539,  ...,  0.9419,  1.3322, -0.8043],
         [ 1.2522, -0.5892, -0.3442,  ..., -0.8...65,  2.1878,  ..., -0.5262,  1.9338,  1.1645],
         [-0.6529, -0.3426, -0.3739,  ..., -0.0925, -0.5453,  0.0732]]])

    def test_metrics_during_adaptation(self, attention_tile, input_tensor):
        """Test metrics during resolution adaptation."""
        # Initial state
        attention_tile._process_impl(input_tensor, update_metrics=True)
        initial_metrics = attention_tile.get_metrics().copy()
        attention_tile._metrics["ae"] = 0.5  # Set initial AE
    
        # Simulate adaptation
        adaptation_metrics = []
        for _ in range(5):
            # Process with current resolution
            attention_tile._process_impl(input_tensor, update_metrics=True)
            attention_tile._metrics["ae"] += 0.1  # Simulate AE change
    
            # Adapt resolution based on density
            attention_tile.adapt_resolution(
                density_metric=0.5,
                strategy=ResolutionStrategy.ADAPTIVE,
            )
    
            # Store metrics
            adaptation_metrics.append(attention_tile.get_metrics().copy())
    
        # Verify adaptation is reflected in metrics
        assert len(adaptation_metrics) == 5
>       assert adaptation_metrics[-1]["ae"] != initial_metrics["ae"]
E       assert 1.0 != 1.0

tests/metrics/test_metrics_integration.py:135: AssertionError
________________ TestCoreOperations.test_attention_computation _________________

self = <test_core.TestCoreOperations object at 0x705544128cb0>

    def test_attention_computation(self):
        """Benchmark attention computation performance."""
        attention = AttentionCompute()
    
        for size in self.sizes:
            for batch_size in self.batch_sizes:
                # Generate test data
                query = torch.randn(batch_size, size, 64)
                key = torch.randn(batch_size, size, 64)
                value = torch.randn(batch_size, size, 64)
    
                # Warm-up run
>               _ = attention(query, key, value)

tests/performance/benchmarks/test_core.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = AttentionCompute()
input = (tensor([[[ 1.7389,  0.3159, -0.4454,  ...,  0.4229, -0.1994,  0.4697],
         [-0.3226,  1.4244, -2.0687,  ..., -1.....3345e+00],
         [ 2.6921e-01,  1.2270e+00,  1.0367e+00,  ...,  1.6481e+00,
           4.3468e-01, -9.6135e-02]]]))

    def _forward_unimplemented(self, *input: Any) -> None:
        r"""Define the computation performed at every call.
    
        Should be overridden by all subclasses.
    
        .. note::
            Although the recipe for forward pass needs to be defined within
            this function, one should call the :class:`Module` instance afterwards
            instead of this since the former takes care of running the
            registered hooks while the latter silently ignores them.
        """
>       raise NotImplementedError(
            f'Module [{type(self).__name__}] is missing the required "forward" function'
        )
E       NotImplementedError: Module [AttentionCompute] is missing the required "forward" function

venv/lib/python3.12/site-packages/torch/nn/modules/module.py:397: NotImplementedError
__________________ TestCoreOperations.test_pattern_formation ___________________

self = <test_core.TestCoreOperations object at 0x705544128770>

    def test_pattern_formation(self):
        """Benchmark pattern formation and evolution efficiency."""
        pattern = PatternEvolution(framework=self.riemannian_framework)
    
        for size in self.sizes:
            # Initialize pattern state
            state = torch.randn(size, size)
    
            # Warm-up
>           _ = pattern.step(state, torch.randn_like(state))

tests/performance/benchmarks/test_core.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/patterns/evolution.py:60: in step
    updated_pattern = self.framework.exp_map(pattern, self.velocity)
src/core/patterns/riemannian.py:571: in exp_map
    vector_norm = torch.sqrt(torch.einsum('i,ij,j->', vector, metric, vector))
venv/lib/python3.12/site-packages/torch/functional.py:412: in einsum
    tupled_path = _opt_einsum.contract_path(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

subscripts = 'i,ij,j->', use_blas = True, optimize = 'auto', memory_limit = None
shapes = False
operands = (tensor([[-0.0089, -0.0058,  0.0085,  ..., -0.0114,  0.0237,  0.0029],
        [-0.0002,  0.0039,  0.0062,  ..., -0.00...135,  0.0149,  ..., -0.0131, -0.0009, -0.0206],
        [-0.0071,  0.0241,  0.0068,  ...,  0.0097, -0.0038, -0.0165]]))
kwargs = {}, einsum_call_arg = False
operands_ = ['i,ij,j->', tensor([[-0.0089, -0.0058,  0.0085,  ..., -0.0114,  0.0237,  0.0029],
        [-0.0002,  0.0039,  0.0062,...135,  0.0149,  ..., -0.0131, -0.0009, -0.0206],
        [-0.0071,  0.0241,  0.0068,  ...,  0.0097, -0.0038, -0.0165]])]
input_subscripts = 'i,ij,j', output_subscript = ''
operands_prepped = [tensor([[-0.0089, -0.0058,  0.0085,  ..., -0.0114,  0.0237,  0.0029],
        [-0.0002,  0.0039,  0.0062,  ..., -0.00...135,  0.0149,  ..., -0.0131, -0.0009, -0.0206],
        [-0.0071,  0.0241,  0.0068,  ...,  0.0097, -0.0038, -0.0165]])]
input_list = ['i', 'ij', 'j']

    def contract_path(
        subscripts: Any,
        *operands: Any,
        use_blas: bool = True,
        optimize: OptimizeKind = True,
        memory_limit: _MemoryLimit = None,
        shapes: bool = False,
        **kwargs: Any,
    ) -> Tuple[PathType, PathInfo]:
        """Find a contraction order `path`, without performing the contraction.
    
        Parameters:
              subscripts: Specifies the subscripts for summation.
              *operands: These are the arrays for the operation.
              use_blas: Do you use BLAS for valid operations, may use extra memory for more intermediates.
              optimize: Choose the type of path the contraction will be optimized with.
                    - if a list is given uses this as the path.
                    - `'optimal'` An algorithm that explores all possible ways of
                    contracting the listed tensors. Scales factorially with the number of
                    terms in the contraction.
                    - `'dp'` A faster (but essentially optimal) algorithm that uses
                    dynamic programming to exhaustively search all contraction paths
                    without outer-products.
                    - `'greedy'` An cheap algorithm that heuristically chooses the best
                    pairwise contraction at each step. Scales linearly in the number of
                    terms in the contraction.
                    - `'random-greedy'` Run a randomized version of the greedy algorithm
                    32 times and pick the best path.
                    - `'random-greedy-128'` Run a randomized version of the greedy
                    algorithm 128 times and pick the best path.
                    - `'branch-all'` An algorithm like optimal but that restricts itself
                    to searching 'likely' paths. Still scales factorially.
                    - `'branch-2'` An even more restricted version of 'branch-all' that
                    only searches the best two options at each step. Scales exponentially
                    with the number of terms in the contraction.
                    - `'auto'` Choose the best of the above algorithms whilst aiming to
                    keep the path finding time below 1ms.
                    - `'auto-hq'` Aim for a high quality contraction, choosing the best
                    of the above algorithms whilst aiming to keep the path finding time
                    below 1sec.
    
              memory_limit: Give the upper bound of the largest intermediate tensor contract will build.
                    - None or -1 means there is no limit
                    - `max_input` means the limit is set as largest input tensor
                    - a positive integer is taken as an explicit limit on the number of elements
    
                    The default is None. Note that imposing a limit can make contractions
                    exponentially slower to perform.
    
              shapes: Whether ``contract_path`` should assume arrays (the default) or array shapes have been supplied.
    
        Returns:
              path: The optimized einsum contraciton path
              PathInfo: A printable object containing various information about the path found.
    
        Notes:
              The resulting path indicates which terms of the input contraction should be
              contracted first, the result of this contraction is then appended to the end of
              the contraction list.
    
        Examples:
              We can begin with a chain dot example. In this case, it is optimal to
              contract the b and c tensors represented by the first element of the path (1,
              2). The resulting tensor is added to the end of the contraction and the
              remaining contraction, `(0, 1)`, is then executed.
    
          ```python
          a = np.random.rand(2, 2)
          b = np.random.rand(2, 5)
          c = np.random.rand(5, 2)
          path_info = opt_einsum.contract_path('ij,jk,kl->il', a, b, c)
          print(path_info[0])
          #> [(1, 2), (0, 1)]
          print(path_info[1])
          #>   Complete contraction:  ij,jk,kl->il
          #>          Naive scaling:  4
          #>      Optimized scaling:  3
          #>       Naive FLOP count:  1.600e+02
          #>   Optimized FLOP count:  5.600e+01
          #>    Theoretical speedup:  2.857
          #>   Largest intermediate:  4.000e+00 elements
          #> -------------------------------------------------------------------------
          #> scaling                  current                                remaining
          #> -------------------------------------------------------------------------
          #>    3                   kl,jk->jl                                ij,jl->il
          #>    3                   jl,ij->il                                   il->il
          ```
    
          A more complex index transformation example.
    
          ```python
          I = np.random.rand(10, 10, 10, 10)
          C = np.random.rand(10, 10)
          path_info = oe.contract_path('ea,fb,abcd,gc,hd->efgh', C, C, I, C, C)
    
          print(path_info[0])
          #> [(0, 2), (0, 3), (0, 2), (0, 1)]
          print(path_info[1])
          #>   Complete contraction:  ea,fb,abcd,gc,hd->efgh
          #>          Naive scaling:  8
          #>      Optimized scaling:  5
          #>       Naive FLOP count:  8.000e+08
          #>   Optimized FLOP count:  8.000e+05
          #>    Theoretical speedup:  1000.000
          #>   Largest intermediate:  1.000e+04 elements
          #> --------------------------------------------------------------------------
          #> scaling                  current                                remaining
          #> --------------------------------------------------------------------------
          #>    5               abcd,ea->bcde                      fb,gc,hd,bcde->efgh
          #>    5               bcde,fb->cdef                         gc,hd,cdef->efgh
          #>    5               cdef,gc->defg                            hd,defg->efgh
          #>    5               defg,hd->efgh                               efgh->efgh
          ```
        """
        if (optimize is True) or (optimize is None):
            optimize = "auto"
    
        # Hidden option, only einsum should call this
        einsum_call_arg = kwargs.pop("einsum_call", False)
        if len(kwargs):
            raise TypeError(f"Did not understand the following kwargs: {kwargs.keys()}")
    
        # Python side parsing
        operands_ = [subscripts] + list(operands)
        input_subscripts, output_subscript, operands_prepped = parser.parse_einsum_input(operands_, shapes=shapes)
    
        # Build a few useful list and sets
        input_list = input_subscripts.split(",")
        input_sets = [frozenset(x) for x in input_list]
        if shapes:
            input_shapes = operands_prepped
        else:
            input_shapes = [parser.get_shape(x) for x in operands_prepped]
        output_set = frozenset(output_subscript)
        indices = frozenset(input_subscripts.replace(",", ""))
    
        # Get length of each unique dimension and ensure all dimensions are correct
        size_dict: Dict[str, int] = {}
        for tnum, term in enumerate(input_list):
            sh = input_shapes[tnum]
    
            if len(sh) != len(term):
>               raise ValueError(
                    f"Einstein sum subscript '{input_list[tnum]}' does not contain the "
                    f"correct number of indices for operand {tnum}."
                )
E               ValueError: Einstein sum subscript 'i' does not contain the correct number of indices for operand 0.

venv/lib/python3.12/site-packages/opt_einsum/contract.py:312: ValueError
____________________ TestCoreOperations.test_flow_evolution ____________________

self = <test_core.TestCoreOperations object at 0x705544128980>

    def test_flow_evolution(self):
        """Benchmark flow computation and evolution performance."""
        flow = FlowComputation(dim=2)  # 2D flow
    
        for size in self.sizes:
            # Initialize flow field
            velocity = torch.randn(2, size, size)  # 2D velocity field
            density = torch.randn(size, size)
    
            # Warm-up
>           _ = flow.compute_gradient_flow(velocity, steps=100)

tests/performance/benchmarks/test_core.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/flow/computation.py:52: in compute_gradient_flow
    potential = self.potential(current)
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750: in _call_impl
    return forward_call(*args, **kwargs)
venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250: in forward
    input = module(input)
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=2, out_features=4, bias=True)
input = tensor([[[ 1.6676,  0.3117, -0.7329,  ..., -1.0438, -0.3357, -0.3957],
         [-0.5544,  0.3252, -1.0016,  ..., -1.0...01,  0.5941,  ...,  2.3177, -0.3152, -0.1408],
         [-0.3548, -0.6253,  0.4936,  ..., -1.1843, -0.6716, -0.3295]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (1024x512 and 2x4)

venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125: RuntimeError
___________________ TestCoreOperations.test_memory_patterns ____________________

self = <test_core.TestCoreOperations object at 0x705544128da0>

    def test_memory_patterns(self):
        """Analyze memory allocation patterns and efficiency."""
        for size in self.sizes:
            # Test different allocation patterns
            allocation_sizes = [size // 4, size // 2, size]
    
            for alloc_size in allocation_sizes:
                # Sequential allocation
                start_time = time.perf_counter()
                tensors = []
                for _ in range(4):
                    tensors.append(torch.randn(alloc_size, alloc_size))
                end_time = time.perf_counter()
                seq_time = (end_time - start_time) * 1000  # Convert to ms
    
                # Batch allocation
                start_time = time.perf_counter()
                torch.randn(4, alloc_size, alloc_size)
                end_time = time.perf_counter()
                batch_time = (end_time - start_time) * 1000  # Convert to ms
    
>               self.metrics.record_operation(
                    name="memory_allocation",
                    size=alloc_size,
                    sequential_time=seq_time,
                    batch_time=batch_time,
                    efficiency=batch_time / seq_time
                )

tests/performance/benchmarks/test_core.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = BenchmarkMetrics(operations=[], forward_time=0.0, backward_time=0.0, total_time=0.0, peak_memory_mb=0.0, memory_allocated_mb=0.0, throughput=0.0, flops=0.0, efficiency=0.0, num_parameters=0, batch_size=0, device='cpu', memory_pool=None)
name = 'memory_allocation'
kwargs = {'batch_time': 0.3215219985577278, 'efficiency': 0.9521894304534375, 'sequential_time': 0.3376660024514422, 'size': 128}

    def record_operation(self, name: str, **kwargs):
        """Record metrics for an operation.
    
        Args:
            name: Operation name
            **kwargs: Operation metrics
        """
>       metrics = OperationMetrics(name=name, **kwargs)
E       TypeError: OperationMetrics.__init__() missing 1 required positional argument: 'avg_time'

src/core/benchmarks/metrics.py:64: TypeError
_______________ TestCoreOperations.test_scaling_characteristics ________________

self = <test_core.TestCoreOperations object at 0x705544128f50>

    def test_scaling_characteristics(self):
        """Test scaling behavior with different problem sizes."""
        attention = AttentionCompute()
    
        # Strong scaling (fixed total size, varying batch)
        total_size = 8192
        for batch_size in self.batch_sizes:
            size = total_size // batch_size
            query = torch.randn(batch_size, size, 64)
            key = torch.randn(batch_size, size, 64)
            value = torch.randn(batch_size, size, 64)
    
            # Warm-up
>           _ = attention(query, key, value)

tests/performance/benchmarks/test_core.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = AttentionCompute()
input = (tensor([[[-0.0666, -0.5866,  0.9498,  ...,  1.6541,  0.2141,  0.7739],
         [ 0.6782, -0.1586, -0.4672,  ..., -1....9,  0.4530,  ...,  0.5455,  0.6858, -1.0693],
         [-0.2396,  1.3139,  0.1696,  ...,  0.5357,  0.5186,  0.1547]]]))

    def _forward_unimplemented(self, *input: Any) -> None:
        r"""Define the computation performed at every call.
    
        Should be overridden by all subclasses.
    
        .. note::
            Although the recipe for forward pass needs to be defined within
            this function, one should call the :class:`Module` instance afterwards
            instead of this since the former takes care of running the
            registered hooks while the latter silently ignores them.
        """
>       raise NotImplementedError(
            f'Module [{type(self).__name__}] is missing the required "forward" function'
        )
E       NotImplementedError: Module [AttentionCompute] is missing the required "forward" function

venv/lib/python3.12/site-packages/torch/nn/modules/module.py:397: NotImplementedError
________________ test_fast_path_optimization[0.1-matrix_size0] _________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x705516b192e0>
matrix_size = (64, 64), sparsity = 0.1

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    @pytest.mark.parametrize("sparsity", SPARSITY_LEVELS)
    def test_fast_path_optimization(
        algorithm_optimizer: AlgorithmOptimizer,
        matrix_size: Tuple[int, int],
        sparsity: float,
    ) -> None:
        """Test fast path optimizations for sparse operations."""
        with resource_guard():
            # Generate sparse matrices
            matrix_a = generate_sparse_matrix(matrix_size, sparsity)
            matrix_b = generate_sparse_matrix(matrix_size, sparsity)
    
            # Register fast path for sparse matrix multiplication
            def is_sparse(x: torch.Tensor, threshold: float = 0.5) -> bool:
                return (torch.count_nonzero(x).item() / x.numel()) < threshold
    
            # Register optimized path
            algorithm_optimizer.register_fast_path(
                "sparse_matmul",
                lambda x, y: torch.sparse.mm(x.to_sparse(), y.to_sparse()).to_dense(),
                condition=lambda x, y: is_sparse(x) and is_sparse(y),
            )
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("sparse_matmul", matrix_a, matrix_b)

tests/performance/cpu/test_algorithms.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x705516b192e0>
operation_name = 'sparse_matmul'
args = (tensor([[ 1.0792, -0.0000,  0.4225,  ...,  0.2429, -0.0000,  0.0000],
        [-1.1943,  0.7445,  1.1707,  ...,  0.33...373,  0.0000,  ..., -0.9540, -1.0569, -1.1278],
        [ 2.0131,  0.7651, -0.2353,  ...,  0.3480,  0.0000,  0.5731]]))
kwargs = {}

    def optimize_operation(self, operation_name: str, *args, **kwargs) -> Any:
        """Execute an optimized operation."""
        if operation_name not in self.operations:
>           raise ValueError(f"Operation {operation_name} not registered")
E           ValueError: Operation sparse_matmul not registered

src/core/performance/cpu/algorithms.py:228: ValueError
________________ test_fast_path_optimization[0.1-matrix_size1] _________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173551c0>
matrix_size = (256, 256), sparsity = 0.1

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    @pytest.mark.parametrize("sparsity", SPARSITY_LEVELS)
    def test_fast_path_optimization(
        algorithm_optimizer: AlgorithmOptimizer,
        matrix_size: Tuple[int, int],
        sparsity: float,
    ) -> None:
        """Test fast path optimizations for sparse operations."""
        with resource_guard():
            # Generate sparse matrices
            matrix_a = generate_sparse_matrix(matrix_size, sparsity)
            matrix_b = generate_sparse_matrix(matrix_size, sparsity)
    
            # Register fast path for sparse matrix multiplication
            def is_sparse(x: torch.Tensor, threshold: float = 0.5) -> bool:
                return (torch.count_nonzero(x).item() / x.numel()) < threshold
    
            # Register optimized path
            algorithm_optimizer.register_fast_path(
                "sparse_matmul",
                lambda x, y: torch.sparse.mm(x.to_sparse(), y.to_sparse()).to_dense(),
                condition=lambda x, y: is_sparse(x) and is_sparse(y),
            )
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("sparse_matmul", matrix_a, matrix_b)

tests/performance/cpu/test_algorithms.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173551c0>
operation_name = 'sparse_matmul'
args = (tensor([[-0.6943,  1.4068, -0.1401,  ...,  0.2385, -0.0000, -0.6667],
        [ 2.0571,  0.6174, -0.4864,  ...,  1.35...000,  0.1294,  ..., -0.7801, -0.2468,  0.1399],
        [ 0.0000, -0.2275,  1.2120,  ...,  0.3328, -0.4669,  0.5165]]))
kwargs = {}

    def optimize_operation(self, operation_name: str, *args, **kwargs) -> Any:
        """Execute an optimized operation."""
        if operation_name not in self.operations:
>           raise ValueError(f"Operation {operation_name} not registered")
E           ValueError: Operation sparse_matmul not registered

src/core/performance/cpu/algorithms.py:228: ValueError
________________ test_fast_path_optimization[0.5-matrix_size0] _________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173ebd40>
matrix_size = (64, 64), sparsity = 0.5

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    @pytest.mark.parametrize("sparsity", SPARSITY_LEVELS)
    def test_fast_path_optimization(
        algorithm_optimizer: AlgorithmOptimizer,
        matrix_size: Tuple[int, int],
        sparsity: float,
    ) -> None:
        """Test fast path optimizations for sparse operations."""
        with resource_guard():
            # Generate sparse matrices
            matrix_a = generate_sparse_matrix(matrix_size, sparsity)
            matrix_b = generate_sparse_matrix(matrix_size, sparsity)
    
            # Register fast path for sparse matrix multiplication
            def is_sparse(x: torch.Tensor, threshold: float = 0.5) -> bool:
                return (torch.count_nonzero(x).item() / x.numel()) < threshold
    
            # Register optimized path
            algorithm_optimizer.register_fast_path(
                "sparse_matmul",
                lambda x, y: torch.sparse.mm(x.to_sparse(), y.to_sparse()).to_dense(),
                condition=lambda x, y: is_sparse(x) and is_sparse(y),
            )
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("sparse_matmul", matrix_a, matrix_b)

tests/performance/cpu/test_algorithms.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173ebd40>
operation_name = 'sparse_matmul'
args = (tensor([[ 0.6232, -2.7206, -0.0000,  ..., -0.0000, -0.7204, -0.0000],
        [-0.0000, -0.0000, -0.1073,  ..., -0.47...000, -0.3547,  ..., -0.0000, -0.0000, -0.0000],
        [ 0.0000,  1.2429, -0.0000,  ...,  0.0000, -0.8293, -0.7426]]))
kwargs = {}

    def optimize_operation(self, operation_name: str, *args, **kwargs) -> Any:
        """Execute an optimized operation."""
        if operation_name not in self.operations:
>           raise ValueError(f"Operation {operation_name} not registered")
E           ValueError: Operation sparse_matmul not registered

src/core/performance/cpu/algorithms.py:228: ValueError
________________ test_fast_path_optimization[0.5-matrix_size1] _________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173ebce0>
matrix_size = (256, 256), sparsity = 0.5

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    @pytest.mark.parametrize("sparsity", SPARSITY_LEVELS)
    def test_fast_path_optimization(
        algorithm_optimizer: AlgorithmOptimizer,
        matrix_size: Tuple[int, int],
        sparsity: float,
    ) -> None:
        """Test fast path optimizations for sparse operations."""
        with resource_guard():
            # Generate sparse matrices
            matrix_a = generate_sparse_matrix(matrix_size, sparsity)
            matrix_b = generate_sparse_matrix(matrix_size, sparsity)
    
            # Register fast path for sparse matrix multiplication
            def is_sparse(x: torch.Tensor, threshold: float = 0.5) -> bool:
                return (torch.count_nonzero(x).item() / x.numel()) < threshold
    
            # Register optimized path
            algorithm_optimizer.register_fast_path(
                "sparse_matmul",
                lambda x, y: torch.sparse.mm(x.to_sparse(), y.to_sparse()).to_dense(),
                condition=lambda x, y: is_sparse(x) and is_sparse(y),
            )
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("sparse_matmul", matrix_a, matrix_b)

tests/performance/cpu/test_algorithms.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173ebce0>
operation_name = 'sparse_matmul'
args = (tensor([[-1.6614,  0.0000,  0.0000,  ..., -0.6831, -0.0000, -1.5015],
        [ 0.1433,  0.2235,  0.0000,  ...,  0.34...267, -0.0000,  ..., -0.0000, -0.0045, -0.0000],
        [-1.3861, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]]))
kwargs = {}

    def optimize_operation(self, operation_name: str, *args, **kwargs) -> Any:
        """Execute an optimized operation."""
        if operation_name not in self.operations:
>           raise ValueError(f"Operation {operation_name} not registered")
E           ValueError: Operation sparse_matmul not registered

src/core/performance/cpu/algorithms.py:228: ValueError
________________ test_fast_path_optimization[0.9-matrix_size0] _________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173eb6b0>
matrix_size = (64, 64), sparsity = 0.9

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    @pytest.mark.parametrize("sparsity", SPARSITY_LEVELS)
    def test_fast_path_optimization(
        algorithm_optimizer: AlgorithmOptimizer,
        matrix_size: Tuple[int, int],
        sparsity: float,
    ) -> None:
        """Test fast path optimizations for sparse operations."""
        with resource_guard():
            # Generate sparse matrices
            matrix_a = generate_sparse_matrix(matrix_size, sparsity)
            matrix_b = generate_sparse_matrix(matrix_size, sparsity)
    
            # Register fast path for sparse matrix multiplication
            def is_sparse(x: torch.Tensor, threshold: float = 0.5) -> bool:
                return (torch.count_nonzero(x).item() / x.numel()) < threshold
    
            # Register optimized path
            algorithm_optimizer.register_fast_path(
                "sparse_matmul",
                lambda x, y: torch.sparse.mm(x.to_sparse(), y.to_sparse()).to_dense(),
                condition=lambda x, y: is_sparse(x) and is_sparse(y),
            )
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("sparse_matmul", matrix_a, matrix_b)

tests/performance/cpu/test_algorithms.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173eb6b0>
operation_name = 'sparse_matmul'
args = (tensor([[-0.0000, -0.0000, -0.0000,  ...,  0.0788, -1.0241,  0.0000],
        [ 0.2752,  0.0000, -0.0000,  ...,  0.00...000, -0.0000,  ..., -0.0000,  1.4020,  0.0000],
        [ 0.0000, -1.7970, -2.2202,  ..., -0.0000, -0.0000, -0.0000]]))
kwargs = {}

    def optimize_operation(self, operation_name: str, *args, **kwargs) -> Any:
        """Execute an optimized operation."""
        if operation_name not in self.operations:
>           raise ValueError(f"Operation {operation_name} not registered")
E           ValueError: Operation sparse_matmul not registered

src/core/performance/cpu/algorithms.py:228: ValueError
________________ test_fast_path_optimization[0.9-matrix_size1] _________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173e8950>
matrix_size = (256, 256), sparsity = 0.9

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    @pytest.mark.parametrize("sparsity", SPARSITY_LEVELS)
    def test_fast_path_optimization(
        algorithm_optimizer: AlgorithmOptimizer,
        matrix_size: Tuple[int, int],
        sparsity: float,
    ) -> None:
        """Test fast path optimizations for sparse operations."""
        with resource_guard():
            # Generate sparse matrices
            matrix_a = generate_sparse_matrix(matrix_size, sparsity)
            matrix_b = generate_sparse_matrix(matrix_size, sparsity)
    
            # Register fast path for sparse matrix multiplication
            def is_sparse(x: torch.Tensor, threshold: float = 0.5) -> bool:
                return (torch.count_nonzero(x).item() / x.numel()) < threshold
    
            # Register optimized path
            algorithm_optimizer.register_fast_path(
                "sparse_matmul",
                lambda x, y: torch.sparse.mm(x.to_sparse(), y.to_sparse()).to_dense(),
                condition=lambda x, y: is_sparse(x) and is_sparse(y),
            )
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("sparse_matmul", matrix_a, matrix_b)

tests/performance/cpu/test_algorithms.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173e8950>
operation_name = 'sparse_matmul'
args = (tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],
        [ 0.0000, -0.0000, -0.5847,  ...,  0.00...166, -0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.4602,  0.0000]]))
kwargs = {}

    def optimize_operation(self, operation_name: str, *args, **kwargs) -> Any:
        """Execute an optimized operation."""
        if operation_name not in self.operations:
>           raise ValueError(f"Operation {operation_name} not registered")
E           ValueError: Operation sparse_matmul not registered

src/core/performance/cpu/algorithms.py:228: ValueError
____________________ test_branch_prediction[matrix_size0-1] ____________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173ebad0>
batch_size = 1, matrix_size = (64, 64)

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("batch_size", BATCH_SIZES)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    def test_branch_prediction(
        algorithm_optimizer: AlgorithmOptimizer,
        batch_size: int,
        matrix_size: Tuple[int, int],
    ) -> None:
        """Test branch prediction efficiency."""
        with resource_guard():
            # Create input tensors
            inputs = [torch.randn(matrix_size) for _ in range(batch_size)]
    
            # Define test operations
            def operation_a(x: torch.Tensor) -> torch.Tensor:
                return torch.relu(x)
    
            def operation_b(x: torch.Tensor) -> torch.Tensor:
                return torch.sigmoid(x)
    
            # Register operations
            algorithm_optimizer.register_operation("op_a", operation_a)
            algorithm_optimizer.register_operation("op_b", operation_b)
    
            # Warm-up run
            for x in inputs:
>               _ = algorithm_optimizer.optimize_operation(
                    "op_a" if torch.mean(x).item() > 0 else "op_b", x
                )

tests/performance/cpu/test_algorithms.py:145: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
___________________ test_branch_prediction[matrix_size0-16] ____________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173eb650>
batch_size = 16, matrix_size = (64, 64)

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("batch_size", BATCH_SIZES)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    def test_branch_prediction(
        algorithm_optimizer: AlgorithmOptimizer,
        batch_size: int,
        matrix_size: Tuple[int, int],
    ) -> None:
        """Test branch prediction efficiency."""
        with resource_guard():
            # Create input tensors
            inputs = [torch.randn(matrix_size) for _ in range(batch_size)]
    
            # Define test operations
            def operation_a(x: torch.Tensor) -> torch.Tensor:
                return torch.relu(x)
    
            def operation_b(x: torch.Tensor) -> torch.Tensor:
                return torch.sigmoid(x)
    
            # Register operations
            algorithm_optimizer.register_operation("op_a", operation_a)
            algorithm_optimizer.register_operation("op_b", operation_b)
    
            # Warm-up run
            for x in inputs:
>               _ = algorithm_optimizer.optimize_operation(
                    "op_a" if torch.mean(x).item() > 0 else "op_b", x
                )

tests/performance/cpu/test_algorithms.py:145: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
____________________ test_branch_prediction[matrix_size1-1] ____________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173eb680>
batch_size = 1, matrix_size = (256, 256)

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("batch_size", BATCH_SIZES)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    def test_branch_prediction(
        algorithm_optimizer: AlgorithmOptimizer,
        batch_size: int,
        matrix_size: Tuple[int, int],
    ) -> None:
        """Test branch prediction efficiency."""
        with resource_guard():
            # Create input tensors
            inputs = [torch.randn(matrix_size) for _ in range(batch_size)]
    
            # Define test operations
            def operation_a(x: torch.Tensor) -> torch.Tensor:
                return torch.relu(x)
    
            def operation_b(x: torch.Tensor) -> torch.Tensor:
                return torch.sigmoid(x)
    
            # Register operations
            algorithm_optimizer.register_operation("op_a", operation_a)
            algorithm_optimizer.register_operation("op_b", operation_b)
    
            # Warm-up run
            for x in inputs:
>               _ = algorithm_optimizer.optimize_operation(
                    "op_a" if torch.mean(x).item() > 0 else "op_b", x
                )

tests/performance/cpu/test_algorithms.py:145: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
___________________ test_branch_prediction[matrix_size1-16] ____________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173ea030>
batch_size = 16, matrix_size = (256, 256)

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("batch_size", BATCH_SIZES)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    def test_branch_prediction(
        algorithm_optimizer: AlgorithmOptimizer,
        batch_size: int,
        matrix_size: Tuple[int, int],
    ) -> None:
        """Test branch prediction efficiency."""
        with resource_guard():
            # Create input tensors
>           inputs = [torch.randn(matrix_size) for _ in range(batch_size)]
E           RuntimeError: [enforce fail at alloc_cpu.cpp:118] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 262144 bytes. Error code 12 (Cannot allocate memory)

tests/performance/cpu/test_algorithms.py:130: RuntimeError
__________________________ test_loop_optimization[O0] __________________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173e8140>
optimization_level = 'O0'

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("optimization_level", OPTIMIZATION_LEVELS)
    def test_loop_optimization(
        algorithm_optimizer: AlgorithmOptimizer, optimization_level: str
    ):
        """Test loop optimization strategies."""
        with resource_guard():
            size = 256  # Reduced from 1024
            matrix = torch.randn(size, size)
    
            # Configure optimization level
            algorithm_optimizer.set_optimization_level(optimization_level)
    
            # Define test operation with loops
            def loop_operation(x: torch.Tensor) -> torch.Tensor:
                result = torch.zeros_like(x)
                for i in range(x.shape[0]):
                    for j in range(x.shape[1]):
                        result[i, j] = torch.tanh(x[i, j])
                return result
    
            # Register operation
            algorithm_optimizer.register_operation("loop_op", loop_operation)
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("loop_op", matrix)

tests/performance/cpu/test_algorithms.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
__________________________ test_loop_optimization[O1] __________________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x7055173e9b50>
optimization_level = 'O1'

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("optimization_level", OPTIMIZATION_LEVELS)
    def test_loop_optimization(
        algorithm_optimizer: AlgorithmOptimizer, optimization_level: str
    ):
        """Test loop optimization strategies."""
        with resource_guard():
            size = 256  # Reduced from 1024
            matrix = torch.randn(size, size)
    
            # Configure optimization level
            algorithm_optimizer.set_optimization_level(optimization_level)
    
            # Define test operation with loops
            def loop_operation(x: torch.Tensor) -> torch.Tensor:
                result = torch.zeros_like(x)
                for i in range(x.shape[0]):
                    for j in range(x.shape[1]):
                        result[i, j] = torch.tanh(x[i, j])
                return result
    
            # Register operation
            algorithm_optimizer.register_operation("loop_op", loop_operation)
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("loop_op", matrix)

tests/performance/cpu/test_algorithms.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
__________________________ test_loop_optimization[O2] __________________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x705516b1bcb0>
optimization_level = 'O2'

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("optimization_level", OPTIMIZATION_LEVELS)
    def test_loop_optimization(
        algorithm_optimizer: AlgorithmOptimizer, optimization_level: str
    ):
        """Test loop optimization strategies."""
        with resource_guard():
            size = 256  # Reduced from 1024
            matrix = torch.randn(size, size)
    
            # Configure optimization level
            algorithm_optimizer.set_optimization_level(optimization_level)
    
            # Define test operation with loops
            def loop_operation(x: torch.Tensor) -> torch.Tensor:
                result = torch.zeros_like(x)
                for i in range(x.shape[0]):
                    for j in range(x.shape[1]):
                        result[i, j] = torch.tanh(x[i, j])
                return result
    
            # Register operation
            algorithm_optimizer.register_operation("loop_op", loop_operation)
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("loop_op", matrix)

tests/performance/cpu/test_algorithms.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
__________________________ test_loop_optimization[O3] __________________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x705516b1bc50>
optimization_level = 'O3'

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("optimization_level", OPTIMIZATION_LEVELS)
    def test_loop_optimization(
        algorithm_optimizer: AlgorithmOptimizer, optimization_level: str
    ):
        """Test loop optimization strategies."""
        with resource_guard():
            size = 256  # Reduced from 1024
            matrix = torch.randn(size, size)
    
            # Configure optimization level
            algorithm_optimizer.set_optimization_level(optimization_level)
    
            # Define test operation with loops
            def loop_operation(x: torch.Tensor) -> torch.Tensor:
                result = torch.zeros_like(x)
                for i in range(x.shape[0]):
                    for j in range(x.shape[1]):
                        result[i, j] = torch.tanh(x[i, j])
                return result
    
            # Register operation
            algorithm_optimizer.register_operation("loop_op", loop_operation)
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("loop_op", matrix)

tests/performance/cpu/test_algorithms.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
____________________ test_numerical_stability[matrix_size0] ____________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x705516b18650>
matrix_size = (64, 64)

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    def test_numerical_stability(
        algorithm_optimizer: AlgorithmOptimizer, matrix_size: tuple[int, int]
    ):
        """Test numerical stability of optimized computations."""
        with resource_guard():
            # Generate test matrix
            matrix = torch.randn(matrix_size)
    
            # Define numerically sensitive operation
            def sensitive_operation(x: torch.Tensor) -> torch.Tensor:
                return torch.log1p(torch.exp(x))  # LogSumExp
    
            # Register operation
            algorithm_optimizer.register_operation("sensitive_op", sensitive_operation)
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("sensitive_op", matrix)

tests/performance/cpu/test_algorithms.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
____________________ test_numerical_stability[matrix_size1] ____________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x705516b1a180>
matrix_size = (256, 256)

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("matrix_size", MATRIX_SIZES)
    def test_numerical_stability(
        algorithm_optimizer: AlgorithmOptimizer, matrix_size: tuple[int, int]
    ):
        """Test numerical stability of optimized computations."""
        with resource_guard():
            # Generate test matrix
            matrix = torch.randn(matrix_size)
    
            # Define numerically sensitive operation
            def sensitive_operation(x: torch.Tensor) -> torch.Tensor:
                return torch.log1p(torch.exp(x))  # LogSumExp
    
            # Register operation
            algorithm_optimizer.register_operation("sensitive_op", sensitive_operation)
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("sensitive_op", matrix)

tests/performance/cpu/test_algorithms.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
__________________________ test_optimization_overhead __________________________

algorithm_optimizer = <src.core.performance.cpu.algorithms.AlgorithmOptimizer object at 0x705516b1b050>

    @pytest.mark.benchmark(min_rounds=5)
    def test_optimization_overhead(algorithm_optimizer: AlgorithmOptimizer):
        """Test overhead of optimization techniques."""
        with resource_guard():
            size = 256  # Reduced from 512
            matrix = torch.randn(size, size)
    
            def simple_operation(x: torch.Tensor) -> torch.Tensor:
                return torch.relu(x)
    
            # Register operation
            algorithm_optimizer.register_operation("simple_op", simple_operation)
    
            # Measure baseline time
            start_time = time.perf_counter()
            baseline_result = simple_operation(matrix)
            baseline_time = time.perf_counter() - start_time
    
            # Warm-up run
>           _ = algorithm_optimizer.optimize_operation("simple_op", matrix)

tests/performance/cpu/test_algorithms.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu/algorithms.py:229: in optimize_operation
    return self.operations[operation_name](*args, **kwargs)
src/core/performance/cpu/algorithms.py:277: in wrapper
    start_time = torch.cuda.Event(enable_timing=True)
venv/lib/python3.12/site-packages/torch/cuda/streams.py:164: in __new__
    return super().__new__(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

obj = <class 'torch.cuda.streams.Event'>, args = ()
kwargs = {'blocking': False, 'enable_timing': True, 'interprocess': False}
class_name = 'Event'

    def err_fn(obj, *args, **kwargs):
        if is_init:
            class_name = obj.__class__.__name__
        else:
            class_name = obj.__name__
>       raise RuntimeError(f"Tried to instantiate dummy base class {class_name}")
E       RuntimeError: Tried to instantiate dummy base class Event

venv/lib/python3.12/site-packages/torch/_utils.py:960: RuntimeError
_____________________ test_memory_pool_efficiency[32-1024] _____________________

memory_manager = <src.core.performance.cpu.memory_management.MemoryManager object at 0x705516b18da0>
pool_size = 1024, block_size = 32

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("pool_size", POOL_SIZES)
    @pytest.mark.parametrize("block_size", BLOCK_SIZES)
    def test_memory_pool_efficiency(
        memory_manager: MemoryManager, pool_size: int, block_size: int
    ) -> None:
        """Test memory pool allocation and deallocation efficiency."""
        with resource_guard():
            # Allocate blocks
            blocks = []
            for _i in range(pool_size // block_size):
                block = memory_manager.allocate_tensor((block_size,))
                blocks.append(block)
    
            # Get metrics after allocation
            allocated_memory = memory_manager.get_allocated_memory()
            fragmentation_ratio = memory_manager.get_fragmentation_ratio()
    
            # Verify allocation efficiency
            assert allocated_memory <= pool_size * 1024  # Convert KB to bytes
            assert fragmentation_ratio < 0.2  # Less than 20% fragmentation
    
            # Deallocate in reverse order
            for block in reversed(blocks):
                del block
            gc.collect()
    
            # Get metrics after deallocation
            allocated_memory = memory_manager.get_allocated_memory()
            fragmentation_ratio = memory_manager.get_fragmentation_ratio()
    
            # Verify cleanup
>           assert allocated_memory == 0
E           assert 4096 == 0

tests/performance/cpu/test_memory.py:120: AssertionError
_____________________ test_memory_pool_efficiency[32-4096] _____________________

memory_manager = <src.core.performance.cpu.memory_management.MemoryManager object at 0x705516b19d60>
pool_size = 4096, block_size = 32

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("pool_size", POOL_SIZES)
    @pytest.mark.parametrize("block_size", BLOCK_SIZES)
    def test_memory_pool_efficiency(
        memory_manager: MemoryManager, pool_size: int, block_size: int
    ) -> None:
        """Test memory pool allocation and deallocation efficiency."""
        with resource_guard():
            # Allocate blocks
            blocks = []
            for _i in range(pool_size // block_size):
                block = memory_manager.allocate_tensor((block_size,))
                blocks.append(block)
    
            # Get metrics after allocation
            allocated_memory = memory_manager.get_allocated_memory()
            fragmentation_ratio = memory_manager.get_fragmentation_ratio()
    
            # Verify allocation efficiency
            assert allocated_memory <= pool_size * 1024  # Convert KB to bytes
            assert fragmentation_ratio < 0.2  # Less than 20% fragmentation
    
            # Deallocate in reverse order
            for block in reversed(blocks):
                del block
            gc.collect()
    
            # Get metrics after deallocation
            allocated_memory = memory_manager.get_allocated_memory()
            fragmentation_ratio = memory_manager.get_fragmentation_ratio()
    
            # Verify cleanup
>           assert allocated_memory == 0
E           assert 16384 == 0

tests/performance/cpu/test_memory.py:120: AssertionError
____________________ test_memory_pool_efficiency[128-1024] _____________________

memory_manager = <src.core.performance.cpu.memory_management.MemoryManager object at 0x705516b19580>
pool_size = 1024, block_size = 128

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("pool_size", POOL_SIZES)
    @pytest.mark.parametrize("block_size", BLOCK_SIZES)
    def test_memory_pool_efficiency(
        memory_manager: MemoryManager, pool_size: int, block_size: int
    ) -> None:
        """Test memory pool allocation and deallocation efficiency."""
        with resource_guard():
            # Allocate blocks
            blocks = []
            for _i in range(pool_size // block_size):
                block = memory_manager.allocate_tensor((block_size,))
                blocks.append(block)
    
            # Get metrics after allocation
            allocated_memory = memory_manager.get_allocated_memory()
            fragmentation_ratio = memory_manager.get_fragmentation_ratio()
    
            # Verify allocation efficiency
            assert allocated_memory <= pool_size * 1024  # Convert KB to bytes
            assert fragmentation_ratio < 0.2  # Less than 20% fragmentation
    
            # Deallocate in reverse order
            for block in reversed(blocks):
                del block
            gc.collect()
    
            # Get metrics after deallocation
            allocated_memory = memory_manager.get_allocated_memory()
            fragmentation_ratio = memory_manager.get_fragmentation_ratio()
    
            # Verify cleanup
>           assert allocated_memory == 0
E           assert 4096 == 0

tests/performance/cpu/test_memory.py:120: AssertionError
____________________ test_memory_pool_efficiency[128-4096] _____________________

memory_manager = <src.core.performance.cpu.memory_management.MemoryManager object at 0x705516b1b830>
pool_size = 4096, block_size = 128

    @pytest.mark.benchmark(min_rounds=5)
    @pytest.mark.parametrize("pool_size", POOL_SIZES)
    @pytest.mark.parametrize("block_size", BLOCK_SIZES)
    def test_memory_pool_efficiency(
        memory_manager: MemoryManager, pool_size: int, block_size: int
    ) -> None:
        """Test memory pool allocation and deallocation efficiency."""
        with resource_guard():
            # Allocate blocks
            blocks = []
            for _i in range(pool_size // block_size):
                block = memory_manager.allocate_tensor((block_size,))
                blocks.append(block)
    
            # Get metrics after allocation
            allocated_memory = memory_manager.get_allocated_memory()
            fragmentation_ratio = memory_manager.get_fragmentation_ratio()
    
            # Verify allocation efficiency
            assert allocated_memory <= pool_size * 1024  # Convert KB to bytes
            assert fragmentation_ratio < 0.2  # Less than 20% fragmentation
    
            # Deallocate in reverse order
            for block in reversed(blocks):
                del block
            gc.collect()
    
            # Get metrics after deallocation
            allocated_memory = memory_manager.get_allocated_memory()
            fragmentation_ratio = memory_manager.get_fragmentation_ratio()
    
            # Verify cleanup
>           assert allocated_memory == 0
E           assert 16384 == 0

tests/performance/cpu/test_memory.py:120: AssertionError
____________________________ test_resource_cleanup _____________________________

memory_manager = <src.core.performance.cpu.memory_management.MemoryManager object at 0x705516b1ae10>

    @pytest.mark.benchmark(min_rounds=5)
    def test_resource_cleanup(memory_manager: MemoryManager) -> None:
        """Test proper cleanup of memory resources."""
        with resource_guard():
            pool_size = 1024  # KB
            block_size = 32  # KB
            num_blocks = pool_size // block_size
    
            # Allocate all blocks
            blocks = []
            for _ in range(num_blocks):
                block = memory_manager.allocate_tensor((block_size,))
                blocks.append(block)
    
            # Delete half the blocks
            for i in range(0, len(blocks), 2):
>               del blocks[i]
E               IndexError: list assignment index out of range

tests/performance/cpu/test_memory.py:234: IndexError
__________________ TestScaleCohomology.test_scale_connection ___________________

self = <test_scale.TestScaleCohomology object at 0x705544040140>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x7055161b6060>
space_dim = 4

    def test_scale_connection(self, scale_system, space_dim):
        """Test scale connection properties."""
        # Create test scales
        scale1, scale2 = torch.tensor(1.0), torch.tensor(2.0)
    
        # Compute connection
>       connection = scale_system.scale_connection(scale1, scale2)
E       AttributeError: 'ScaleCohomology' object has no attribute 'scale_connection'

tests/test_core/test_crystal/test_scale.py:46: AttributeError
________________ TestScaleCohomology.test_renormalization_flow _________________

self = <test_scale.TestScaleCohomology object at 0x705544040920>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x7055161b46b0>

    def test_renormalization_flow(self, scale_system):
        """Test renormalization group flow properties."""
    
        # Create test observable
        def test_observable(x: torch.Tensor) -> torch.Tensor:
            return torch.sum(x**2)
    
        # Compute RG flow
>       flow = scale_system.renormalization_flow(test_observable)
E       AttributeError: 'ScaleCohomology' object has no attribute 'renormalization_flow'

tests/test_core/test_crystal/test_scale.py:87: AttributeError
____________________ TestScaleCohomology.test_fixed_points _____________________

self = <test_scale.TestScaleCohomology object at 0x705544040bc0>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x7055163e2a80>

    def test_fixed_points(self, scale_system):
        """Test fixed point analysis."""
    
        # Create test flow
        def beta_function(x):
            """Simple β-function with known fixed point."""
            return x * (1 - x)
    
        # Find fixed points
>       fixed_points = scale_system.fixed_points(beta_function)
E       AttributeError: 'ScaleCohomology' object has no attribute 'fixed_points'

tests/test_core/test_crystal/test_scale.py:122: AttributeError
_________________ TestScaleCohomology.test_anomaly_polynomial __________________

self = <test_scale.TestScaleCohomology object at 0x705544040dd0>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x7055163e1550>

    def test_anomaly_polynomial(self, scale_system):
        """Test anomaly polynomial computation."""
    
        # Create test symmetry
        def symmetry_action(x):
            """Simple U(1) symmetry."""
            return torch.exp(1j * x)
    
        # Compute anomaly
>       anomaly = scale_system.anomaly_polynomial(symmetry_action)
E       AttributeError: 'ScaleCohomology' object has no attribute 'anomaly_polynomial'

tests/test_core/test_crystal/test_scale.py:163: AttributeError
__________________ TestScaleCohomology.test_scale_invariants ___________________

self = <test_scale.TestScaleCohomology object at 0x705544040fe0>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x7055161b7590>

    def test_scale_invariants(self, scale_system):
        """Test scale invariant quantity computation."""
        # Create test structure
        structure = torch.randn(10, 10)
    
        # Compute invariants
>       invariants = scale_system.scale_invariants(structure)
E       AttributeError: 'ScaleCohomology' object has no attribute 'scale_invariants'

tests/test_core/test_crystal/test_scale.py:190: AttributeError
___________________ TestScaleCohomology.test_callan_symanzik ___________________

self = <test_scale.TestScaleCohomology object at 0x705544041190>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x7055161657c0>
space_dim = 4

    def test_callan_symanzik(self, scale_system, space_dim):
        """Test Callan-Symanzik equation properties."""
    
        # Create test correlation function
        def correlation(x1, x2, coupling):
            """Simple two-point correlation function."""
            return torch.exp(-coupling * torch.norm(x1 - x2))
    
        # Define beta function
        def beta(g):
            """Simple beta function."""
            return -(g**2)
    
        # Define anomalous dimension
        def gamma(g):
            """Anomalous dimension."""
            return g**2 / (4 * np.pi) ** 2
    
        # Test points
        x1 = torch.zeros(space_dim)
        x2 = torch.ones(space_dim)
        g = torch.tensor(0.1)  # coupling
    
        # Compute CS equation terms
>       cs_operator = scale_system.callan_symanzik_operator(beta, gamma)
E       AttributeError: 'ScaleCohomology' object has no attribute 'callan_symanzik_operator'

tests/test_core/test_crystal/test_scale.py:248: AttributeError
_________________ TestScaleCohomology.test_operator_expansion __________________

self = <test_scale.TestScaleCohomology object at 0x7055440413a0>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x705516159010>

    def test_operator_expansion(self, scale_system):
        """Test operator product expansion."""
    
        # Create test operators
        def op1(x):
            """First operator."""
            return torch.sin(x)
    
        def op2(x):
            """Second operator."""
            return torch.exp(-(x**2))
    
        # Compute OPE
>       ope = scale_system.operator_product_expansion(op1, op2)
E       AttributeError: 'ScaleCohomology' object has no attribute 'operator_product_expansion'

tests/test_core/test_crystal/test_scale.py:281: AttributeError
_________________ TestScaleCohomology.test_conformal_symmetry __________________

self = <test_scale.TestScaleCohomology object at 0x705544041550>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x7055161cfb30>
space_dim = 4

    def test_conformal_symmetry(self, scale_system, space_dim):
        """Test conformal symmetry properties."""
        # Create test field and state
        field = torch.randn(10, 10)
        state = torch.randn(10, space_dim)  # Define state for mutual information tests
    
        # Test special conformal transformations
        def test_special_conformal(b_vector):
            """Test special conformal transformation."""
            x = torch.randn(space_dim)
            scale_system.special_conformal_transform(x, b_vector)
            # Should preserve angles
            v1 = torch.randn(space_dim)
            v2 = torch.randn(space_dim)
            angle1 = torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))
            transformed_v1 = scale_system.transform_vector(v1, x, b_vector)
            transformed_v2 = scale_system.transform_vector(v2, x, b_vector)
            angle2 = torch.dot(transformed_v1, transformed_v2) / (
                torch.norm(transformed_v1) * torch.norm(transformed_v2)
            )
            return torch.allclose(angle1, angle2, rtol=1e-3)
    
>       assert test_special_conformal(
            torch.ones(space_dim)
        ), "Special conformal transformation should preserve angles"

tests/test_core/test_crystal/test_scale.py:333: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

b_vector = tensor([1., 1., 1., 1.])

    def test_special_conformal(b_vector):
        """Test special conformal transformation."""
        x = torch.randn(space_dim)
>       scale_system.special_conformal_transform(x, b_vector)
E       AttributeError: 'ScaleCohomology' object has no attribute 'special_conformal_transform'

tests/test_core/test_crystal/test_scale.py:321: AttributeError
_________________ TestScaleCohomology.test_holographic_scaling _________________

self = <test_scale.TestScaleCohomology object at 0x705544041700>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x7055163e0b60>
space_dim = 4

    def test_holographic_scaling(self, scale_system, space_dim):
        """Test holographic scaling relations."""
        # Create bulk and boundary data
        boundary_field = torch.randn(10, 10)
        radial_coordinate = torch.linspace(0.1, 10.0, 50)
    
        # Test radial evolution
>       bulk_field = scale_system.holographic_lift(boundary_field, radial_coordinate)
E       AttributeError: 'ScaleCohomology' object has no attribute 'holographic_lift'

tests/test_core/test_crystal/test_scale.py:373: AttributeError
________________ TestScaleCohomology.test_entanglement_scaling _________________

self = <test_scale.TestScaleCohomology object at 0x7055440418b0>
scale_system = <src.core.crystal.scale.ScaleCohomology object at 0x7055163e27b0>
space_dim = 4

    def test_entanglement_scaling(self, scale_system, space_dim):
        """Test entanglement entropy scaling."""
        # Create test state
        state = torch.randn(32, 32)  # Lattice state
    
        # Test area law
        def test_area_law(region_sizes):
            """Test area law scaling of entanglement."""
            entropies = []
            areas = []
            for size in region_sizes:
                region = torch.ones(size, size)
                entropy = scale_system.entanglement_entropy(state, region)
                area = 4 * size  # Perimeter of square region
                entropies.append(entropy)
                areas.append(area)
    
            # Fit to area law S = α A + β
            coeffs = np.polyfit(areas, entropies, 1)
            return coeffs[0] > 0  # α should be positive
    
        sizes = [2, 4, 6, 8]
>       assert test_area_law(sizes), "Should satisfy area law scaling"

tests/test_core/test_crystal/test_scale.py:421: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

region_sizes = [2, 4, 6, 8]

    def test_area_law(region_sizes):
        """Test area law scaling of entanglement."""
        entropies = []
        areas = []
        for size in region_sizes:
            region = torch.ones(size, size)
>           entropy = scale_system.entanglement_entropy(state, region)
E           AttributeError: 'ScaleCohomology' object has no attribute 'entanglement_entropy'

tests/test_core/test_crystal/test_scale.py:411: AttributeError
_____________ TestConnectionFormHypothesis.test_linearity_property _____________

self = <test_fiber_bundle.TestConnectionFormHypothesis object at 0x705543f17bc0>

    @given(
>       st.integers(min_value=2, max_value=5),  # base_dim
        st.integers(min_value=2, max_value=5),  # fiber_dim
        st.integers(min_value=1, max_value=5),  # batch_size
    )

tests/test_core/test_patterns/test_fiber_bundle.py:812: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<test_fiber_bundle.TestConnectionFormHypothesis object at 0x705543f17bc0>, 3, 5, 3)
kwargs = {}, arg_drawtime = 0.00013124099496053532, arg_stateful = 0.0
arg_gctime = 0.01655927197134588, start = 60530.869150235, result = None
finish = 60531.101924344, in_drawtime = 0.0, in_stateful = 0.0
in_gctime = 0.000507369011756964, runtime = 0.23226673999306513

    @proxies(self.test)
    def test(*args, **kwargs):
        arg_drawtime = math.fsum(data.draw_times.values())
        arg_stateful = math.fsum(data._stateful_run_times.values())
        arg_gctime = gc_cumulative_time()
        start = time.perf_counter()
        try:
            with unwrap_markers_from_group(), ensure_free_stackframes():
                result = self.test(*args, **kwargs)
        finally:
            finish = time.perf_counter()
            in_drawtime = math.fsum(data.draw_times.values()) - arg_drawtime
            in_stateful = (
                math.fsum(data._stateful_run_times.values()) - arg_stateful
            )
            in_gctime = gc_cumulative_time() - arg_gctime
            runtime = finish - start - in_drawtime - in_stateful - in_gctime
            self._timing_features = {
                "execute:test": runtime,
                "overall:gc": in_gctime,
                **data.draw_times,
                **data._stateful_run_times,
            }
    
        if (current_deadline := self.settings.deadline) is not None:
            if not is_final:
                current_deadline = (current_deadline // 4) * 5
            if runtime >= current_deadline.total_seconds():
>               raise DeadlineExceeded(
                    datetime.timedelta(seconds=runtime), self.settings.deadline
                )
E               hypothesis.errors.DeadlineExceeded: Test took 232.27ms, which exceeds the deadline of 200.00ms
E               Falsifying example: test_linearity_property(
E                   self=<test_fiber_bundle.TestConnectionFormHypothesis object at 0x705543f17bc0>,
E                   base_dim=3,
E                   fiber_dim=5,
E                   batch_size=3,
E               )

venv/lib/python3.12/site-packages/hypothesis/core.py:910: DeadlineExceeded
_______ TestConnectionFormHypothesis.test_levi_civita_symmetry_property ________

self = <test_fiber_bundle.TestConnectionFormHypothesis object at 0x705543f17d40>

    @given(
>       st.integers(min_value=2, max_value=5),  # base_dim
        st.integers(min_value=2, max_value=5),  # fiber_dim
        st.integers(min_value=1, max_value=5),  # batch_size
    )

tests/test_core/test_patterns/test_fiber_bundle.py:844: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<test_fiber_bundle.TestConnectionFormHypothesis object at 0x705543f17d40>, 4, 3, 1)
kwargs = {}, arg_drawtime = 0.0001516109987278469, arg_stateful = 0.0
arg_gctime = 0.021836976979102474, start = 60531.50975076, result = None
finish = 60531.831883593, in_drawtime = 0.0, in_stateful = 0.0
in_gctime = 0.000570805997995194, runtime = 0.3215620269984356

    @proxies(self.test)
    def test(*args, **kwargs):
        arg_drawtime = math.fsum(data.draw_times.values())
        arg_stateful = math.fsum(data._stateful_run_times.values())
        arg_gctime = gc_cumulative_time()
        start = time.perf_counter()
        try:
            with unwrap_markers_from_group(), ensure_free_stackframes():
                result = self.test(*args, **kwargs)
        finally:
            finish = time.perf_counter()
            in_drawtime = math.fsum(data.draw_times.values()) - arg_drawtime
            in_stateful = (
                math.fsum(data._stateful_run_times.values()) - arg_stateful
            )
            in_gctime = gc_cumulative_time() - arg_gctime
            runtime = finish - start - in_drawtime - in_stateful - in_gctime
            self._timing_features = {
                "execute:test": runtime,
                "overall:gc": in_gctime,
                **data.draw_times,
                **data._stateful_run_times,
            }
    
        if (current_deadline := self.settings.deadline) is not None:
            if not is_final:
                current_deadline = (current_deadline // 4) * 5
            if runtime >= current_deadline.total_seconds():
>               raise DeadlineExceeded(
                    datetime.timedelta(seconds=runtime), self.settings.deadline
                )
E               hypothesis.errors.DeadlineExceeded: Test took 321.56ms, which exceeds the deadline of 200.00ms
E               Falsifying example: test_levi_civita_symmetry_property(
E                   self=<test_fiber_bundle.TestConnectionFormHypothesis object at 0x705543f17d40>,
E                   base_dim=4,
E                   fiber_dim=3,
E                   batch_size=1,
E               )

venv/lib/python3.12/site-packages/hypothesis/core.py:910: DeadlineExceeded
_______________ TestGeometricComponents.test_metric_derivatives ________________

self = <test_fiber_bundle.TestGeometricComponents object at 0x705543f484d0>
pattern_bundle = PatternFiberBundle()
test_config = {'euclidean_tests': {'dimensions': 2, 'test_batch_size': 1}, 'geometric_tests': {'batch_size': 1, 'dimensions': 2, 'dt...none', 'ram_gb': 4}}, 'hyperbolic_tests': {'precision': 'float32', 'test_norms': [0.01], 'vector_scales': [0.01]}, ...}

    def test_metric_derivatives(self, pattern_bundle, test_config):
        """Test computation of metric derivatives.
    
        Verifies:
        1. Shape correctness
        2. Symmetry properties
        3. Consistency with finite differences
        """
        # Get test dimensions
        base_dim = pattern_bundle.base_dim
        fiber_dim = pattern_bundle.fiber_dim
        total_dim = base_dim + fiber_dim
    
        # Create test point
        point = torch.randn(total_dim, requires_grad=True)
    
        # Get metric at point - ensure it's computed at the point
        metric = pattern_bundle.compute_metric(point.unsqueeze(0)).values[0]
    
        print("\nInitial metric:")
        print(metric)
    
        # Compute derivatives using autograd
        metric_derivs = []
        for i in range(total_dim):
            for j in range(total_dim):
                deriv = torch.autograd.grad(
                    metric[i, j],
                    point,
                    create_graph=True,
                    retain_graph=True
                )[0]
                metric_derivs.append(deriv)
        metric_derivs = torch.stack(metric_derivs).reshape(total_dim, total_dim, total_dim)
    
        # Verify shape
        assert metric_derivs.shape == (total_dim, total_dim, total_dim), \
            "Metric derivatives should have shape (total_dim, total_dim, total_dim)"
    
        # Verify symmetry in last two indices (metric symmetry)
        for i in range(total_dim):
            print(f"\nDerivatives for i={i}:")
            print("Original:")
            print(metric_derivs[i])
            print("Transposed:")
            print(metric_derivs[i].transpose(-2, -1))
            print("Difference:")
            print(metric_derivs[i] - metric_derivs[i].transpose(-2, -1))
    
>           assert torch.allclose(
                metric_derivs[i],
                metric_derivs[i].transpose(-2, -1),
                rtol=1e-5
            ), f"Metric derivatives should be symmetric in last two indices for i={i}"
E           AssertionError: Metric derivatives should be symmetric in last two indices for i=2
E           assert False
E            +  where False = <built-in method allclose of type object at 0x70560ced8b20>(tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1667,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.1755, -0.0833,  0.0000],\n        [ 0.0000,  0.0000,  0.0188,  0.0000, -0.0833]],\n       grad_fn=<SelectBackward0>), tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1667,  0.1755,  0.0188],\n        [ 0.0000,  0.0000,  0.0000, -0.0833,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0833]],\n       grad_fn=<TransposeBackward0>), rtol=1e-05)
E            +    where <built-in method allclose of type object at 0x70560ced8b20> = torch.allclose
E            +    and   tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1667,  0.1755,  0.0188],\n        [ 0.0000,  0.0000,  0.0000, -0.0833,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0833]],\n       grad_fn=<TransposeBackward0>) = <built-in method transpose of Tensor object at 0x7054fc2ad860>(-2, -1)
E            +      where <built-in method transpose of Tensor object at 0x7054fc2ad860> = tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1667,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.1755, -0.0833,  0.0000],\n        [ 0.0000,  0.0000,  0.0188,  0.0000, -0.0833]],\n       grad_fn=<SelectBackward0>).transpose

tests/test_core/test_patterns/test_fiber_bundle.py:1128: AssertionError
_________________________ test_christoffel_properties __________________________

riemannian_structure = BaseRiemannianStructure()
test_points = tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]], dtype=torch.float64)

    def test_christoffel_properties(riemannian_structure, test_points):
        """Test that Christoffel symbols satisfy required properties."""
>       christoffel = riemannian_structure.compute_christoffel(test_points)

tests/test_core/test_patterns/test_riemannian.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/patterns/riemannian.py:264: in compute_christoffel
    self.cache[f'metric_deriv_{k}'] = torch.autograd.grad(
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:496: in grad
    result = _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(3.0013, dtype=torch.float64, grad_fn=<SumBackward0>),)
args = ((tensor(1., dtype=torch.float64),), True, True, (tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]], dtype=torch.float64, requires_grad=True),), False)
kwargs = {'accumulate_grad': False}, attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
___________________________ test_parallel_transport ____________________________

riemannian_structure = BaseRiemannianStructure()

    def test_parallel_transport(riemannian_structure):
        """Test parallel transport preserves inner product."""
        # Create a simple path
        t = torch.linspace(0, 1, 10, dtype=torch.float64)
        path = torch.stack([
            torch.cos(2 * torch.pi * t),
            torch.sin(2 * torch.pi * t),
            torch.zeros_like(t)
        ], dim=1)
    
        # Initial vector
        vector = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float64)
    
        # Perform parallel transport
>       transported = riemannian_structure.parallel_transport(vector, path)

tests/test_core/test_patterns/test_riemannian.py:87: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/patterns/riemannian.py:326: in parallel_transport
    connection = self.compute_christoffel(path)
src/core/patterns/riemannian.py:264: in compute_christoffel
    self.cache[f'metric_deriv_{k}'] = torch.autograd.grad(
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:496: in grad
    result = _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(10.0015, dtype=torch.float64, grad_fn=<SumBackward0>),)
args = ((tensor(1., dtype=torch.float64),), True, True, (tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00],
        [ 7.6604e-0....0000e+00],
        [ 1.0000e+00, -2.4493e-16,  0.0000e+00]], dtype=torch.float64,
       requires_grad=True),), False)
kwargs = {'accumulate_grad': False}, attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
__________________________ test_curvature_identities ___________________________

riemannian_structure = BaseRiemannianStructure()
test_points = tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]], dtype=torch.float64)

    def test_curvature_identities(riemannian_structure, test_points):
        """Test that curvature tensor satisfies Bianchi identities."""
>       curvature = riemannian_structure.compute_curvature(test_points)

tests/test_core/test_patterns/test_riemannian.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/patterns/riemannian.py:362: in compute_curvature
    christoffel = self.compute_christoffel(points)
src/core/patterns/riemannian.py:264: in compute_christoffel
    self.cache[f'metric_deriv_{k}'] = torch.autograd.grad(
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:496: in grad
    result = _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(3.0012, dtype=torch.float64, grad_fn=<SumBackward0>),)
args = ((tensor(1., dtype=torch.float64),), True, True, (tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]], dtype=torch.float64, requires_grad=True),), False)
kwargs = {'accumulate_grad': False}, attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
____________________________ test_geodesic_equation ____________________________

riemannian_structure = BaseRiemannianStructure()

    def test_geodesic_equation(riemannian_structure):
        """Test that geodesic flow satisfies the geodesic equation."""
        # Initial conditions
        initial_point = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float64)
        initial_velocity = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float64)
    
        # Compute geodesic
>       points, velocities = riemannian_structure.geodesic_flow(
            initial_point,
            initial_velocity,
            steps=100,
            step_size=0.01
        )

tests/test_core/test_patterns/test_riemannian.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/patterns/riemannian.py:454: in geodesic_flow
    christoffel = self.compute_christoffel(points[t].unsqueeze(0))
src/core/patterns/riemannian.py:264: in compute_christoffel
    self.cache[f'metric_deriv_{k}'] = torch.autograd.grad(
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:496: in grad
    result = _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(1.0000, dtype=torch.float64, grad_fn=<SumBackward0>),)
args = ((tensor(1., dtype=torch.float64),), True, True, (tensor([[1., 0., 0.]], dtype=torch.float64, requires_grad=True),), False)
kwargs = {'accumulate_grad': False}, attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
___________________________ test_sectional_curvature ___________________________

riemannian_structure = BaseRiemannianStructure()
test_points = tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]], dtype=torch.float64)

    def test_sectional_curvature(riemannian_structure, test_points):
        """Test properties of sectional curvature."""
        # Create orthonormal vectors
        v1 = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float64)
        v2 = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float64)
    
        # Compute sectional curvature
>       K = riemannian_structure.sectional_curvature(test_points[0], v1, v2)

tests/test_core/test_patterns/test_riemannian.py:173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/patterns/riemannian.py:538: in sectional_curvature
    curvature = self.compute_curvature(point.unsqueeze(0))
src/core/patterns/riemannian.py:362: in compute_curvature
    christoffel = self.compute_christoffel(points)
src/core/patterns/riemannian.py:264: in compute_christoffel
    self.cache[f'metric_deriv_{k}'] = torch.autograd.grad(
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:496: in grad
    result = _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(1.0010, dtype=torch.float64, grad_fn=<SumBackward0>),)
args = ((tensor(1., dtype=torch.float64),), True, True, (tensor([[1., 0., 0.]], dtype=torch.float64, requires_grad=True),), False)
kwargs = {'accumulate_grad': False}, attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
_____________________________ test_lie_derivative ______________________________

riemannian_structure = BaseRiemannianStructure()
test_points = tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]], dtype=torch.float64)

    def test_lie_derivative(riemannian_structure, test_points):
        """Test properties of Lie derivative."""
        # Define a simple vector field
        def vector_field(x: Tensor) -> Tensor:
            return torch.tensor([
                -x[1],
                x[0],
                0.0
            ], dtype=torch.float64)
    
        # Compute Lie derivative
>       lie_deriv = riemannian_structure.lie_derivative_metric(
            test_points[0],
            vector_field
        )

tests/test_core/test_patterns/test_riemannian.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/patterns/riemannian.py:494: in lie_derivative_metric
    X_grad = torch.autograd.grad(
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:496: in grad
    result = _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(1., dtype=torch.float64),)
args = ((None,), True, True, (tensor([1., 0., 0.], dtype=torch.float64, requires_grad=True),), False)
kwargs = {'accumulate_grad': False}, attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
____________________ TestHilbertSpace.test_state_tomography ____________________

self = <test_state_space.TestHilbertSpace object at 0x705543f4a6c0>
hilbert_space = <src.core.quantum.state_space.HilbertSpace object at 0x7054fc276d80>
hilbert_dim = 4

    def test_state_tomography(self, hilbert_space, hilbert_dim):
        """Test quantum state tomography procedures."""
        # Create unknown test state
        true_state = hilbert_space.prepare_state(torch.randn(hilbert_dim * 2, dtype=torch.float64))
    
        # Generate Pauli basis measurements
        pauli_x = torch.tensor([[0, 1], [1, 0]], dtype=torch.complex128)
        pauli_y = torch.tensor([[0, -1j], [1j, 0]], dtype=torch.complex128)
        pauli_z = torch.tensor([[1, 0], [0, -1]], dtype=torch.complex128)
    
        # Perform tomographic measurements
        measurements = {
            "X": hilbert_space.measure_observable(true_state, pauli_x),
            "Y": hilbert_space.measure_observable(true_state, pauli_y),
            "Z": hilbert_space.measure_observable(true_state, pauli_z),
        }
    
        # Reconstruct state
        reconstructed_state = hilbert_space.reconstruct_state(measurements)
    
        # Test fidelity between true and reconstructed states
        fidelity = torch.abs(torch.vdot(true_state.amplitudes, reconstructed_state.amplitudes))**2
>       assert fidelity > 0.95, "Tomographic reconstruction should be accurate"
E       AssertionError: Tomographic reconstruction should be accurate
E       assert tensor(0., dtype=torch.float64) > 0.95

tests/test_core/test_quantum/test_state_space.py:229: AssertionError
____________________ TestHilbertSpace.test_geometric_phase _____________________

self = <test_state_space.TestHilbertSpace object at 0x705543f4aa20>
hilbert_space = <src.core.quantum.state_space.HilbertSpace object at 0x7054fc277260>
hilbert_dim = 4

    def test_geometric_phase(self, hilbert_space, hilbert_dim):
        """Test geometric (Berry) phase computation."""
    
        # Create cyclic evolution path
        def hamiltonian(t):
            theta = 2 * np.pi * t
            return torch.tensor(
                [[np.cos(theta), np.sin(theta)], [np.sin(theta), -np.cos(theta)]],
                dtype=torch.complex128,
            )
    
        # Initial state
        initial_state = hilbert_space.prepare_state(
            torch.tensor([1.0, 0.0], dtype=torch.float64)
        )
    
        # Test intermediate states
        times_debug = torch.tensor([0.0, 0.25, 0.5, 0.75, 1.0], dtype=torch.float64)
        for t in times_debug:
            H = hamiltonian(t.item())
            eigenvals, eigenvecs = torch.linalg.eigh(H)
            # Check eigenvalues are ±1
            assert torch.allclose(
                torch.sort(torch.abs(eigenvals))[0],
                torch.tensor([1.0, 1.0], dtype=torch.float64),
            ), f"Eigenvalues at t={t} should be ±1"
            # Check eigenvectors are orthonormal
            overlap = torch.abs(torch.vdot(eigenvecs[:, 0], eigenvecs[:, 1]))
            assert overlap < 1e-10, f"Eigenvectors at t={t} should be orthogonal"
    
        # Compute Berry phase
        times = torch.linspace(0, 1.0, 100, dtype=torch.float64)
        berry_phase = hilbert_space.compute_berry_phase(
            initial_state, hamiltonian, times
        )
    
        # Test phase is real
        assert torch.abs(berry_phase.imag) < 1e-5, "Berry phase should be real"
    
        # Print debug info
        print(f"\nBerry phase: {berry_phase.real}")
        print(f"Expected phase: {np.pi}")
        print(f"Relative error: {torch.abs(berry_phase.real - np.pi) / np.pi}")
    
        # Test matches theoretical value
>       assert torch.allclose(
            berry_phase.real, torch.tensor(np.pi, dtype=torch.float64), rtol=1e-2
        ), "Berry phase should match theoretical value"
E       AssertionError: Berry phase should match theoretical value
E       assert False
E        +  where False = <built-in method allclose of type object at 0x70560ced8b20>(tensor(0., dtype=torch.float64), tensor(3.1416, dtype=torch.float64), rtol=0.01)
E        +    where <built-in method allclose of type object at 0x70560ced8b20> = torch.allclose
E        +    and   tensor(0., dtype=torch.float64) = tensor(0.+0.j, dtype=torch.complex128).real
E        +    and   tensor(3.1416, dtype=torch.float64) = <built-in method tensor of type object at 0x70560ced8b20>(3.141592653589793, dtype=torch.float64)
E        +      where <built-in method tensor of type object at 0x70560ced8b20> = torch.tensor
E        +      and   3.141592653589793 = np.pi
E        +      and   torch.float64 = torch.float64

tests/test_core/test_quantum/test_state_space.py:301: AssertionError
__________________ TestInfrastructure.test_vulkan_integration __________________

self = <test_infrastructure.TestInfrastructure object at 0x705543f70c50>
batch_size = 32, data_dim = 1024

    def test_vulkan_integration(self, batch_size: int, data_dim: int):
        """Test Vulkan integration features."""
        # Create Vulkan integration
        vulkan = VulkanIntegration()
    
        # Test device capabilities
        info = vulkan.get_device_info()
        assert info is not None
        assert info.compute_support
        assert len(info.memory_types) > 0
    
        # Test memory transfer
        cpu_data = torch.randn(batch_size, data_dim)
        gpu_buffer = vulkan.create_buffer(cpu_data)
        assert gpu_buffer is not None
    
        # Test compute shader
        shader_code = """
        #version 450
        layout(local_size_x = 256) in;
        layout(std430, binding = 0) buffer Data {
            float data[];
        };
        void main() {
            uint idx = gl_GlobalInvocationID.x;
            if (idx < data.length()) {
                data[idx] = data[idx] * data[idx];
            }
        }
        """
        result = vulkan.compute(shader_code, gpu_buffer, workgroup_size=256)
        assert result is not None
        result_data = vulkan.download_buffer(result)
>       assert torch.allclose(result_data, cpu_data * cpu_data)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x70560ced8b20>(tensor([[ 0.7943,  0.0405,  0.9793,  ..., -0.3959, -0.9054, -0.0940],\n        [-0.9957,  0.3704, -0.1437,  ..., -0.5049,  0.6850, -1.0955],\n        [ 0.5569, -0.4143, -0.6180,  ...,  0.8617, -1.5136, -1.0572],\n        ...,\n        [ 1.4023,  0.6294,  0.1145,  ..., -2.1110,  2.2075,  1.3325],\n        [ 0.6760,  0.8445,  0.1970,  ..., -1.3187, -0.0175, -0.7021],\n        [ 0.1391,  0.8902, -0.7941,  ...,  1.6346, -0.7417, -0.2376]]), (tensor([[ 0.7943,  0.0405,  0.9793,  ..., -0.3959, -0.9054, -0.0940],\n        [-0.9957,  0.3704, -0.1437,  ..., -0.5049,  0.6850, -1.0955],\n        [ 0.5569, -0.4143, -0.6180,  ...,  0.8617, -1.5136, -1.0572],\n        ...,\n        [ 1.4023,  0.6294,  0.1145,  ..., -2.1110,  2.2075,  1.3325],\n        [ 0.6760,  0.8445,  0.1970,  ..., -1.3187, -0.0175, -0.7021],\n        [ 0.1391,  0.8902, -0.7941,  ...,  1.6346, -0.7417, -0.2376]]) * tensor([[ 0.7943,  0.0405,  0.9793,  ..., -0.3959, -0.9054, -0.0940],\n        [-0.9957,  0.3704, -0.1437,  ..., -0.5049,  0.6850, -1.0955],\n        [ 0.5569, -0.4143, -0.6180,  ...,  0.8617, -1.5136, -1.0572],\n        ...,\n        [ 1.4023,  0.6294,  0.1145,  ..., -2.1110,  2.2075,  1.3325],\n        [ 0.6760,  0.8445,  0.1970,  ..., -1.3187, -0.0175, -0.7021],\n        [ 0.1391,  0.8902, -0.7941,  ...,  1.6346, -0.7417, -0.2376]])))
E        +    where <built-in method allclose of type object at 0x70560ced8b20> = torch.allclose

tests/test_infrastructure/test_infrastructure.py:140: AssertionError
_________________ TestInfrastructure.test_resource_allocation __________________

self = <test_infrastructure.TestInfrastructure object at 0x705543f71400>
batch_size = 32, data_dim = 1024

    def test_resource_allocation(self, batch_size: int, data_dim: int):
        """Test resource allocation features."""
        # Create resource allocator
        allocator = ResourceAllocator(memory_limit=2**30, compute_limit=80)  # 80% CPU limit
    
        # Test resource monitoring
        status = allocator.get_status()
        assert status is not None
        assert "available_memory" in status
        assert "cpu_usage" in status
    
        # Test allocation planning
        plan = allocator.plan_allocation(
            memory_size=batch_size * data_dim * 4,
            compute_intensity=50  # 50% CPU intensity
        )
        assert plan is not None
        assert "recommended_batch_size" in plan
        assert "thread_count" in plan
    
        # Test resource limits
        def resource_intensive_task():
            return torch.randn(10000, 10000)
    
>       with pytest.raises(ResourceAllocationError):
E       Failed: DID NOT RAISE <class 'src.infrastructure.base.ResourceAllocationError'>

tests/test_infrastructure/test_infrastructure.py:202: Failed
______________ TestInfrastructure.test_infrastructure_integration ______________

self = <test_infrastructure.TestInfrastructure object at 0x705543f715b0>
batch_size = 32, data_dim = 1024, num_threads = 16

    def test_infrastructure_integration(
        self, batch_size: int, data_dim: int, num_threads: int
    ):
        """Test integrated infrastructure components."""
        # Create infrastructure components
        cpu_opt = CPUOptimizer(enable_profiling=True)
        mem_mgr = MemoryManager(pool_size=2**30)
        vulkan = VulkanIntegration()
        parallel = ParallelProcessor(num_threads=num_threads)
        allocator = ResourceAllocator(memory_limit=2**30, compute_limit=80)
    
        # Generate test data
        data = torch.randn(batch_size, data_dim)
    
        # Test integrated computation pipeline
        def compute_pipeline(x: torch.Tensor) -> torch.Tensor:
            # CPU optimization
            with cpu_opt.profile():
                x = torch.matmul(x, x.t())
    
            # Memory management
            with mem_mgr.optimize():
                x = torch.fft.fft2(x)
    
            # Vulkan acceleration
            if vulkan.get_device_info() is not None:
                gpu_buffer = vulkan.create_buffer(x)
                shader_code = """
                #version 450
                layout(local_size_x = 256) in;
                layout(std430, binding = 0) buffer Data {
                    float data[];
                };
                void main() {
                    uint idx = gl_GlobalInvocationID.x;
                    if (idx < data.length()) {
                        data[idx] = data[idx] * data[idx];
                    }
                }
                """
                x = vulkan.download_buffer(vulkan.compute(shader_code, gpu_buffer))
    
            # Parallel processing
            partitions = parallel.partition_data(x)
            results = parallel.process_parallel(torch.nn.functional.relu, partitions)
            return parallel.merge_results(results)
    
        # Execute pipeline with resource limits
>       result = allocator.run_with_limits(
            lambda: compute_pipeline(data),
            memory_limit=2**30,
            cpu_limit=80
        )

tests/test_infrastructure/test_infrastructure.py:261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/infrastructure/base.py:203: in run_with_limits
    return func()
tests/test_infrastructure/test_infrastructure.py:262: in <lambda>
    lambda: compute_pipeline(data),
tests/test_infrastructure/test_infrastructure.py:257: in compute_pipeline
    results = parallel.process_parallel(torch.nn.functional.relu, partitions)
src/infrastructure/base.py:159: in process_parallel
    return [func(chunk) for chunk in chunks]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[ 3.4310e+04+0.0000e+00j, -6.7444e+02-9.6284e+02j,
          2.3046e+02+1.3363e+02j,  9.6388e+02+4.9017e+02j,
...         -2.2621e+03+9.1711e+01j,  1.4694e+03-8.2537e+02j,
          2.6452e+02-1.4601e+03j,  3.2217e+04-6.1035e-04j]])
inplace = False

    def relu(input: Tensor, inplace: bool = False) -> Tensor:  # noqa: D400,D402
        r"""relu(input, inplace=False) -> Tensor
    
        Applies the rectified linear unit function element-wise. See
        :class:`~torch.nn.ReLU` for more details.
        """
        if has_torch_function_unary(input):
            return handle_torch_function(relu, (input,), input, inplace=inplace)
        if inplace:
            result = torch.relu_(input)
        else:
>           result = torch.relu(input)
E           RuntimeError: clamp is not supported for complex types

venv/lib/python3.12/site-packages/torch/nn/functional.py:1704: RuntimeError
_____________ TestCrossValidation.test_pattern_quantum_interaction _____________

self = <test_cross_validation.TestCrossValidation object at 0x705517503e00>
framework = <src.validation.framework.ValidationFramework object at 0x7054fc2fd2e0>
batch_size = 8, dim = 16

    def test_pattern_quantum_interaction(
        self, framework: ValidationFramework, batch_size: int, dim: int
    ):
        """Test interaction between pattern and quantum components."""
        # Generate quantum state that represents pattern
        state = torch.randn(batch_size, 1, dim, dtype=torch.complex64)  # Add sequence dimension
        state = state / torch.norm(state, dim=2, keepdim=True)  # Normalize along feature dimension
    
        # Extract pattern from quantum state
        pattern = torch.abs(state) ** 2
    
        # Validate both representations
>       state_result = framework.validate_quantum_state(state.squeeze(1))  # Remove sequence dimension for validation

tests/test_integration/test_cross_validation.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/framework.py:635: in validate_quantum_state
    return self.quantum_validator.validate(
src/validation/quantum/state.py:843: in validate
    preparation = self.preparation_validator.validate_preparation(target, prepared)
src/validation/quantum/state.py:241: in validate_preparation
    fidelity = self._compute_fidelity(target, prepared)
src/validation/quantum/state.py:289: in _compute_fidelity
    if target.is_pure(self.tolerance) and prepared.is_pure(self.tolerance):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumState(amplitudes=tensor([[ 3.0786e-01+0.0639j, -1.6422e-01-0.0467j,  9.7136e-02-0.2921j,
          1.0929e-01-0....j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,
         0.+0.j, 0.+0.j]]))
tolerance = 1e-06

    def is_pure(self, tolerance: float = 1e-6) -> bool:
        """Check if the state is pure by computing the purity of its density matrix.
    
        A pure state has Tr(ρ²) = 1, while mixed states have Tr(ρ²) < 1.
    
        Args:
            tolerance: Numerical tolerance for comparison with 1.0
    
        Returns:
            bool: True if the state is pure, False otherwise
        """
        rho = self.density_matrix()
>       purity = torch.trace(torch.matmul(rho, rho)).real
E       RuntimeError: trace: expected a matrix, but got tensor with dim 3

src/core/quantum/types.py:148: RuntimeError
_____________ TestCrossValidation.test_geometric_pattern_coupling ______________

self = <test_cross_validation.TestCrossValidation object at 0x7055175380b0>
framework = <src.validation.framework.ValidationFramework object at 0x7054fc2fe960>
flow = PatternFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features=3...ures=32, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=256, bias=True)
  )
)
batch_size = 8, dim = 16

    def test_geometric_pattern_coupling(
        self, framework: ValidationFramework, flow: PatternFlow, batch_size: int, dim: int
    ):
        """Test coupling between geometric and pattern components."""
        # Generate metric tensor
        metric = torch.randn(batch_size, dim, dim)
        metric = metric @ metric.transpose(-1, -2)
    
        # Generate pattern compatible with metric
        pattern = torch.randn(batch_size, 1, dim)  # Add sequence dimension for PatternFlow
        pattern = pattern / torch.norm(pattern, dim=2, keepdim=True)  # Normalize along feature dimension
    
        # Validate geometric consistency
        metric_result = framework.geometric_validator.validate_model_geometry(
            batch_size=batch_size,
            manifold_dim=dim
        )
>       pattern_result = framework.validate_pattern_formation(
            pattern=pattern.squeeze(1),  # Remove sequence dimension for pattern validation
            dynamics=None,
            time_steps=100
        )

tests/test_integration/test_cross_validation.py:152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/framework.py:657: in validate_pattern_formation
    return pattern_validator.validate(
src/validation/patterns/formation.py:1404: in validate
    spatial = self.spatial_validator.validate_spatial(trajectory[-1])
src/validation/patterns/formation.py:757: in validate_spatial
    wavelength = self._compute_wavelength(pattern)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.patterns.formation.SpatialValidator object at 0x7054fc2ffc80>
pattern = tensor([[ 0.4467,  0.0934,  0.0732,  0.2912,  0.1028,  0.1537, -0.0519,  0.5055,
          0.0166,  0.0501,  0.4094,  ...  0.2546, -0.1966,  0.6294,  0.1103,
          0.1873, -0.0878,  0.0580,  0.0170, -0.2228, -0.0561, -0.0259,  0.3486]])

    def _compute_wavelength(self, pattern: torch.Tensor) -> torch.Tensor:
        """Compute dominant wavelength from pattern.
    
        Args:
            pattern: Pattern tensor of shape (batch_size, channels, height, width)
    
        Returns:
            Wavelength tensor of shape (batch_size, channels)
        """
        # Get size and create frequency grid
        N = pattern.shape[-1]
        freqs = torch.fft.fftfreq(N, dtype=torch.float32, device=pattern.device)
    
        # Create 2D frequency grids for x and y separately
        freqs_x = freqs[None, :].expand(N, N)
        freqs_y = freqs[:, None].expand(N, N)
    
        # Compute power spectrum
        fft = torch.fft.fft2(pattern)
        power = torch.abs(fft).pow(2)
    
        # Create mask for valid frequencies (exclude DC and above Nyquist)
        nyquist = 0.5
        mask_x = (freqs_x.abs() > 0) & (freqs_x.abs() <= nyquist)  # Explicitly exclude DC
        mask_y = (freqs_y.abs() > 0) & (freqs_y.abs() <= nyquist)  # Explicitly exclude DC
        mask = mask_x | mask_y  # Use OR to capture peaks in either direction
    
        # Get valid frequencies and reshape power
        batch_shape = power.shape[:-2]  # (batch_size, channels)
>       power_valid = power.reshape(*batch_shape, -1)[..., mask.reshape(-1)]
E       IndexError: The shape of the mask [256] at index 0 does not match the shape of the indexed tensor [128] at index 0

src/validation/patterns/formation.py:804: IndexError
______________ TestCrossValidation.test_infrastructure_framework _______________

self = <test_cross_validation.TestCrossValidation object at 0x7055175383b0>
framework = <src.validation.framework.ValidationFramework object at 0x7054fc275c70>
batch_size = 8, dim = 16

    def test_infrastructure_framework(
        self, framework: ValidationFramework, batch_size: int, dim: int
    ):
        """Test integration between infrastructure and validation framework."""
        # Initialize infrastructure
        cpu_opt = CPUOptimizer(enable_profiling=True, enable_memory_tracking=True)
        mem_mgr = MemoryManager(pool_size=1024, enable_monitoring=True)  # 1GB pool
    
        # Generate test data
        data = torch.randn(batch_size, 1, dim)  # Add sequence dimension
        metric = data.squeeze(1) @ data.squeeze(1).t()  # Create a valid metric tensor
    
        # Test CPU optimization
        @cpu_opt.profile_execution
        def run_validation(data: torch.Tensor, metric: torch.Tensor) -> FrameworkValidationResult:
            return framework.validate_all(
                model=None,
                data=data.squeeze(1),  # Remove sequence dimension for validation
                metric=metric,
                riemannian=None
            )
    
>       result = run_validation(data, metric)

tests/test_integration/test_cross_validation.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/performance/cpu_optimizer.py:67: in wrapper
    result = func(*args, **kwargs)
tests/test_integration/test_cross_validation.py:190: in run_validation
    return framework.validate_all(
src/validation/framework.py:484: in validate_all
    quantum_result = self.validate_quantum_state(data)
src/validation/framework.py:635: in validate_quantum_state
    return self.quantum_validator.validate(
src/validation/quantum/state.py:843: in validate
    preparation = self.preparation_validator.validate_preparation(target, prepared)
src/validation/quantum/state.py:241: in validate_preparation
    fidelity = self._compute_fidelity(target, prepared)
src/validation/quantum/state.py:289: in _compute_fidelity
    if target.is_pure(self.tolerance) and prepared.is_pure(self.tolerance):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumState(amplitudes=tensor([[-0.2807+0.j,  0.0441+0.j,  0.1583+0.j,  0.0650+0.j,  0.4449+0.j,  0.3316+0.j,
       ....j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,
         0.+0.j, 0.+0.j]]))
tolerance = 1e-06

    def is_pure(self, tolerance: float = 1e-6) -> bool:
        """Check if the state is pure by computing the purity of its density matrix.
    
        A pure state has Tr(ρ²) = 1, while mixed states have Tr(ρ²) < 1.
    
        Args:
            tolerance: Numerical tolerance for comparison with 1.0
    
        Returns:
            bool: True if the state is pure, False otherwise
        """
        rho = self.density_matrix()
>       purity = torch.trace(torch.matmul(rho, rho)).real
E       RuntimeError: trace: expected a matrix, but got tensor with dim 3

src/core/quantum/types.py:148: RuntimeError
________________ TestCrossValidation.test_end_to_end_validation ________________

self = <test_cross_validation.TestCrossValidation object at 0x7055175386b0>
framework = <src.validation.framework.ValidationFramework object at 0x7054fc29cc20>
batch_size = 8, dim = 16

    def test_end_to_end_validation(
        self, framework: ValidationFramework, batch_size: int, dim: int
    ):
        """Test complete end-to-end validation pipeline."""
        # Generate test configuration
        config = {
            "metric": torch.randn(batch_size, dim, dim),
            "quantum_state": torch.randn(batch_size, 1, dim, dtype=torch.complex64),  # Add sequence dimension
            "pattern": torch.randn(batch_size, 1, dim),  # Add sequence dimension
            "flow": torch.randn(batch_size, dim, dim),
            "parameters": torch.linspace(0, 1, 10),
        }
    
        # Normalize and prepare data
        config["metric"] = config["metric"] @ config["metric"].transpose(-1, -2)
        config["quantum_state"] = config["quantum_state"] / torch.norm(
            config["quantum_state"], dim=2, keepdim=True  # Normalize along feature dimension
        )
        config["pattern"] = config["pattern"] / torch.norm(
            config["pattern"], dim=2, keepdim=True  # Normalize along feature dimension
        )
    
        # Run complete validation
>       result = framework.validate_all(
            model=None,
            data=config["pattern"].squeeze(1),  # Remove sequence dimension for validation
            metric=config["metric"]
        )

tests/test_integration/test_cross_validation.py:287: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.framework.ValidationFramework object at 0x7054fc29cc20>
model = None
data = tensor([[ 0.2558, -0.3857,  0.1851, -0.3149,  0.0885, -0.1595,  0.0061,  0.1185,
         -0.0938,  0.3962, -0.3960,  ...  0.0454,  0.1730, -0.1402,  0.4001,
          0.2395,  0.2658, -0.2585,  0.0806, -0.2786, -0.1828,  0.5462, -0.1898]])
metric = tensor([[[ 19.0468,  -3.4756,  -2.4433,  ...,  -5.1901,   3.6875,  -2.7702],
         [ -3.4756,  17.1952,  -1.8963,  ...38,  ...,   0.9399,  21.1718,  -1.7543],
         [  1.6822,  -3.4582,  -1.8457,  ...,  -5.6441,  -1.7543,  11.9236]]])
riemannian = PatternRiemannianStructure()

    def validate_all(
        self,
        model: Optional[nn.Module],
        data: torch.Tensor,
        metric: Optional[torch.Tensor] = None,
        riemannian: Optional[RiemannianFramework] = None,
    ) -> FrameworkValidationResult:
        """Run complete validation on model and data."""
        # Initialize messages list
        messages = []
    
        # Run geometric validation
        geometric_result = None
        if metric is not None or riemannian is not None:
            if riemannian is None and metric is not None:
                # Create basic Riemannian framework from metric
                manifold_dim = metric.shape[-1]
                riemannian = PatternRiemannianStructure(
                    manifold_dim=manifold_dim,
                    pattern_dim=manifold_dim  # Using manifold_dim as pattern_dim for basic case
                )
                # Initialize metric factors from input metric
                with torch.no_grad():
>                   riemannian.metric_factors.copy_(metric.reshape(manifold_dim, -1))
E                   RuntimeError: The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 1

src/validation/framework.py:479: RuntimeError
________________ TestCrossValidation.test_validation_stability _________________

self = <test_cross_validation.TestCrossValidation object at 0x705517538740>
framework = <src.validation.framework.ValidationFramework object at 0x7054fc29e240>
batch_size = 8, dim = 16

    def test_validation_stability(
        self, framework: ValidationFramework, batch_size: int, dim: int
    ):
        """Test stability of validation results under perturbations."""
        # Generate base configuration
        base_config = {
            "metric": torch.randn(batch_size, dim, dim),
            "quantum_state": torch.randn(batch_size, 1, dim, dtype=torch.complex64),  # Add sequence dimension
            "pattern": torch.randn(batch_size, 1, dim),  # Add sequence dimension
        }
    
        # Normalize base configuration
        base_config["metric"] = base_config["metric"] @ base_config["metric"].transpose(
            -1, -2
        )
        base_config["quantum_state"] = base_config["quantum_state"] / torch.norm(
            base_config["quantum_state"], dim=2, keepdim=True  # Normalize along feature dimension
        )
        base_config["pattern"] = base_config["pattern"] / torch.norm(
            base_config["pattern"], dim=2, keepdim=True  # Normalize along feature dimension
        )
    
        # Get base validation result
>       base_result = framework.validate_all(
            model=None,
            data=base_config["pattern"].squeeze(1),  # Remove sequence dimension for validation
            metric=base_config["metric"]
        )

tests/test_integration/test_cross_validation.py:328: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.framework.ValidationFramework object at 0x7054fc29e240>
model = None
data = tensor([[-0.2870, -0.0624, -0.2534, -0.1063,  0.2730, -0.1038,  0.2182,  0.0255,
          0.1380, -0.1422,  0.7209, -... -0.2658,  0.4115, -0.0791,  0.1775,
          0.3099, -0.2703,  0.0385, -0.0310,  0.0030, -0.3060, -0.3080,  0.0998]])
metric = tensor([[[ 15.3294,  -3.9379,   3.5471,  ...,   1.0542,  -1.4029,  -0.2342],
         [ -3.9379,  20.8213,  -0.6395,  ...68,  ...,   5.9147,  15.5702,  -3.0143],
         [  3.4417,  -1.2889,  -2.2409,  ...,   7.6078,  -3.0143,  17.8870]]])
riemannian = PatternRiemannianStructure()

    def validate_all(
        self,
        model: Optional[nn.Module],
        data: torch.Tensor,
        metric: Optional[torch.Tensor] = None,
        riemannian: Optional[RiemannianFramework] = None,
    ) -> FrameworkValidationResult:
        """Run complete validation on model and data."""
        # Initialize messages list
        messages = []
    
        # Run geometric validation
        geometric_result = None
        if metric is not None or riemannian is not None:
            if riemannian is None and metric is not None:
                # Create basic Riemannian framework from metric
                manifold_dim = metric.shape[-1]
                riemannian = PatternRiemannianStructure(
                    manifold_dim=manifold_dim,
                    pattern_dim=manifold_dim  # Using manifold_dim as pattern_dim for basic case
                )
                # Initialize metric factors from input metric
                with torch.no_grad():
>                   riemannian.metric_factors.copy_(metric.reshape(manifold_dim, -1))
E                   RuntimeError: The size of tensor a (16) must match the size of tensor b (128) at non-singleton dimension 1

src/validation/framework.py:479: RuntimeError
_____________________________ test_exponential_map _____________________________

    def test_exponential_map():
        """Test exponential map computation."""
        dim = 3
        exp_map = HyperbolicExponential(dim)
    
        # Test case 1: Small tangent vector
        x = torch.tensor([1.2, 0.3, 0.4])  # Point on hyperboloid
        v = torch.tensor([0.0, 1e-8, 1e-8])  # Small tangent vector
        result = exp_map.forward(x, v)
    
        # Check properties
        assert not torch.any(torch.isnan(result))
        assert result[0] >= 1.0 + 1e-7  # Time component constraint
        inner = exp_map.minkowski_inner(result, result)
        assert torch.allclose(inner, torch.tensor(-1.0), atol=1e-6)
    
        # Test case 2: Normal tangent vector
        x = torch.tensor([1.5, 0.5, 0.0])
        v = torch.tensor([0.0, 0.3, 0.4])
        result = exp_map.forward(x, v)
        assert not torch.any(torch.isnan(result))
        inner = exp_map.minkowski_inner(result, result)
        assert torch.allclose(inner, torch.tensor(-1.0), atol=1e-6)
    
        # Test case 3: Large tangent vector (should be clamped)
        x = torch.tensor([2.0, 1.0, 1.0])
        v = torch.tensor([0.0, 10.0, 10.0])
        result = exp_map.forward(x, v)
        assert not torch.any(torch.isnan(result))
        inner = exp_map.minkowski_inner(result, result)
>       assert torch.allclose(inner, torch.tensor(-1.0), atol=1e-6)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x70560ced8b20>(tensor(0.), tensor(-1.), atol=1e-06)
E        +    where <built-in method allclose of type object at 0x70560ced8b20> = torch.allclose
E        +    and   tensor(-1.) = <built-in method tensor of type object at 0x70560ced8b20>(-1.0)
E        +      where <built-in method tensor of type object at 0x70560ced8b20> = torch.tensor

tests/test_neural/test_attention/test_exponential.py:35: AssertionError
______________________________ test_logarithm_map ______________________________

    def test_logarithm_map():
        """Test logarithm map computation."""
        dim = 3
        exp_map = HyperbolicExponential(dim)
        log_map = HyperbolicLogarithm(dim)
    
        # Test case 1: Same point
        x = torch.tensor([1.2, 0.3, 0.4])
        x = exp_map.project_to_hyperboloid(x)
        result = log_map.forward(x, x)
        assert torch.allclose(result, torch.zeros_like(x), atol=1e-6)
    
        # Test case 2: Close points
        x = torch.tensor([1.2, 0.3, 0.4])
        y = torch.tensor([1.2, 0.3 + 1e-8, 0.4])
        result = log_map.forward(x, y)
        assert not torch.any(torch.isnan(result))
        assert torch.norm(result) < 1e-7  # Should be very small
    
        # Test case 3: Normal points
        x = torch.tensor([1.5, 0.5, 0.0])
        y = torch.tensor([1.5, 0.7, 0.2])
        result = log_map.forward(x, y)
        assert not torch.any(torch.isnan(result))
    
        # The result should be in the tangent space at x
        inner = exp_map.minkowski_inner(x, result)
>       assert torch.abs(inner) < 1e-6
E       assert tensor(0.0296) < 1e-06
E        +  where tensor(0.0296) = <built-in method abs of type object at 0x70560ced8b20>(tensor(-0.0296))
E        +    where <built-in method abs of type object at 0x70560ced8b20> = torch.abs

tests/test_neural/test_attention/test_logarithm.py:32: AssertionError
________________________ test_logarithm_map_properties _________________________

    def test_logarithm_map_properties():
        """Test mathematical properties of logarithm map."""
        dim = 3
        exp_map = HyperbolicExponential(dim)
        log_map = HyperbolicLogarithm(dim)
    
        # Property 1: log_x(x) = 0
        x = torch.tensor([1.2, 0.3, 0.4])
        x = exp_map.project_to_hyperboloid(x)
        result = log_map.forward(x, x)
        assert torch.allclose(result, torch.zeros_like(x), atol=1e-6)
    
        # Property 2: log_x(exp_x(v)) ≈ v for small v
        x = torch.tensor([1.5, 0.5, 0.0])
        v = torch.tensor([0.0, 0.1, 0.1])  # Small tangent vector
        y = exp_map.forward(x, v)
        v_recovered = log_map.forward(x, y)
>       assert torch.allclose(v_recovered, v, atol=1e-5)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x70560ced8b20>(tensor([0.0524, 0.1171, 0.0937]), tensor([0.0000, 0.1000, 0.1000]), atol=1e-05)
E        +    where <built-in method allclose of type object at 0x70560ced8b20> = torch.allclose

tests/test_neural/test_attention/test_logarithm.py:64: AssertionError
_____________________________ test_exp_log_inverse _____________________________

    def test_exp_log_inverse():
        """Test inverse relationship between exponential and logarithm maps."""
        dim = 3
        exp_map = HyperbolicExponential(dim)
        log_map = HyperbolicLogarithm(dim)
    
        # Test points
        x = torch.tensor([1.2, 0.3, 0.4])
        x = exp_map.project_to_hyperboloid(x)
    
        # Small tangent vectors
        vectors = [
            torch.tensor([0.0, 0.1, 0.1]),
            torch.tensor([0.0, -0.1, 0.2]),
            torch.tensor([0.0, 0.2, -0.1])
        ]
    
        for v in vectors:
            # Test exp(log(y)) = y
            y = exp_map.forward(x, v)
            v_recovered = log_map.forward(x, y)
            y_recovered = exp_map.forward(x, v_recovered)
    
>           assert torch.allclose(y, y_recovered, atol=1e-5)
E           assert False
E            +  where False = <built-in method allclose of type object at 0x70560ced8b20>(tensor([1.2106, 0.4252, 0.5335]), tensor([1.1996, 0.4118, 0.5191]), atol=1e-05)
E            +    where <built-in method allclose of type object at 0x70560ced8b20> = torch.allclose

tests/test_neural/test_attention/test_logarithm.py:99: AssertionError
______________________ test_advanced_minkowski_properties ______________________

    def test_advanced_minkowski_properties():
        """Test advanced properties of Minkowski operations including normalization,
        projection stability, and edge cases."""
        dim = 3
        exp_map = HyperbolicExponential(dim)
    
        # Test 1: Verify normalization to hyperboloid
        x = torch.tensor([2.5, 1.2, 0.8])  # Arbitrary point
        x_norm = exp_map.project_to_hyperboloid(x)
        inner_xx = exp_map.minkowski_inner(x_norm, x_norm)
        assert torch.allclose(inner_xx, torch.tensor(-1.0), atol=1e-6), \
            f"Expected normalized point to have Minkowski norm -1, got {inner_xx}"
    
        # Test 2: Projection stability (projecting already projected vectors)
        v = torch.tensor([0.0, 0.5, -0.3])  # Initial vector
        v_proj1 = exp_map.project_to_tangent(x_norm, v)
        v_proj2 = exp_map.project_to_tangent(x_norm, v_proj1)
        # The second projection should not change the vector significantly
        assert torch.allclose(v_proj1, v_proj2, atol=1e-6), \
            "Projection should be stable under repeated application"
    
        # Test 3: Edge cases with extreme vectors
        # Very large vector
        v_large = torch.tensor([0.0, 1e5, 1e5])
        v_large_proj = exp_map.project_to_tangent(x_norm, v_large)
        inner_large = exp_map.minkowski_inner(x_norm, v_large_proj)
>       assert torch.allclose(inner_large, torch.tensor(0.0), atol=1e-6), \
            "Large vector projection should still be orthogonal"
E       AssertionError: Large vector projection should still be orthogonal
E       assert False
E        +  where False = <built-in method allclose of type object at 0x70560ced8b20>(tensor(-0.0625), tensor(0.), atol=1e-06)
E        +    where <built-in method allclose of type object at 0x70560ced8b20> = torch.allclose
E        +    and   tensor(0.) = <built-in method tensor of type object at 0x70560ced8b20>(0.0)
E        +      where <built-in method tensor of type object at 0x70560ced8b20> = torch.tensor

tests/test_neural/test_attention/test_minkowski.py:174: AssertionError
__________________________ test_bifurcation_analysis ___________________________

pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2eaea0>
grid_size = 32

    def test_bifurcation_analysis(pattern_system, grid_size):
        """Test bifurcation analysis."""
        # Create test pattern
        pattern = torch.randn(1, 2, grid_size, grid_size)
    
        # Define parameter range
        parameter_range = torch.linspace(0, 2, 100)
    
        # Create parameterized reaction term
        def parameterized_reaction(state, param):
            u, v = state[:, 0], state[:, 1]
            du = param * u**2 * v - u
            dv = u**2 - v
            return torch.stack([du, dv], dim=1)
    
        # Analyze bifurcations
        diagram = pattern_system.bifurcation_analysis(
            pattern, parameterized_reaction, parameter_range
        )
    
        # Test diagram properties
        assert isinstance(diagram, BifurcationDiagram), "Should return bifurcation diagram"
>       assert diagram.bifurcation_points.numel() > 0, "Should detect bifurcations"
E       AssertionError: Should detect bifurcations
E       assert 0 > 0
E        +  where 0 = <built-in method numel of Tensor object at 0x7054fc2d9b30>()
E        +    where <built-in method numel of Tensor object at 0x7054fc2d9b30> = tensor([]).numel
E        +      where tensor([]) = BifurcationDiagram(solution_states=tensor([[[[1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          ...,\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.]],\n\n         [[1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          ...,\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.]]],\n\n\n        [[[1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          ...,\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.]],\n\n         [[1000., 1000., 1000....0., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          ...,\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.],\n          [1000., 1000., 1000.,  ..., 1000., 1000., 1000.]]]]), solution_params=tensor([0.0202, 0.0404, 0.0606, 0.0808, 0.1010, 0.1212, 0.1414, 0.1616, 0.1818,\n        0.2020, 0.2222, 0.2424, 0.2626, 0.2828, 0.3030, 0.3232, 0.3434, 0.3636,\n        0.3838, 0.4040, 0.4242, 0.4444, 0.4646, 0.4848, 0.5051, 0.5253, 0.5455,\n        0.5657, 0.5859, 0.6061, 0.6263, 0.6465, 0.6667, 0.6869, 0.7071, 0.7273,\n        0.7475, 0.7677, 0.7879, 0.8081, 0.8283, 0.8485, 0.8687, 0.8889, 0.9091,\n        0.9293, 0.9495, 0.9697, 0.9899, 1.0101, 1.0303, 1.0505, 1.0707, 1.0909,\n        1.1111, 1.1313, 1.1515, 1.1717, 1.1919, 1.2121, 1.2323, 1.2525, 1.2727,\n        1.2929, 1.3131, 1.3333, 1.3535, 1.3737, 1.3939, 1.4141, 1.4343, 1.4545,\n        1.4747, 1.4949, 1.5152, 1.5354, 1.5556, 1.5758, 1.5960, 1.6162, 1.6364,\n        1.6566, 1.6768, 1.6970, 1.7172, 1.7374, 1.7576, 1.7778, 1.7980, 1.8182,\n        1.8384, 1.8586, 1.8788, 1.8990, 1.9192, 1.9394, 1.9596, 1.9798, 2.0000]), bifurcation_points=tensor([])).bifurcation_points

tests/test_neural/test_attention/test_pattern/test_bifurcation.py:32: AssertionError
_____________________ test_bifurcation_detection_threshold _____________________

pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2bee10>
grid_size = 32

    def test_bifurcation_detection_threshold(pattern_system, grid_size):
        """Test that bifurcation detection threshold is appropriate."""
        # Create test pattern
        pattern = torch.randn(1, 2, grid_size, grid_size)
    
        # Define parameter range
        parameter_range = torch.linspace(0, 2, 100)
    
        # Create parameterized reaction term with known bifurcation
        def parameterized_reaction(state, param):
            u, v = state[:, 0], state[:, 1]
            # Pitchfork bifurcation at param = 1
            du = param * u - u**3
            dv = -v  # Simple linear decay
            return torch.stack([du, dv], dim=1)
    
        # Analyze bifurcations
        diagram = pattern_system.bifurcation_analysis(
            pattern, parameterized_reaction, parameter_range
        )
    
        # Should detect the pitchfork bifurcation near param = 1
        bifurcation_params = diagram.bifurcation_points
>       assert any(abs(p - 1.0) < 0.1 for p in bifurcation_params), \
            "Should detect bifurcation near param = 1"
E       AssertionError: Should detect bifurcation near param = 1
E       assert False
E        +  where False = any(<generator object test_bifurcation_detection_threshold.<locals>.<genexpr> at 0x7054fc3f74c0>)

tests/test_neural/test_attention/test_pattern/test_bifurcation.py:80: AssertionError
____________________________ test_solution_branches ____________________________

pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2bc9b0>
grid_size = 32

    def test_solution_branches(pattern_system, grid_size):
        """Test that solution branches are correctly tracked."""
        # Create test pattern
        pattern = torch.ones(1, 2, grid_size, grid_size)  # Start from uniform state
    
        # Define parameter range
        parameter_range = torch.linspace(0, 2, 100)
    
        # Create parameterized reaction term with known solution structure
        def parameterized_reaction(state, param):
            u, v = state[:, 0], state[:, 1]
            # Simple linear system with known solution u = param * u_initial
            du = (param - 1) * u
            dv = -v
            return torch.stack([du, dv], dim=1)
    
        # Analyze bifurcations
        diagram = pattern_system.bifurcation_analysis(
            pattern, parameterized_reaction, parameter_range
        )
    
        # Test solution states are non-zero
        for state in diagram.solution_states:
            assert not torch.allclose(state, torch.zeros_like(state)), \
                "Solution states should not be zero"
    
        # Check that solution magnitude increases with parameter
>       magnitudes = torch.norm(diagram.solution_states.reshape(diagram.solution_states.shape[0], -1), dim=1)
E       RuntimeError: cannot reshape tensor of 0 elements into shape [0, -1] because the unspecified dimension size -1 can be any value and is ambiguous

tests/test_neural/test_attention/test_pattern/test_bifurcation.py:151: RuntimeError
_____________________________ test_pattern_control _____________________________

pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2be150>
grid_size = 32

    def test_pattern_control(pattern_system, grid_size):
        """Test pattern control mechanisms."""
        # Create current and target patterns
        current = torch.randn(1, 2, grid_size, grid_size)
        target = torch.randn(1, 2, grid_size, grid_size)
    
        # Define constraints
        constraints = [
            lambda x: torch.mean(x) - 1.0,  # Mean constraint
            lambda x: torch.var(x) - 0.1,  # Variance constraint
        ]
    
        # Compute control signal
        control = pattern_system.pattern_control(current, target, constraints)
    
        # Test control signal properties
        assert isinstance(control, ControlSignal), "Should return control signal"
        assert control.magnitude is not None, "Should compute control magnitude"
        assert control.direction is not None, "Should compute control direction"
    
        # Test constraint satisfaction
        controlled_state = current + control.magnitude * control.direction
        for constraint in constraints:
>           assert abs(constraint(controlled_state)) < 1e-2, "Should satisfy constraints"
E           AssertionError: Should satisfy constraints
E           assert tensor(1.0002) < 0.01
E            +  where tensor(1.0002) = abs(tensor(-1.0002))
E            +    where tensor(-1.0002) = <function test_pattern_control.<locals>.<lambda> at 0x7054fc35f240>(tensor([[[[ 0.8742, -1.1787,  0.3064,  ..., -0.6920, -1.0043, -0.3626],\n          [ 0.8565,  0.6200,  0.4973,  ..., -2.4678,  1.2733, -0.4112],\n          [ 0.2515, -1.3465,  1.0957,  ...,  1.0306,  0.7009, -2.3592],\n          ...,\n          [-1.1494,  0.0261,  0.0257,  ...,  0.5027,  1.1970,  1.5421],\n          [ 3.0468, -1.0784,  0.3583,  ..., -0.1538,  0.4215,  1.5457],\n          [-1.2078,  1.7962, -0.9874,  ..., -0.1151, -1.2325,  0.5752]],\n\n         [[-0.4318,  0.1279,  0.6225,  ...,  2.3031, -1.2062,  0.9995],\n          [-0.3725,  0.4158,  0.6454,  ..., -0.7294, -0.2169,  0.5922],\n          [-1.5108, -0.6042,  0.6228,  ..., -0.9265,  0.0615, -1.0408],\n          ...,\n          [-0.4022,  1.2088, -0.2562,  ..., -0.4005, -0.0258, -0.1549],\n          [ 1.7329, -0.4327,  0.3396,  ..., -0.9890,  0.1241, -0.7692],\n          [-0.6586,  0.2964, -0.2632,  ...,  1.0561,  0.7693,  0.5375]]]]))

tests/test_neural/test_attention/test_pattern/test_control.py:33: AssertionError
________________________ test_spatiotemporal_evolution _________________________

x = tensor([[[[ 0.9486,  0.4108, -0.3553,  ..., -1.1188,  0.4287,  0.4740],
          [-0.8669,  0.6408, -2.1350,  ..., -1..., -0.1360,  ..., -0.6621, -1.1079, -0.0562],
          [ 0.6942, -0.6080,  0.5042,  ...,  2.3401,  0.4805,  1.5884]]]])
t = 0.0

    def coupling(x, t):
        """Space-time coupling term."""
>       return 0.1 * torch.sin(2 * torch.pi * t) * x
E       TypeError: sin(): argument 'input' (position 1) must be Tensor, not float

tests/test_neural/test_attention/test_pattern/test_control.py:49: TypeError

During handling of the above exception, another exception occurred:

pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2bf080>
grid_size = 32

    def test_spatiotemporal_evolution(pattern_system, grid_size):
        """Test spatiotemporal pattern evolution."""
        # Create initial pattern
        initial = torch.randn(1, 2, grid_size, grid_size)
    
        # Define space-time coupling term
        def coupling(x, t):
            """Space-time coupling term."""
            return 0.1 * torch.sin(2 * torch.pi * t) * x
    
        # Evolve pattern
>       evolution = pattern_system.evolve_spatiotemporal(
            initial, coupling, steps=100
        )

tests/test_neural/test_attention/test_pattern/test_control.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2bf080>
initial = tensor([[[[ 0.9486,  0.4108, -0.3553,  ..., -1.1188,  0.4287,  0.4740],
          [-0.8669,  0.6408, -2.1350,  ..., -1..., -0.1360,  ..., -0.6621, -1.1079, -0.0562],
          [ 0.6942, -0.6080,  0.5042,  ...,  2.3401,  0.4805,  1.5884]]]])
coupling = <function test_spatiotemporal_evolution.<locals>.coupling at 0x7054fc35db20>
steps = 100, t_span = None, dt = 0.01

    def evolve_spatiotemporal(
        self,
        initial: torch.Tensor,
        coupling: Callable,
        steps: int = 100,
        t_span: Optional[Tuple[float, float]] = None,
        dt: Optional[float] = None
    ) -> List[torch.Tensor]:
        """Evolve spatiotemporal pattern with coupling.
    
        Args:
            initial: Initial state tensor
            coupling: Coupling function between spatial points
            steps: Number of evolution steps
            t_span: Optional time span (start, end)
            dt: Optional time step override
    
        Returns:
            List of evolved states
        """
        dt = dt if dt is not None else self.dt
    
        if t_span is not None:
            t_start, t_end = t_span
            t = t_start
            dt = (t_end - t_start) / steps
        else:
            t = 0.0
    
        evolution = [initial]
        state = initial
    
        for _ in range(steps):
            # Apply spatial coupling with time dependence
            try:
                coupled = coupling(state, t)
            except TypeError:
                # Fallback for time-independent coupling
>               coupled = coupling(state)
E               TypeError: test_spatiotemporal_evolution.<locals>.coupling() missing 1 required positional argument: 't'

src/neural/attention/pattern/dynamics.py:426: TypeError
_______________________ test_convergence_to_steady_state _______________________

pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x705543f70d70>
test_params = {'D': 0.1, 'batch_size': 4, 'dt': 0.01, 'dx': 1.0, ...}

    def test_convergence_to_steady_state(pattern_system, test_params):
        """Test that diffusion converges to steady state.
    
        This test verifies three key properties of diffusion:
        1. The system converges to a uniform steady state
        2. The mean value is preserved during diffusion
        3. Total mass is conserved throughout the process
    
        For each test pattern (impulse, checkerboard, gradient), we:
        - Apply diffusion until the state is sufficiently uniform
        - Verify the final state matches theoretical expectations
        - Check that mass is conserved during the entire process
        """
        for pattern in ['impulse', 'checkerboard', 'gradient']:
            state = create_test_state(test_params, pattern)
            initial_mean = state.mean(dim=(-2, -1), keepdim=True)
    
            # Apply diffusion for multiple steps
            current = state
            for _ in range(100):
                current = pattern_system.apply_diffusion(current, test_params['D'], test_params['dt'])
    
                # Verify mass conservation at each step
                assert_mass_conserved(state, current)
    
                # Check mean preservation
                current_mean = current.mean(dim=(-2, -1), keepdim=True)
                assert_tensor_equal(initial_mean, current_mean, msg="Mean should be preserved during diffusion")
    
            # Final state should be approximately uniform
>           assert torch.std(current) < 1e-3, "Should converge to uniform state"
E           AssertionError: Should converge to uniform state
E           assert tensor(0.0565) < 0.001
E            +  where tensor(0.0565) = <built-in method std of type object at 0x70560ced8b20>(tensor([[[[1.9858e-21, 1.4137e-20, 1.0172e-19,  ..., 4.6544e-19,\n           1.0172e-19, 1.4137e-20],\n          [1.4137e-20, 4.9937e-19, 6.2885e-18,  ..., 3.9432e-17,\n           6.2885e-18, 4.9937e-19],\n          [1.0172e-19, 6.2885e-18, 4.3425e-16,  ..., 4.7469e-15,\n           4.3425e-16, 6.2885e-18],\n          ...,\n          [4.6544e-19, 3.9432e-17, 4.7469e-15,  ..., 3.2004e-13,\n           4.7469e-15, 3.9432e-17],\n          [1.0172e-19, 6.2885e-18, 4.3425e-16,  ..., 4.7469e-15,\n           4.3425e-16, 6.2885e-18],\n          [1.4137e-20, 4.9937e-19, 6.2885e-18,  ..., 3.9432e-17,\n           6.2885e-18, 4.9937e-19]],\n\n         [[1.9858e-21, 1.4137e-20, 1.0172e-19,  ..., 4.6544e-19,\n           1.0172e-19, 1.4137e-20],\n          [1.4137e-20, 4.9937e-19, 6.2885e-18,  ..., 3.9432e-17,\n           6.2885e-18, 4.9937e-19],\n          [1.0172e-19, 6.2885e-18, 4.3425e-16,  ..., 4.7469e-15,\n           4.3425e-16, 6.2885e-18],\n          ...,\n          [4.6544e-19, 3.9432e-17, 4.7469e-15,  ..., 3.2004e-13,\n           4.7469e-15, 3.9432e-17],\n          [1.0172e-19, 6.2885e-18, 4.3425e-16,  ..., 4.7469e-15,\n           4.3425e-16, 6.2885e-18],\n          [1.4137e-20, 4.9937e-19, 6.2885e-18,  ..., 3.... ..., 4.6544e-19,\n           1.0172e-19, 1.4137e-20],\n          [1.4137e-20, 4.9937e-19, 6.2885e-18,  ..., 3.9432e-17,\n           6.2885e-18, 4.9937e-19],\n          [1.0172e-19, 6.2885e-18, 4.3425e-16,  ..., 4.7469e-15,\n           4.3425e-16, 6.2885e-18],\n          ...,\n          [4.6544e-19, 3.9432e-17, 4.7469e-15,  ..., 3.2004e-13,\n           4.7469e-15, 3.9432e-17],\n          [1.0172e-19, 6.2885e-18, 4.3425e-16,  ..., 4.7469e-15,\n           4.3425e-16, 6.2885e-18],\n          [1.4137e-20, 4.9937e-19, 6.2885e-18,  ..., 3.9432e-17,\n           6.2885e-18, 4.9937e-19]],\n\n         [[1.9858e-21, 1.4137e-20, 1.0172e-19,  ..., 4.6544e-19,\n           1.0172e-19, 1.4137e-20],\n          [1.4137e-20, 4.9937e-19, 6.2885e-18,  ..., 3.9432e-17,\n           6.2885e-18, 4.9937e-19],\n          [1.0172e-19, 6.2885e-18, 4.3425e-16,  ..., 4.7469e-15,\n           4.3425e-16, 6.2885e-18],\n          ...,\n          [4.6544e-19, 3.9432e-17, 4.7469e-15,  ..., 3.2004e-13,\n           4.7469e-15, 3.9432e-17],\n          [1.0172e-19, 6.2885e-18, 4.3425e-16,  ..., 4.7469e-15,\n           4.3425e-16, 6.2885e-18],\n          [1.4137e-20, 4.9937e-19, 6.2885e-18,  ..., 3.9432e-17,\n           6.2885e-18, 4.9937e-19]]]]))
E            +    where <built-in method std of type object at 0x70560ced8b20> = torch.std

tests/test_neural/test_attention/test_pattern/test_diffusion_properties.py:137: AssertionError
__________________ TestQuantumPatterns.test_quantum_evolution __________________

self = <test_quantum.TestQuantumPatterns object at 0x70551758ebd0>
quantum_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2bd4c0>

    def test_quantum_evolution(self, quantum_system):
        """Test quantum state evolution."""
        # Create test state
        state = torch.randn(1, quantum_system.dim, quantum_system.size, quantum_system.size)
    
        # Evolve state
>       evolved = quantum_system.compute_next_state(state)

tests/test_neural/test_attention/test_pattern/test_quantum.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/neural/attention/pattern/dynamics.py:110: in compute_next_state
    evolved = quantum_state.evolve(H, self.dt)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumState(amplitude=tensor([[[[1.0958, 0.7565, 0.5573, 1.0590, 0.2217, 0.4251, 2.7955, 1.2526],
          [0.3917, ....1416, 3.1416, 3.1416, 3.1416, 0.0000],
          [3.1416, 0.0000, 3.1416, 3.1416, 0.0000, 3.1416, 0.0000, 3.1416]]]]))
hamiltonian = tensor([[1.+0.j, 0.+0.j],
        [0.+0.j, 1.+0.j]]), dt = 0.01

    def evolve(self, hamiltonian: torch.Tensor, dt: float) -> 'QuantumState':
        """Evolve quantum state using Hamiltonian.
    
        Args:
            hamiltonian: Hamiltonian operator
            dt: Time step
    
        Returns:
            Evolved quantum state
        """
        # Convert to complex tensor
        state = self.state_vector
    
        # Time evolution operator
        U = torch.matrix_exp(-1j * hamiltonian * dt)
    
        # Evolve state
>       evolved = torch.matmul(U, state)
E       RuntimeError: Expected size for first two dimensions of batch2 tensor to be: [2, 2] but got: [2, 8].

src/neural/attention/pattern/quantum.py:38: RuntimeError
__________________ TestQuantumPatterns.test_quantum_potential __________________

self = <test_quantum.TestQuantumPatterns object at 0x70551758f170>
quantum_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7055175666f0>

    def test_quantum_potential(self, quantum_system):
        """Test quantum potential computation."""
        # Create test state
        state = torch.randn(1, quantum_system.dim, quantum_system.size, quantum_system.size)
    
        # Compute potential
>       V = quantum_system.compute_quantum_potential(state)

tests/test_neural/test_attention/test_pattern/test_quantum.py:95: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7055175666f0>
state = tensor([[[[-0.0878, -0.6345, -0.1882,  0.0073,  0.0211, -2.0226,  0.2988,
            0.5028],
          [-0.4855,  0....
           -0.4184],
          [ 0.4789,  1.4121, -0.4181, -0.4856,  1.5565, -0.5576,  0.4395,
            1.4834]]]])

    def compute_quantum_potential(self, state: torch.Tensor) -> torch.Tensor:
        """Compute quantum potential for pattern state.
    
        Args:
            state: Pattern state tensor
    
        Returns:
            Quantum potential tensor
        """
        if not self.quantum_enabled:
            raise RuntimeError("Quantum features not enabled")
    
        # Convert to quantum state
        quantum_state = self._to_quantum_state(state)
    
        # Compute quantum geometric tensor
        Q = self.quantum_tensor.compute_tensor(quantum_state)
    
        # Extract metric
        g, _ = self.quantum_tensor.decompose(Q)
    
        # Compute quantum potential (simplified version)
        # TODO: Implement full quantum potential
        psi = quantum_state.state_vector
        laplacian = torch.zeros_like(psi)
        for i in range(self.dim):
            for j in range(self.dim):
>               laplacian += g[i,j] * torch.gradient(torch.gradient(psi, dim=i)[0], dim=j)[0]
E               RuntimeError: torch.gradient expected each dimension size to be at least edge_order+1

src/neural/attention/pattern/dynamics.py:1141: RuntimeError
___________________________ test_reaction_diffusion ____________________________

pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2e9f40>
grid_size = 32, batch_size = 8

    def test_reaction_diffusion(pattern_system, grid_size, batch_size):
        """Test reaction-diffusion dynamics."""
        # Create initial state (Turing-like pattern)
        state = torch.randn(batch_size, 2, grid_size, grid_size)  # 2 species
    
        # Define diffusion tensor (different rates for species)
        diffusion_tensor = torch.tensor([[0.1, 0.0], [0.0, 0.05]])
    
        # Define reaction term (activator-inhibitor)
        def reaction_term(state):
            u, v = state[:, 0:1], state[:, 1:2]  # Keep dimensions
            # Mass-conserving reaction terms
            du = u * v / (1 + u**2) - u  # Saturating reaction
            dv = u**2 / (1 + u**2) - v   # Balancing term
            return torch.cat([du, dv], dim=1)  # Preserve dimensions
    
        # Evolve system
>       evolved = pattern_system.reaction_diffusion(
            state, diffusion_tensor, reaction_term
        )

tests/test_neural/test_attention/test_pattern/test_reaction_diffusion.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2e9f40>
state = tensor([[[[-4.6448e-01,  3.0665e-01,  8.4515e-01,  ..., -9.6485e-01,
            1.2767e+00,  2.2835e+00],
          [...401e+00],
          [ 4.8170e-01,  2.1700e+00, -1.0076e+00,  ..., -9.1855e-01,
            7.3426e-01,  2.8244e-01]]]])
reaction_term = tensor([[0.1000, 0.0000],
        [0.0000, 0.0500]])
max_iterations = <function test_reaction_diffusion.<locals>.reaction_term at 0x7054fc35de40>
dt = 0.1

    def reaction_diffusion(
        self,
        state: torch.Tensor,
        reaction_term: Optional[Callable] = None,
        max_iterations: int = 100,
        dt: float = 0.1
    ) -> torch.Tensor:
        """Compute one step of reaction-diffusion dynamics.
    
        Args:
            state: Current state tensor
            reaction_term: Optional custom reaction term
            max_iterations: Maximum iterations for numerical integration
            dt: Time step for numerical integration
    
        Returns:
            Updated state tensor
        """
        # Ensure state has batch dimension
        if len(state.shape) == 3:
            state = state.unsqueeze(0)  # Add batch dimension
    
        # Apply reaction term
        if reaction_term is not None:
>           reaction = reaction_term(state)
E           TypeError: 'Tensor' object is not callable

src/neural/attention/pattern/dynamics.py:886: TypeError
___________________________ test_stability_analysis ____________________________

pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2e9490>
grid_size = 32

    def test_stability_analysis(pattern_system, grid_size):
        """Test pattern stability analysis."""
        # Create test pattern
        pattern = torch.randn(1, 2, grid_size, grid_size)
    
        # Create perturbation
        perturbation = 0.1 * torch.randn_like(pattern)
    
        # Analyze stability
>       metrics = pattern_system.stability_analysis(pattern, perturbation)

tests/test_neural/test_attention/test_pattern/test_stability.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/neural/attention/pattern/dynamics.py:928: in stability_analysis
    return self.stability.analyze_stability(state_tensor, perturbation)
src/neural/attention/pattern/stability.py:209: in analyze_stability
    nonlinear_stability = self.compute_stability(perturbed_state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.neural.attention.pattern.stability.StabilityAnalyzer object at 0x7054fc2ebb60>
state = tensor([[[[ 0.9834, -3.3682,  1.0009,  ..., -1.9723,  0.1943, -1.7678],
          [ 0.7666,  0.3996, -0.1806,  ..., -0..., -0.1433,  ...,  1.1446,  0.8464, -0.1426],
          [-0.0287,  0.4346, -1.5580,  ..., -1.8942, -1.2583,  0.3936]]]])
reaction_term = None

    def compute_stability(
        self,
        state: torch.Tensor,
        reaction_term: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
    ) -> torch.Tensor:
        """Compute stability value for a state.
    
        Args:
            state (torch.Tensor): State to compute stability for
            reaction_term (Callable, optional): Reaction function. If None, uses system reaction.
    
        Returns:
            torch.Tensor: Stability value
        """
        # Use system reaction if none provided
        if reaction_term is None:
>           reaction_term = self.pattern_system.reaction.compute_reaction
E           AttributeError: 'ReactionSystem' object has no attribute 'compute_reaction'

src/neural/attention/pattern/stability.py:70: AttributeError
______________ TestPatternDynamics.test_stability_analysis_basic _______________

self = <test_pattern_dynamics.TestPatternDynamics object at 0x70551758eb10>
pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc3a58b0>

    def test_stability_analysis_basic(
        self, pattern_system: PatternDynamics
    ) -> None:
        """Test basic pattern stability analysis."""
        # Create test pattern
        pattern = torch.randn(1, pattern_system.dim, pattern_system.size, pattern_system.size)
    
        # Create perturbation
        perturbation = 0.1 * torch.randn_like(pattern)
    
        # Analyze stability using stability analyzer
>       metrics = pattern_system.stability_analysis(pattern, perturbation)

tests/test_neural/test_attention/test_pattern_dynamics.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/neural/attention/pattern/dynamics.py:928: in stability_analysis
    return self.stability.analyze_stability(state_tensor, perturbation)
src/neural/attention/pattern/stability.py:209: in analyze_stability
    nonlinear_stability = self.compute_stability(perturbed_state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.neural.attention.pattern.stability.StabilityAnalyzer object at 0x7054fc3a7980>
state = tensor([[[[ 0.1222,  1.4078, -2.3900,  0.3507, -0.2946,  0.2291, -1.4957,
           -0.5503],
          [ 1.2367, -0....
           -0.5398],
          [-0.0535,  0.2307,  1.8141, -1.2612,  1.0686,  0.2498, -1.0398,
            0.8434]]]])
reaction_term = None

    def compute_stability(
        self,
        state: torch.Tensor,
        reaction_term: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
    ) -> torch.Tensor:
        """Compute stability value for a state.
    
        Args:
            state (torch.Tensor): State to compute stability for
            reaction_term (Callable, optional): Reaction function. If None, uses system reaction.
    
        Returns:
            torch.Tensor: Stability value
        """
        # Use system reaction if none provided
        if reaction_term is None:
>           reaction_term = self.pattern_system.reaction.compute_reaction
E           AttributeError: 'ReactionSystem' object has no attribute 'compute_reaction'

src/neural/attention/pattern/stability.py:70: AttributeError
_____________ TestPatternDynamics.test_stability_analysis_advanced _____________

self = <test_pattern_dynamics.TestPatternDynamics object at 0x7055175b8a70>
pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2e9220>

    def test_stability_analysis_advanced(
        self, pattern_system: PatternDynamics
    ) -> None:
        """Test advanced stability analysis features."""
        # Create test pattern
        pattern = torch.randn(1, pattern_system.dim, pattern_system.size, pattern_system.size)
    
        # Test Lyapunov spectrum
        lyapunov_spectrum = pattern_system.compute_lyapunov_spectrum(pattern)
        assert len(lyapunov_spectrum) > 0, "Should compute Lyapunov exponents"
        assert torch.all(
>           torch.imag(lyapunov_spectrum) == 0
        ), "Lyapunov exponents should be real"
E       RuntimeError: imag is not implemented for tensors with non-complex dtypes.

tests/test_neural/test_attention/test_pattern_dynamics.py:95: RuntimeError
__________________ TestPatternDynamics.test_pattern_formation __________________

self = <test_pattern_dynamics.TestPatternDynamics object at 0x7055175b8c50>
pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2ea9f0>

    def test_pattern_formation(
        self, pattern_system: PatternDynamics
    ) -> None:
        """Test pattern formation dynamics."""
        # Create initial state with small random perturbations
        state = torch.ones(1, pattern_system.dim, pattern_system.size, pattern_system.size) + 0.01 * torch.randn(1, pattern_system.dim, pattern_system.size, pattern_system.size)
    
        # Define reaction term
        def reaction_term(state: torch.Tensor) -> torch.Tensor:
            """Pattern-forming reaction terms."""
            u, v = state[:, 0:1], state[:, 1:2]
            du = u**2 * v - u
            dv = u**2 - v
            return torch.cat([du, dv], dim=1)
    
        # Evolve system
        time_evolution = pattern_system.evolve_pattern(
            state, diffusion_coefficient=0.1, reaction_term=reaction_term, steps=100
        )
    
        # Test pattern formation
        final_pattern = time_evolution[-1]
>       assert pattern_system.detect_pattern_formation(time_evolution), "Should detect pattern formation"
E       AssertionError: Should detect pattern formation
E       assert False
E        +  where False = detect_pattern_formation([tensor([[[[1.0018, 0.9882, 0.9992, 1.0055, 1.0161, 1.0038, 0.9814, 0.9972],\n          [1.0013, 0.9829, 0.9932, 0.9894, 1.0011, 1.0015, 1.0134, 1.0132],\n          [0.9967, 1.0011, 0.9954, 0.9987, 0.9814, 1.0039, 1.0114, 1.0015],\n          [1.0108, 1.0081, 0.9896, 0.9921, 0.9893, 1.0274, 0.9856, 1.0089],\n          [0.9957, 1.0032, 1.0083, 1.0001, 0.9823, 0.9975, 0.9930, 1.0044],\n          [1.0119, 0.9908, 0.9976, 0.9900, 0.9830, 1.0085, 1.0006, 1.0171],\n          [1.0091, 1.0131, 0.9960, 0.9927, 1.0029, 1.0068, 0.9896, 0.9920],\n          [1.0050, 1.0030, 0.9991, 1.0024, 1.0034, 0.9921, 0.9966, 1.0106]],\n\n         [[0.9940, 0.9947, 1.0105, 0.9880, 0.9948, 0.9853, 0.9927, 1.0002],\n          [1.0048, 0.9998, 0.9975, 1.0009, 1.0054, 0.9836, 0.9962, 0.9946],\n          [1.0107, 0.9945, 0.9901, 1.0115, 0.9844, 0.9990, 0.9989, 0.9982],\n          [1.0058, 0.9903, 1.0011, 0.9907, 0.9973, 1.0005, 0.9776, 0.9855],\n          [1.0234, 0.9886, 1.0005, 0.9741, 1.0162, 0.9992, 0.9936, 1.0039],\n          [1.0061, 0.9904, 1.0153, 0.9883, 1.0092, 1.0278, 0.9865, 0.9849],\n          [1.0068, 0.9827, 1.0004, 1.0053, 1.0038, 0.9873, 1.0097, 0.9952],\n          [1.0064, 1.0136, 0.9974, 0.9919, 1.0173, 1.00...1.0528, 1.0587, 1.0708, 1.0566, 1.0321, 1.0500],\n          [1.0549, 1.0342, 1.0455, 1.0414, 1.0547, 1.0540, 1.0678, 1.0676],\n          [1.0501, 1.0541, 1.0475, 1.0524, 1.0318, 1.0575, 1.0658, 1.0548],\n          [1.0655, 1.0617, 1.0417, 1.0439, 1.0412, 1.0837, 1.0360, 1.0623],\n          [1.0496, 1.0561, 1.0624, 1.0519, 1.0344, 1.0503, 1.0451, 1.0583],\n          [1.0667, 1.0424, 1.0514, 1.0414, 1.0348, 1.0642, 1.0531, 1.0713],\n          [1.0637, 1.0668, 1.0487, 1.0453, 1.0566, 1.0600, 1.0421, 1.0441],\n          [1.0590, 1.0572, 1.0521, 1.0554, 1.0579, 1.0448, 1.0495, 1.0662]],\n\n         [[1.0463, 1.0455, 1.0625, 1.0406, 1.0486, 1.0377, 1.0428, 1.0520],\n          [1.0570, 1.0501, 1.0488, 1.0519, 1.0576, 1.0358, 1.0497, 1.0481],\n          [1.0625, 1.0467, 1.0417, 1.0635, 1.0345, 1.0515, 1.0522, 1.0505],\n          [1.0590, 1.0433, 1.0521, 1.0420, 1.0483, 1.0556, 1.0281, 1.0385],\n          [1.0751, 1.0410, 1.0535, 1.0262, 1.0664, 1.0510, 1.0449, 1.0564],\n          [1.0595, 1.0415, 1.0672, 1.0393, 1.0595, 1.0809, 1.0386, 1.0388],\n          [1.0599, 1.0362, 1.0521, 1.0566, 1.0562, 1.0401, 1.0607, 1.0464],\n          [1.0590, 1.0661, 1.0494, 1.0442, 1.0698, 1.0574, 1.0538, 1.0744]]]]), ...])
E        +    where detect_pattern_formation = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2ea9f0>.detect_pattern_formation

tests/test_neural/test_attention/test_pattern_dynamics.py:133: AssertionError
____________________ TestPatternDynamics.test_forward_pass _____________________

self = <test_pattern_dynamics.TestPatternDynamics object at 0x7055175b8e30>
pattern_system = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2bf4a0>

    def test_forward_pass(
        self, pattern_system: PatternDynamics
    ) -> None:
        """Test forward pass of pattern dynamics."""
        # Create input states
        seq_length = 16
        states = torch.randn(1, seq_length, pattern_system.hidden_dim)
    
        # Run forward pass
>       output = pattern_system(states, return_patterns=True)

tests/test_neural/test_attention/test_pattern_dynamics.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/neural/attention/pattern/dynamics.py:1112: in __call__
    return self.forward(states, return_patterns)
src/neural/attention/pattern/dynamics.py:1089: in forward
    patterns = self.evolve_pattern(states)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc2bf4a0>
pattern = tensor([[[-1.2999, -0.7753, -0.1829,  ..., -1.2292, -0.9799, -0.6705],
         [-0.1965, -1.7372, -1.4191,  ..., -0.8...95, -0.8937,  ...,  0.2746,  0.9012, -0.4202],
         [ 1.6332, -0.3425, -0.7838,  ...,  1.5706,  2.4543, -0.3422]]])
diffusion_coefficient = 0.1, reaction_term = None, steps = 100

    def evolve_pattern(
        self,
        pattern: torch.Tensor,
        diffusion_coefficient: float = 0.1,
        reaction_term: Optional[Callable] = None,
        steps: int = 100
    ) -> List[torch.Tensor]:
        """Evolve pattern forward in time.
    
        Args:
            pattern: Initial pattern
            diffusion_coefficient: Diffusion coefficient
            reaction_term: Optional custom reaction term
            steps: Number of timesteps
    
        Returns:
            List of evolved patterns
        """
        trajectory = []
        current = pattern
    
        for _ in range(steps):
            trajectory.append(current)
            # Apply reaction and diffusion with optional custom reaction term
            if reaction_term is not None:
                reaction = reaction_term(current)
            else:
                reaction = self.reaction.reaction_term(current)
            diffusion = self.diffusion.apply_diffusion(current, diffusion_coefficient=diffusion_coefficient, dt=self.dt)
>           current = current + self.dt * (reaction + diffusion)
E           RuntimeError: The size of tensor a (2) must match the size of tensor b (16) at non-singleton dimension 2

src/neural/attention/pattern/dynamics.py:151: RuntimeError
________ TestQuantumGeometricAttention.test_attention_state_preparation ________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ec410>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64, num_heads = 8

    def test_attention_state_preparation(
        self, attention_layer, batch_size, seq_length, hidden_dim, num_heads
    ):
        """Test attention state preparation and properties."""
        # Create input tensor
        x = torch.randn(batch_size, seq_length, hidden_dim)
        mask = torch.ones(batch_size, seq_length).bool()
    
        # Prepare attention state
        state = attention_layer.prepare_attention_state(x, mask)
    
        # Test state properties
        assert isinstance(state, AttentionState), "Should return AttentionState"
        assert state.quantum_state is not None, "Should have quantum state"
        assert state.geometric_state is not None, "Should have geometric state"
    
        # Test state dimensions
        assert state.quantum_state.shape[0] == batch_size, "Batch dimension preserved"
>       assert state.quantum_state.shape[1] == attention_layer.num_heads, "Head dimension correct"
E       AssertionError: Head dimension correct
E       assert 32 == 8
E        +  where 8 = QuantumGeometricAttention(\n  (attention): QuantumMotivicTile(\n    (query): Linear(in_features=64, out_features=64, bias=True)\n    (key): Linear(in_features=64, out_features=64, bias=True)\n    (value): Linear(in_features=64, out_features=64, bias=True)\n    (output): Linear(in_features=64, out_features=64, bias=True)\n    (dropout_layer): Dropout(p=0.1, inplace=False)\n    (cohomology_proj): Linear(in_features=64, out_features=32, bias=True)\n    (field_proj): Linear(in_features=64, out_features=64, bias=True)\n    (height_proj): Linear(in_features=32, out_features=1, bias=True)\n  )\n  (flow): PatternFlow(\n    (metric): RiemannianMetric()\n    (arithmetic): ArithmeticDynamics(\n      (height_map): Linear(in_features=64, out_features=4, bias=True)\n      (flow): Linear(in_features=4, out_features=4, bias=True)\n      (l_function): Sequential(\n        (0): Linear(in_features=64, out_features=32, bias=True)\n        (1): SiLU()\n        (2): Linear(in_features=32, out_features=4, bias=True)\n      )\n      (adelic_proj): Linear(in_features=64, out_features=32, bias=True)\n      (output_proj): Linear(in_features=4, out_features=64, bias=True)\n    )\n    (flow_field): ModuleList(\n      (0): Linear(in_...(metric_net): Sequential(\n      (0): Linear(in_features=8, out_features=64, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n    )\n    (ricci_net): Sequential(\n      (0): Linear(in_features=16, out_features=64, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=64, out_features=64, bias=True)\n    )\n  )\n  (arithmetic): ArithmeticPattern(\n    (layers): ModuleList(\n      (0-2): 3 x ArithmeticDynamics(\n        (height_map): Linear(in_features=64, out_features=4, bias=True)\n        (flow): Linear(in_features=4, out_features=4, bias=True)\n        (l_function): Sequential(\n          (0): Linear(in_features=64, out_features=32, bias=True)\n          (1): SiLU()\n          (2): Linear(in_features=32, out_features=4, bias=True)\n        )\n        (adelic_proj): Linear(in_features=64, out_features=32, bias=True)\n        (output_proj): Linear(in_features=4, out_features=64, bias=True)\n      )\n    )\n    (pattern_proj): Linear(in_features=64, out_features=64, bias=True)\n  )\n  (exp_map): HyperbolicExponential()\n  (log_map): HyperbolicLogarithm()\n  (transport): ParallelTransport()\n  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)\n).num_heads

tests/test_neural/test_attention/test_quantum_geometric_attention.py:109: AssertionError
_______ TestQuantumGeometricAttention.test_attention_pattern_computation _______

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ec5f0>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64, num_heads = 8

    def test_attention_pattern_computation(
        self, attention_layer, batch_size, seq_length, hidden_dim, num_heads
    ):
        """Test attention pattern computation."""
        # Create query, key, value tensors
        query = torch.randn(batch_size, attention_layer.num_heads, seq_length, hidden_dim // attention_layer.num_heads)
        key = torch.randn(batch_size, attention_layer.num_heads, seq_length, hidden_dim // attention_layer.num_heads)
        value = torch.randn(batch_size, attention_layer.num_heads, seq_length, hidden_dim // attention_layer.num_heads)
    
        # Compute attention patterns
>       result = attention_layer.compute_attention_patterns(
            query, key, value, scale=1.0
        )
E       TypeError: QuantumGeometricAttention.compute_attention_patterns() got an unexpected keyword argument 'scale'

tests/test_neural/test_attention/test_quantum_geometric_attention.py:133: TypeError
_________ TestQuantumGeometricAttention.test_geometric_attention_flow __________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ec860>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64, num_heads = 8

    def test_geometric_attention_flow(
        self, attention_layer, batch_size, seq_length, hidden_dim, num_heads
    ):
        """Test geometric attention flow computation."""
        # Create patterns and metric tensor
        patterns = torch.softmax(
            torch.randn(batch_size, attention_layer.num_heads, seq_length, seq_length), dim=-1
        )
        metric = torch.eye(hidden_dim).expand(batch_size, attention_layer.num_heads, -1, -1)
    
        # Compute geometric flow
>       flow_result = attention_layer.geometric_attention_flow(patterns, metric)

tests/test_neural/test_attention/test_quantum_geometric_attention.py:175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
name = 'geometric_attention_flow'

    def __getattr__(self, name: str) -> Union[Tensor, "Module"]:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'QuantumGeometricAttention' object has no attribute 'geometric_attention_flow'

venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1928: AttributeError
________ TestQuantumGeometricAttention.test_quantum_classical_interface ________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ecad0>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64

    def test_quantum_classical_interface(
        self, attention_layer, batch_size, seq_length, hidden_dim
    ):
        """Test quantum-classical information interface."""
        # Create classical input
        classical_input = torch.randn(batch_size, seq_length, hidden_dim)
    
        # Convert to quantum state
        quantum_state = attention_layer.classical_to_quantum(classical_input)
    
        # Test quantum state properties
>       assert attention_layer.is_valid_quantum_state(
            quantum_state
        ), "Should be valid quantum state"

tests/test_neural/test_attention/test_quantum_geometric_attention.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
name = 'is_valid_quantum_state'

    def __getattr__(self, name: str) -> Union[Tensor, "Module"]:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'QuantumGeometricAttention' object has no attribute 'is_valid_quantum_state'

venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1928: AttributeError
__________ TestQuantumGeometricAttention.test_multi_head_integration ___________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ecd10>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64, num_heads = 8

    def test_multi_head_integration(
        self, attention_layer, batch_size, seq_length, hidden_dim, num_heads
    ):
        """Test multi-head attention integration."""
        # Create per-head inputs
        head_states = [
            torch.randn(batch_size, seq_length, hidden_dim // attention_layer.num_heads)
            for _ in range(attention_layer.num_heads)
        ]
    
        # Integrate heads
>       integrated = attention_layer.integrate_heads(head_states)

tests/test_neural/test_attention/test_quantum_geometric_attention.py:238: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
name = 'integrate_heads'

    def __getattr__(self, name: str) -> Union[Tensor, "Module"]:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'QuantumGeometricAttention' object has no attribute 'integrate_heads'

venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1928: AttributeError
_____________ TestQuantumGeometricAttention.test_geometric_phases ______________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ecf80>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64

    def test_geometric_phases(
        self, attention_layer, batch_size, seq_length, hidden_dim
    ):
        """Test quantum geometric phases in attention."""
    
        # Create cyclic attention path
        def create_cyclic_path(steps: int = 100):
            """Create a cyclic path in attention parameter space."""
            t = torch.linspace(0, 2 * np.pi, steps)
            return [
                attention_layer.create_attention_parameters(
                    torch.cos(ti), torch.sin(ti)
                )
                for ti in t
            ]
    
        # Compute Berry phase
>       path = create_cyclic_path()

tests/test_neural/test_attention/test_quantum_geometric_attention.py:287: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_neural/test_attention/test_quantum_geometric_attention.py:280: in create_cyclic_path
    attention_layer.create_attention_parameters(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = tensor(1.), seq_len = tensor(0.)

    def create_attention_parameters(self, batch_size: int, seq_len: int) -> Dict[str, torch.Tensor]:
        """Create attention mechanism parameters.
    
        Args:
            batch_size: Batch size
            seq_len: Sequence length
    
        Returns:
            Dictionary of attention parameters
        """
        # Create query, key, value projections
>       q_weight = torch.randn(batch_size, self.num_heads, seq_len, self.hidden_dim // self.num_heads)
E       TypeError: randn() received an invalid combination of arguments - got (Tensor, int, Tensor, int), but expected one of:
E        * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)
E        * (tuple of ints size, *, torch.Generator generator, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)
E        * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)
E        * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)

src/core/tiling/quantum_geometric_attention.py:380: TypeError
____________ TestQuantumGeometricAttention.test_manifold_curvature _____________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ed1c0>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64

    def test_manifold_curvature(
        self, attention_layer, batch_size, seq_length, hidden_dim
    ):
        """Test attention manifold curvature properties."""
        # Create local patch of attention states
        states = torch.randn(batch_size, seq_length, hidden_dim)
    
        # Compute metric tensor
>       metric = attention_layer.compute_metric_tensor(states)

tests/test_neural/test_attention/test_quantum_geometric_attention.py:323: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
state = tensor([[[ 0.9356,  0.2136, -1.0227,  ..., -3.7034, -1.2906,  0.5950],
         [-0.3959, -0.3968, -1.2246,  ..., -1.0...19, -0.1895,  ...,  1.7006,  0.6285, -0.2675],
         [-1.0950, -0.4524,  0.1307,  ..., -0.5370,  0.0505,  0.3904]]])

    def compute_metric_tensor(self, state: AttentionState) -> torch.Tensor:
        """Compute metric tensor for attention manifold.
    
        Args:
            state: Current attention state
    
        Returns:
            Metric tensor
        """
        # Extract geometric state
>       g_state = state.geometric_state
E       AttributeError: 'Tensor' object has no attribute 'geometric_state'

src/core/tiling/quantum_geometric_attention.py:404: AttributeError
__________ TestQuantumGeometricAttention.test_attention_entanglement ___________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ed400>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64

    def test_attention_entanglement(
        self, attention_layer, batch_size, seq_length, hidden_dim
    ):
        """Test entanglement properties in attention states."""
        # Create attention state
        state = attention_layer.prepare_attention_state(
            torch.randn(batch_size, seq_length, hidden_dim),
            torch.ones(batch_size, seq_length).bool(),
        )
    
        # Test bipartite entanglement
        def test_bipartite(split_idx: int):
            """Test entanglement across bipartition."""
            entropy = attention_layer.compute_entanglement_entropy(
                state.quantum_state, split_idx
            )
            assert entropy >= 0, "Entropy should be non-negative"
            assert entropy <= np.log(2) * min(
                split_idx, seq_length - split_idx
            ), "Entropy should satisfy area law"
            return entropy
    
        # Test various bipartitions
>       entropies = [test_bipartite(i) for i in range(1, seq_length)]

tests/test_neural/test_attention/test_quantum_geometric_attention.py:377: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_neural/test_attention/test_quantum_geometric_attention.py:367: in test_bipartite
    entropy = attention_layer.compute_entanglement_entropy(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
name = 'compute_entanglement_entropy'

    def __getattr__(self, name: str) -> Union[Tensor, "Module"]:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'QuantumGeometricAttention' object has no attribute 'compute_entanglement_entropy'

venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1928: AttributeError
_____________ TestQuantumGeometricAttention.test_error_correction ______________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ed640>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64

    def test_error_correction(
        self, attention_layer, batch_size, seq_length, hidden_dim
    ):
        """Test quantum error correction in attention."""
        # Create code state
        code_state = attention_layer.prepare_code_state(
            torch.randn(batch_size, seq_length, hidden_dim)
        )
    
        # Test encoding map
>       logical_ops = attention_layer.get_logical_operators()

tests/test_neural/test_attention/test_quantum_geometric_attention.py:405: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
name = 'get_logical_operators'

    def __getattr__(self, name: str) -> Union[Tensor, "Module"]:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'QuantumGeometricAttention' object has no attribute 'get_logical_operators'

venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1928: AttributeError
___________ TestQuantumGeometricAttention.test_topological_features ____________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ed880>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64

    def test_topological_features(
        self, attention_layer, batch_size, seq_length, hidden_dim
    ):
        """Test topological features in attention."""
        # Create attention complex
>       attention_complex = attention_layer.build_attention_complex(
            torch.randn(batch_size, seq_length, hidden_dim)
        )
E       TypeError: QuantumGeometricAttention.build_attention_complex() missing 2 required positional arguments: 'key' and 'value'

tests/test_neural/test_attention/test_quantum_geometric_attention.py:439: TypeError
____________ TestQuantumGeometricAttention.test_attention_patterns _____________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ec200>
attention_layer = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
batch_size = 16, seq_length = 32, hidden_dim = 64, num_heads = 8

    def test_attention_patterns(
        self,
        attention_layer: QuantumGeometricAttention,
        batch_size: int,
        seq_length: int,
        hidden_dim: int,
        num_heads: int,
    ) -> None:
        """Test attention pattern computation."""
        # Create query, key, value tensors
        query = torch.randn(batch_size, attention_layer.num_heads, seq_length, hidden_dim // attention_layer.num_heads)
        key = torch.randn(batch_size, attention_layer.num_heads, seq_length, hidden_dim // attention_layer.num_heads)
        value = torch.randn(batch_size, attention_layer.num_heads, seq_length, hidden_dim // attention_layer.num_heads)
    
        # Compute attention patterns
>       result = attention_layer.compute_attention_patterns(query, key)

tests/test_neural/test_attention/test_quantum_geometric_attention.py:494: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/core/tiling/quantum_geometric_attention.py:308: in compute_attention_patterns
    geometric_features = self._compute_geometric_features(state.geometric_state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumGeometricAttention(
  (attention): QuantumMotivicTile(
    (query): Linear(in_features=64, out_features=64, bia...icLogarithm()
  (transport): ParallelTransport()
  (pattern_proj): Linear(in_features=64, out_features=64, bias=True)
)
x = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0... [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])

    def _compute_geometric_features(self, x: torch.Tensor) -> torch.Tensor:
        """Compute geometric features using tensor operations."""
        # Use proper tensor operations
>       features = F.linear(x, self.metric)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (4096x8 and 64x64)

src/core/tiling/quantum_geometric_attention.py:282: RuntimeError
___________ TestQuantumGeometricAttention.test_geometric_structures ____________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175edac0>
geometric_structures = GeometricStructures(dim=64, manifold_type='hyperbolic', curvature=-1.0, parallel_transport_method='schild')
hidden_dim = 64

    def test_geometric_structures(self, geometric_structures, hidden_dim):
        """Test geometric structures functionality."""
        # Test metric initialization
>       assert geometric_structures.metric.shape == (hidden_dim, hidden_dim)
E       AttributeError: 'GeometricStructures' object has no attribute 'metric'

tests/test_neural/test_attention/test_quantum_geometric_attention.py:515: AttributeError
_____________ TestQuantumGeometricAttention.test_pattern_dynamics ______________

self = <test_quantum_geometric_attention.TestQuantumGeometricAttention object at 0x7055175ed550>
pattern_dynamics = <src.core.tiling.quantum_geometric_attention.PatternDynamics object at 0x7054fc39ffb0>
hidden_dim = 64, batch_size = 16

    def test_pattern_dynamics(self, pattern_dynamics, hidden_dim, batch_size):
        """Test pattern dynamics functionality."""
        # Test pattern library initialization
        assert pattern_dynamics.patterns.shape == (64, hidden_dim)
>       assert pattern_dynamics.pattern_importance.shape == (64,)
E       AttributeError: 'PatternDynamics' object has no attribute 'pattern_importance'

tests/test_neural/test_attention/test_quantum_geometric_attention.py:530: AttributeError
_________________ TestGeometricFlow.test_geometric_invariants __________________

self = <test_geometric_flow.TestGeometricFlow object at 0x705517410710>
flow = GeometricFlow(
  (ricci): RicciTensorNetwork(
    (metric_network): Sequential(
      (0): Linear(in_features=2, out_f...
      (2): Linear(in_features=32, out_features=1, bias=True)
      (3): Softplus(beta=1.0, threshold=20.0)
    )
  )
)
random_states = tensor([[-0.3726,  0.4829, -1.2544,  0.6674],
        [ 0.3203,  0.7761, -0.2529, -2.3189],
        [-1.5616,  0.4271, -1.1491, -0.2731],
        [-0.1609, -0.5597, -0.8349, -0.4557]], requires_grad=True)
geometric_validator = <src.validation.geometric.flow.TilingFlowValidator object at 0x70551608cbc0>

    @pytest.mark.dependency(depends=["TestGeometricFlow::test_flow_normalization"])
    def test_geometric_invariants(self, flow, random_states, geometric_validator):
        """Test geometric invariant preservation."""
        # Extract position components
        points = random_states[:, :flow.manifold_dim]
    
        # Compute metric and flow
        metric = flow.compute_metric(points)
        ricci = flow.compute_ricci_tensor(metric, points)
        evolved_metric, _ = flow.flow_step(metric, ricci)
    
        # Validate geometric invariants
>       result = geometric_validator.validate_invariants(flow, points, evolved_metric)
E       AttributeError: 'TilingFlowValidator' object has no attribute 'validate_invariants'

tests/test_neural/test_flow/test_geometric_flow.py:172: AttributeError
___________________ TestGeometricFlow.test_flow_convergence ____________________

self = <test_geometric_flow.TestGeometricFlow object at 0x705517410e60>
flow = GeometricFlow(
  (ricci): RicciTensorNetwork(
    (metric_network): Sequential(
      (0): Linear(in_features=2, out_f...
      (2): Linear(in_features=32, out_features=1, bias=True)
      (3): Softplus(beta=1.0, threshold=20.0)
    )
  )
)
random_states = tensor([[-0.2167, -0.4315,  0.5744, -0.2467],
        [-0.0357, -0.5927, -2.3690, -0.9363],
        [ 0.1261, -0.6856, -0.4457, -0.5390],
        [-0.6094, -0.0442, -0.5373, -0.5040]], requires_grad=True)
convergence_validator = <src.validation.geometric.flow.TilingFlowValidator object at 0x7054fc3a6c90>

    @pytest.mark.dependency(depends=["TestGeometricFlow::test_flow_stability"])
    def test_flow_convergence(self, flow, random_states, convergence_validator):
        """Test that flow converges to stable points."""
        points = random_states[:, :flow.manifold_dim]
        metric = flow.compute_metric(points)
        ricci = flow.compute_ricci_tensor(metric, points)
    
        # Evolve to convergence
        for _ in range(100):
            metric, _ = flow.flow_step(metric, ricci)
    
        # Validate convergence
>       result = convergence_validator.validate_convergence(flow, points, metric)
E       AttributeError: 'TilingFlowValidator' object has no attribute 'validate_convergence'

tests/test_neural/test_flow/test_geometric_flow.py:225: AttributeError
______________ TestHamiltonianSystem.test_hamiltonian_computation ______________

self = <test_hamiltonian.TestHamiltonianSystem object at 0x705517412f90>
hamiltonian_system = HamiltonianSystem(
  (hamiltonian_network): Sequential(
    (0): Linear(in_features=4, out_features=128, bias=True)
  ...res=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=1, bias=True)
  )
)
phase_dim = 4, batch_size = 8

    def test_hamiltonian_computation(self, hamiltonian_system: HamiltonianSystem, phase_dim: int, batch_size: int):
        """Test Hamiltonian energy computation."""
        # Create test phase space points
        state = torch.randn(batch_size, phase_dim // 2)  # Position
        momentum = torch.randn(batch_size, phase_dim // 2)  # Momentum
        phase_point = PhaseSpacePoint(position=state, momentum=momentum, time=0.0)
    
        # Compute energy
        energy = hamiltonian_system.compute_energy(torch.cat([state, momentum], dim=-1))
    
        # Test energy properties
        assert energy.shape == (batch_size,), "Energy should be scalar per batch"
>       assert torch.all(energy >= 0), "Energy should be non-negative"
E       AssertionError: Energy should be non-negative
E       assert tensor(False)
E        +  where tensor(False) = <built-in method all of type object at 0x70560ced8b20>(tensor([-0.0337,  0.0643,  0.1319,  0.3349, -0.0072,  0.1631,  0.0891, -0.2515],\n       grad_fn=<SqueezeBackward1>) >= 0)
E        +    where <built-in method all of type object at 0x70560ced8b20> = torch.all

tests/test_neural/test_flow/test_hamiltonian.py:59: AssertionError
_____________________ TestHamiltonianSystem.test_evolution _____________________

self = <test_hamiltonian.TestHamiltonianSystem object at 0x705517411940>
hamiltonian_system = HamiltonianSystem(
  (hamiltonian_network): Sequential(
    (0): Linear(in_features=4, out_features=128, bias=True)
  ...res=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=1, bias=True)
  )
)
phase_dim = 4

    def test_evolution(self, hamiltonian_system: HamiltonianSystem, phase_dim: int):
        """Test Hamiltonian evolution."""
        # Create test phase space point
        state = torch.randn(phase_dim // 2)
        momentum = torch.randn(phase_dim // 2)
        phase_point = PhaseSpacePoint(position=state, momentum=momentum, time=0.0)
    
        # Test evolution
>       evolved = hamiltonian_system.evolve(torch.cat([state, momentum], dim=-1))

tests/test_neural/test_flow/test_hamiltonian.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/neural/flow/hamiltonian.py:492: in evolve
    dH = torch.autograd.grad(H.sum(), points, create_graph=True)[0]
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:496: in grad
    result = _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(-0.0751, grad_fn=<SumBackward0>),)
args = ((tensor(1.),), True, True, (tensor([ 1.4385,  1.3820, -1.1354, -1.2543]),), False)
kwargs = {'accumulate_grad': False}, attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: One of the differentiated Tensors does not require grad

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
_____________ TestHamiltonianSystem.test_canonical_transformations _____________

self = <test_hamiltonian.TestHamiltonianSystem object at 0x705517411370>
hamiltonian_system = HamiltonianSystem(
  (hamiltonian_network): Sequential(
    (0): Linear(in_features=4, out_features=128, bias=True)
  ...res=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=1, bias=True)
  )
)
phase_dim = 4

    def test_canonical_transformations(self, hamiltonian_system: HamiltonianSystem, phase_dim: int):
        """Test canonical transformation properties."""
        # Create test canonical transformation
        transform = CanonicalTransform(phase_dim=phase_dim)
    
        # Test point
        q = torch.randn(phase_dim // 2)
        p = torch.randn(phase_dim // 2)
        point = PhaseSpacePoint(position=q, momentum=p, time=0.0)
    
        # Apply transformation
>       transformed = transform.transform(point)

tests/test_neural/test_flow/test_hamiltonian.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/neural/flow/hamiltonian.py:95: in transform
    return self._f1_transform(point)
src/neural/flow/hamiltonian.py:110: in _f1_transform
    p_new = torch.autograd.grad(
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:496: in grad
    result = _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(0.8336),)
args = ((None,), True, True, (tensor([-0.1860, -0.8939]),), False)
kwargs = {'accumulate_grad': False}, attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
______________ TestHamiltonianSystem.test_symplectic_integration _______________

self = <test_hamiltonian.TestHamiltonianSystem object at 0x705517410ce0>
hamiltonian_system = HamiltonianSystem(
  (hamiltonian_network): Sequential(
    (0): Linear(in_features=4, out_features=128, bias=True)
  ...res=128, out_features=128, bias=True)
    (3): Tanh()
    (4): Linear(in_features=128, out_features=1, bias=True)
  )
)
symplectic_structure = <src.core.patterns.symplectic.SymplecticStructure object at 0x7055161b6420>
phase_dim = 4

    def test_symplectic_integration(
        self,
        hamiltonian_system: HamiltonianSystem,
        symplectic_structure: SymplecticStructure,
        phase_dim: int
    ):
        """Test symplectic integration properties."""
        # Create test point
        state = torch.randn(phase_dim // 2)
        momentum = torch.randn(phase_dim // 2)
        point = PhaseSpacePoint(position=state, momentum=momentum, time=0.0)
        full_state = torch.cat([state, momentum], dim=-1)
    
        # Evolve system
>       evolved = hamiltonian_system.evolve(full_state)

tests/test_neural/test_flow/test_hamiltonian.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/neural/flow/hamiltonian.py:492: in evolve
    dH = torch.autograd.grad(H.sum(), points, create_graph=True)[0]
venv/lib/python3.12/site-packages/torch/autograd/__init__.py:496: in grad
    result = _engine_run_backward(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

t_outputs = (tensor(0.2289, grad_fn=<SumBackward0>),)
args = ((tensor(1.),), True, True, (tensor([-0.7808, -0.7776,  0.6764, -1.6577]),), False)
kwargs = {'accumulate_grad': False}, attach_logging_hooks = False

    def _engine_run_backward(
        t_outputs: Sequence[Union[torch.Tensor, GradientEdge]],
        *args: Any,
        **kwargs: Any,
    ) -> Tuple[torch.Tensor, ...]:
        attach_logging_hooks = log.getEffectiveLevel() <= logging.DEBUG
        if attach_logging_hooks:
            unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
        try:
>           return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
                t_outputs, *args, **kwargs
            )  # Calls into the C++ engine to run the backward pass
E           RuntimeError: One of the differentiated Tensors does not require grad

venv/lib/python3.12/site-packages/torch/autograd/graph.py:823: RuntimeError
___________________ TestTensorShapes.test_validation_shapes ____________________

self = <test_tensor_shapes.TestTensorShapes object at 0x705517424740>
flow = GeometricFlow(
  (ricci): RicciTensorNetwork(
    (metric_network): Sequential(
      (0): Linear(in_features=2, out_f...
      (2): Linear(in_features=32, out_features=1, bias=True)
      (3): Softplus(beta=1.0, threshold=20.0)
    )
  )
)
phase_points = tensor([[ 0.3300, -0.8554, -0.6950,  0.4140],
        [ 0.3604,  0.5483,  0.7099, -0.7779],
        [-1.2449, -0.5505,...0547],
        [-0.4949,  0.0151,  0.5626, -1.5877],
        [-0.5732, -1.4010,  0.2458, -0.5132]], requires_grad=True)

    def test_validation_shapes(self, flow, phase_points):
        """Test shapes in validation computations."""
        batch_size = phase_points.shape[0]
        manifold_dim = flow.manifold_dim  # This is the position space dimension (2)
    
        # Test energy validation (uses full phase space)
        validator = FlowValidator(
            flow=flow,
            stability_threshold=1e-6,
            curvature_bounds=(-1.0, 1.0),
            max_energy=1e3
        )
        result = validator.validate_flow(phase_points)
    
        assert isinstance(result, FlowValidationResult), \
            "Validation result should be a FlowValidationResult"
        assert result.data is not None, \
            "Validation result should have data"
>       assert 'energy' in result.data, \
            "Validation result should have energy metrics"
E       AssertionError: Validation result should have energy metrics
E       assert 'energy' in {'error': 'Metric must be set before accessing'}
E        +  where {'error': 'Metric must be set before accessing'} = TilingFlowValidationResult(is_valid=False, message='Error validating geometric flow: Metric must be set before accessing', data={'error': 'Metric must be set before accessing'}).data

tests/test_neural/test_flow/test_tensor_shapes.py:147: AssertionError
___________________ TestTensorShapes.test_convergence_shapes ___________________

self = <test_tensor_shapes.TestTensorShapes object at 0x705517424950>
flow = GeometricFlow(
  (ricci): RicciTensorNetwork(
    (metric_network): Sequential(
      (0): Linear(in_features=2, out_f...
      (2): Linear(in_features=32, out_features=1, bias=True)
      (3): Softplus(beta=1.0, threshold=20.0)
    )
  )
)
phase_points = tensor([[-1.7220, -0.1956, -0.4464,  1.7883],
        [-0.2576,  2.2470,  0.2121, -1.0560],
        [ 0.7390, -0.0579,...0249],
        [-0.8250, -0.5763, -0.7441,  1.8909],
        [-0.4533,  0.8100, -1.4677, -0.5836]], requires_grad=True)

    def test_convergence_shapes(self, flow, phase_points):
        """Test shapes in convergence computations."""
        batch_size = phase_points.shape[0]
        manifold_dim = flow.manifold_dim  # This is the position space dimension (2)
    
        # Extract position components for convergence validation
        position = phase_points[..., :manifold_dim].clone()  # Clone to avoid in-place modifications
        position.requires_grad_(True)
    
        # Test convergence validation (uses position components only)
        validator = FlowValidator(
            flow=flow,
            stability_threshold=1e-6,
            curvature_bounds=(-1.0, 1.0),
            max_energy=1e3
        )
        result = validator.validate_flow(position)
    
        assert isinstance(result, FlowValidationResult), \
            "Validation result should be a FlowValidationResult"
        assert result.data is not None, \
            "Validation result should have data"
>       assert 'stability' in result.data, \
            "Validation result should have convergence metrics"
E       AssertionError: Validation result should have convergence metrics
E       assert 'stability' in {'error': 'Metric must be set before accessing'}
E        +  where {'error': 'Metric must be set before accessing'} = TilingFlowValidationResult(is_valid=False, message='Error validating geometric flow: Metric must be set before accessing', data={'error': 'Metric must be set before accessing'}).data

tests/test_neural/test_flow/test_tensor_shapes.py:172: AssertionError
____________________________ test_tensor_assertions ____________________________

    def test_tensor_assertions():
        """Test tensor assertion utilities."""
        tensor = torch.randn(3, 4)
    
        # Test shape assertion
        TensorAssertions.assert_shape(tensor, (3, 4))
        with pytest.raises(AssertionError):
            TensorAssertions.assert_shape(tensor, (4, 3))
    
        # Test dtype assertion
        TensorAssertions.assert_dtype(tensor, torch.float32)
        with pytest.raises(AssertionError):
            TensorAssertions.assert_dtype(tensor, torch.int32)
    
        # Test device assertion
        TensorAssertions.assert_device(tensor, torch.device("cpu"))
    
        # Test positive definite assertion
        pd_tensor = torch.randn(3, 3)
        pd_tensor = pd_tensor @ pd_tensor.t()
        TensorAssertions.assert_positive_definite(pd_tensor)
    
        # Test unitary assertion
        q, _ = torch.linalg.qr(torch.randn(4, 4))
>       TensorAssertions.assert_unitary(q)

tests/test_utils/test_helpers.py:301: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = tensor([[-0.3857, -0.9142,  0.0627,  0.1072],
        [ 0.6211, -0.1592,  0.4895,  0.5910],
        [-0.3270,  0.1981, -0.4766,  0.7917],
        [-0.5988,  0.3156,  0.7275,  0.1116]])
rtol = 1e-05

    @staticmethod
    def assert_unitary(tensor: torch.Tensor, rtol: float = 1e-5) -> None:
        """Assert tensor is unitary."""
        identity = torch.eye(tensor.shape[-1], device=tensor.device)
        product = tensor @ tensor.conj().transpose(-2, -1)
>       assert torch.allclose(
            product, identity, rtol=rtol
        ), f"Tensor is not unitary, max deviation: {torch.max(torch.abs(product - identity))}"
E       AssertionError: Tensor is not unitary, max deviation: 2.384185791015625e-07
E       assert False
E        +  where False = <built-in method allclose of type object at 0x70560ced8b20>(tensor([[ 1.0000e+00, -6.8150e-08,  1.2174e-08,  1.5477e-08],\n        [-6.8150e-08,  1.0000e+00, -2.3845e-08,  1.1646e-08],\n        [ 1.2174e-08, -2.3845e-08,  1.0000e+00,  2.6568e-08],\n        [ 1.5477e-08,  1.1646e-08,  2.6568e-08,  1.0000e+00]]), tensor([[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.]]), rtol=1e-05)
E        +    where <built-in method allclose of type object at 0x70560ced8b20> = torch.allclose

tests/test_utils/test_helpers.py:70: AssertionError
___________________________ test_numerical_stability ___________________________

    def test_numerical_stability():
        """Test numerical stability utilities."""
        # Test gradient stability
        model = torch.nn.Linear(10, 1)
        data = torch.randn(5, 10)
        loss_fn = torch.nn.MSELoss()
    
>       is_stable = NumericalStability.check_gradient_stability(model, loss_fn, data)

tests/test_utils/test_helpers.py:316: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_utils/test_helpers.py:97: in check_gradient_stability
    loss = loss_fn(model(data))
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = MSELoss()
args = (tensor([[ 0.6187],
        [ 0.2659],
        [-0.9617],
        [-0.8860],
        [ 0.4129]], grad_fn=<AddmmBackward0>),)
kwargs = {}

    def _call_impl(self, *args, **kwargs):
        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
        # If we don't have any hooks, we want to skip the rest of the logic in
        # this function, and just call forward.
        if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
                or _global_backward_pre_hooks or _global_backward_hooks
                or _global_forward_hooks or _global_forward_pre_hooks):
>           return forward_call(*args, **kwargs)
E           TypeError: MSELoss.forward() missing 1 required positional argument: 'target'

venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750: TypeError
__________________________ test_performance_benchmark __________________________

    def test_performance_benchmark():
        """Test performance benchmarking utilities."""
    
        def dummy_function(x: torch.Tensor) -> torch.Tensor:
            return torch.matmul(x, x.t())
    
        # Test timer
        with PerformanceBenchmark.timer("test_operation") as t:
            torch.randn(100, 100)
        assert isinstance(t.elapsed, float)
    
        # Test benchmark function
        data = torch.randn(100, 100)
        result = PerformanceBenchmark.benchmark_function(dummy_function, data, n_runs=10)
        assert isinstance(result, BenchmarkResult)
        assert result.iterations == 10
    
        # Test memory profiling
>       memory_stats = PerformanceBenchmark.profile_memory(dummy_function, data)

tests/test_utils/test_helpers.py:363: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_utils/test_helpers.py:205: in profile_memory
    torch.cuda.reset_peak_memory_stats()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

device = None

    def reset_peak_memory_stats(device: Union[Device, int] = None) -> None:
        r"""Reset the "peak" stats tracked by the CUDA memory allocator.
    
        See :func:`~torch.cuda.memory_stats` for details. Peak stats correspond to the
        `"peak"` key in each individual stat dict.
    
        Args:
            device (torch.device or int, optional): selected device. Returns
                statistic for the current device, given by :func:`~torch.cuda.current_device`,
                if :attr:`device` is ``None`` (default).
    
        .. note::
            See :ref:`cuda-memory-management` for more details about GPU memory
            management.
        """
        device = _get_device_index(device, optional=True)
>       return torch._C._cuda_resetPeakMemoryStats(device)
E       AttributeError: module 'torch._C' has no attribute '_cuda_resetPeakMemoryStats'

venv/lib/python3.12/site-packages/torch/cuda/memory.py:370: AttributeError
____ TestStabilityValidation.test_validate_stability_return_type[is_valid] _____

self = <test_debug_flow.TestStabilityValidation object at 0x705517427530>
setup_stability = {'dynamics': <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc29cb90>, 'pattern': tensor([[[[-...   requires_grad=True), 'validator': <src.validation.flow.stability.LinearStabilityValidator object at 0x7054fc29f680>}
attr = 'is_valid'

    @pytest.mark.parametrize("attr", ['is_valid', 'message', 'data'])
    def test_validate_stability_return_type(self, setup_stability, attr):
        """Test that validate_stability returns correct type."""
        validator = setup_stability['validator']
        pattern = setup_stability['pattern']
        dynamics = setup_stability['dynamics']
    
>       result = validator.validate_stability(dynamics, pattern)

tests/test_validation/test_debug_flow.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:678: in jacobian
    is_outputs_tuple, outputs = _as_tuple(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = ({'routing_scores': tensor([[[0.0604, 0.0625, 0.0684, 0.0653, 0.0629, 0.0581, 0.0617, 0.0646,
          0.0612, 0.0627...609,
          0.0662, 0.0584, 0.0624, 0.0620, 0.0583, 0.0618, 0.0615, 0.0619]]],
       grad_fn=<SoftmaxBackward0>)},)
arg_name = 'outputs of the user-provided function', fn_name = 'jacobian'

    def _as_tuple(inp, arg_name=None, fn_name=None):
        # Ensures that inp is a tuple of Tensors
        # Returns whether or not the original inp was a tuple and the tupled version of the input
        if arg_name is None and fn_name is None:
            return _as_tuple_nocheck(inp)
    
        is_inp_tuple = True
        if not isinstance(inp, tuple):
            inp = (inp,)
            is_inp_tuple = False
    
        for i, el in enumerate(inp):
            if not isinstance(el, torch.Tensor):
                if is_inp_tuple:
                    raise TypeError(
                        f"The {arg_name} given to {fn_name} must be either a Tensor or a tuple of Tensors but the"
                        f" value at index {i} has type {type(el)}."
                    )
                else:
>                   raise TypeError(
                        f"The {arg_name} given to {fn_name} must be either a Tensor or a tuple of Tensors but the"
                        f" given {arg_name} has type {type(el)}."
                    )
E                   TypeError: The outputs of the user-provided function given to jacobian must be either a Tensor or a tuple of Tensors but the given outputs of the user-provided function has type <class 'dict'>.

venv/lib/python3.12/site-packages/torch/autograd/functional.py:43: TypeError
_____ TestStabilityValidation.test_validate_stability_return_type[message] _____

self = <test_debug_flow.TestStabilityValidation object at 0x705517427680>
setup_stability = {'dynamics': <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x705517424980>, 'pattern': tensor([[[[-...   requires_grad=True), 'validator': <src.validation.flow.stability.LinearStabilityValidator object at 0x705517424140>}
attr = 'message'

    @pytest.mark.parametrize("attr", ['is_valid', 'message', 'data'])
    def test_validate_stability_return_type(self, setup_stability, attr):
        """Test that validate_stability returns correct type."""
        validator = setup_stability['validator']
        pattern = setup_stability['pattern']
        dynamics = setup_stability['dynamics']
    
>       result = validator.validate_stability(dynamics, pattern)

tests/test_validation/test_debug_flow.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:678: in jacobian
    is_outputs_tuple, outputs = _as_tuple(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = ({'routing_scores': tensor([[[0.0589, 0.0715, 0.0682, 0.0634, 0.0568, 0.0629, 0.0605, 0.0605,
          0.0607, 0.0642...702,
          0.0558, 0.0599, 0.0635, 0.0577, 0.0612, 0.0622, 0.0601, 0.0643]]],
       grad_fn=<SoftmaxBackward0>)},)
arg_name = 'outputs of the user-provided function', fn_name = 'jacobian'

    def _as_tuple(inp, arg_name=None, fn_name=None):
        # Ensures that inp is a tuple of Tensors
        # Returns whether or not the original inp was a tuple and the tupled version of the input
        if arg_name is None and fn_name is None:
            return _as_tuple_nocheck(inp)
    
        is_inp_tuple = True
        if not isinstance(inp, tuple):
            inp = (inp,)
            is_inp_tuple = False
    
        for i, el in enumerate(inp):
            if not isinstance(el, torch.Tensor):
                if is_inp_tuple:
                    raise TypeError(
                        f"The {arg_name} given to {fn_name} must be either a Tensor or a tuple of Tensors but the"
                        f" value at index {i} has type {type(el)}."
                    )
                else:
>                   raise TypeError(
                        f"The {arg_name} given to {fn_name} must be either a Tensor or a tuple of Tensors but the"
                        f" given {arg_name} has type {type(el)}."
                    )
E                   TypeError: The outputs of the user-provided function given to jacobian must be either a Tensor or a tuple of Tensors but the given outputs of the user-provided function has type <class 'dict'>.

venv/lib/python3.12/site-packages/torch/autograd/functional.py:43: TypeError
______ TestStabilityValidation.test_validate_stability_return_type[data] _______

self = <test_debug_flow.TestStabilityValidation object at 0x705517427740>
setup_stability = {'dynamics': <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc219bb0>, 'pattern': tensor([[[[-...   requires_grad=True), 'validator': <src.validation.flow.stability.LinearStabilityValidator object at 0x7054fc218e00>}
attr = 'data'

    @pytest.mark.parametrize("attr", ['is_valid', 'message', 'data'])
    def test_validate_stability_return_type(self, setup_stability, attr):
        """Test that validate_stability returns correct type."""
        validator = setup_stability['validator']
        pattern = setup_stability['pattern']
        dynamics = setup_stability['dynamics']
    
>       result = validator.validate_stability(dynamics, pattern)

tests/test_validation/test_debug_flow.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:678: in jacobian
    is_outputs_tuple, outputs = _as_tuple(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = ({'routing_scores': tensor([[[0.0640, 0.0661, 0.0653, 0.0638, 0.0649, 0.0677, 0.0612, 0.0590,
          0.0595, 0.0603...615,
          0.0604, 0.0604, 0.0633, 0.0619, 0.0648, 0.0670, 0.0626, 0.0611]]],
       grad_fn=<SoftmaxBackward0>)},)
arg_name = 'outputs of the user-provided function', fn_name = 'jacobian'

    def _as_tuple(inp, arg_name=None, fn_name=None):
        # Ensures that inp is a tuple of Tensors
        # Returns whether or not the original inp was a tuple and the tupled version of the input
        if arg_name is None and fn_name is None:
            return _as_tuple_nocheck(inp)
    
        is_inp_tuple = True
        if not isinstance(inp, tuple):
            inp = (inp,)
            is_inp_tuple = False
    
        for i, el in enumerate(inp):
            if not isinstance(el, torch.Tensor):
                if is_inp_tuple:
                    raise TypeError(
                        f"The {arg_name} given to {fn_name} must be either a Tensor or a tuple of Tensors but the"
                        f" value at index {i} has type {type(el)}."
                    )
                else:
>                   raise TypeError(
                        f"The {arg_name} given to {fn_name} must be either a Tensor or a tuple of Tensors but the"
                        f" given {arg_name} has type {type(el)}."
                    )
E                   TypeError: The outputs of the user-provided function given to jacobian must be either a Tensor or a tuple of Tensors but the given outputs of the user-provided function has type <class 'dict'>.

venv/lib/python3.12/site-packages/torch/autograd/functional.py:43: TypeError
_________ TestStabilityValidation.test_stability_spectrum_computation __________

self = <test_debug_flow.TestStabilityValidation object at 0x705517427320>
setup_stability = {'dynamics': <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7055161b5ca0>, 'pattern': tensor([[[[ ...   requires_grad=True), 'validator': <src.validation.flow.stability.LinearStabilityValidator object at 0x7055161b71d0>}

    def test_stability_spectrum_computation(self, setup_stability):
        """Test stability spectrum computation."""
        validator = setup_stability['validator']
        pattern = setup_stability['pattern']
        dynamics = setup_stability['dynamics']
    
>       result = validator.validate_stability(dynamics, pattern)

tests/test_validation/test_debug_flow.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:678: in jacobian
    is_outputs_tuple, outputs = _as_tuple(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = ({'routing_scores': tensor([[[0.0692, 0.0621, 0.0540, 0.0551, 0.0613, 0.0611, 0.0653, 0.0650,
          0.0613, 0.0598...674,
          0.0677, 0.0692, 0.0669, 0.0658, 0.0538, 0.0643, 0.0604, 0.0530]]],
       grad_fn=<SoftmaxBackward0>)},)
arg_name = 'outputs of the user-provided function', fn_name = 'jacobian'

    def _as_tuple(inp, arg_name=None, fn_name=None):
        # Ensures that inp is a tuple of Tensors
        # Returns whether or not the original inp was a tuple and the tupled version of the input
        if arg_name is None and fn_name is None:
            return _as_tuple_nocheck(inp)
    
        is_inp_tuple = True
        if not isinstance(inp, tuple):
            inp = (inp,)
            is_inp_tuple = False
    
        for i, el in enumerate(inp):
            if not isinstance(el, torch.Tensor):
                if is_inp_tuple:
                    raise TypeError(
                        f"The {arg_name} given to {fn_name} must be either a Tensor or a tuple of Tensors but the"
                        f" value at index {i} has type {type(el)}."
                    )
                else:
>                   raise TypeError(
                        f"The {arg_name} given to {fn_name} must be either a Tensor or a tuple of Tensors but the"
                        f" given {arg_name} has type {type(el)}."
                    )
E                   TypeError: The outputs of the user-provided function given to jacobian must be either a Tensor or a tuple of Tensors but the given outputs of the user-provided function has type <class 'dict'>.

venv/lib/python3.12/site-packages/torch/autograd/functional.py:43: TypeError
_____________ TestEnergyValidation.test_energy_computation[check0] _____________

self = <test_debug_flow.TestEnergyValidation object at 0x705517425cd0>
setup_energy = {'flow': tensor([[[[[ 4.2587e-02, -9.8705e-02, -1.2867e-01,  ..., -1.7153e-01,
             3.9129e-02,  2.3467e-02],
...3276e-02, -6.4017e-02]]]]]), 'validator': <src.validation.geometric.flow.TilingFlowValidator object at 0x7054fc29f230>}
check = (<built-in method isnan of type object at 0x70560ced8b20>, 'NaN')

    @pytest.mark.parametrize("check", [
        (torch.isnan, "NaN"),
        (torch.isinf, "Inf"),
        (lambda x: x < 0, "negative values")
    ])
    def test_energy_computation(self, setup_energy, check):
        """Test energy computation is numerically stable."""
        validator = setup_energy['validator']
        flow = setup_energy['flow']
        check_fn, desc = check
    
>       energy = validator.compute_energy(flow)
E       AttributeError: 'TilingFlowValidator' object has no attribute 'compute_energy'

tests/test_validation/test_debug_flow.py:110: AttributeError
_____________ TestEnergyValidation.test_energy_computation[check1] _____________

self = <test_debug_flow.TestEnergyValidation object at 0x7055174279b0>
setup_energy = {'flow': tensor([[[[[-4.4687e-02, -3.5697e-02, -1.6325e-02,  ...,  1.4988e-01,
             2.0920e-01,  3.2494e-02],
...0584e-02, -5.0090e-02]]]]]), 'validator': <src.validation.geometric.flow.TilingFlowValidator object at 0x7054fc2182f0>}
check = (<built-in method isinf of type object at 0x70560ced8b20>, 'Inf')

    @pytest.mark.parametrize("check", [
        (torch.isnan, "NaN"),
        (torch.isinf, "Inf"),
        (lambda x: x < 0, "negative values")
    ])
    def test_energy_computation(self, setup_energy, check):
        """Test energy computation is numerically stable."""
        validator = setup_energy['validator']
        flow = setup_energy['flow']
        check_fn, desc = check
    
>       energy = validator.compute_energy(flow)
E       AttributeError: 'TilingFlowValidator' object has no attribute 'compute_energy'

tests/test_validation/test_debug_flow.py:110: AttributeError
_____________ TestEnergyValidation.test_energy_computation[check2] _____________

self = <test_debug_flow.TestEnergyValidation object at 0x705517427a70>
setup_energy = {'flow': tensor([[[[[ 5.4023e-02,  1.0153e-01,  8.8551e-02,  ...,  1.4747e-01,
            -8.6643e-02,  2.2124e-02],
...4271e-02, -9.8088e-02]]]]]), 'validator': <src.validation.geometric.flow.TilingFlowValidator object at 0x7054fc3a4620>}
check = (<function TestEnergyValidation.<lambda> at 0x705517440b80>, 'negative values')

    @pytest.mark.parametrize("check", [
        (torch.isnan, "NaN"),
        (torch.isinf, "Inf"),
        (lambda x: x < 0, "negative values")
    ])
    def test_energy_computation(self, setup_energy, check):
        """Test energy computation is numerically stable."""
        validator = setup_energy['validator']
        flow = setup_energy['flow']
        check_fn, desc = check
    
>       energy = validator.compute_energy(flow)
E       AttributeError: 'TilingFlowValidator' object has no attribute 'compute_energy'

tests/test_validation/test_debug_flow.py:110: AttributeError
___________ TestEnergyValidation.test_energy_conservation_validation ___________

self = <test_debug_flow.TestEnergyValidation object at 0x705517427470>
setup_energy = {'flow': tensor([[[[[ 0.0710, -0.1343,  0.0077,  ...,  0.0459, -0.0894, -0.0568],
           [ 0.1113, -0.0119,  0.026...758,  0.0449,  0.1160]]]]]), 'validator': <src.validation.geometric.flow.TilingFlowValidator object at 0x7054fc218080>}

    def test_energy_conservation_validation(self, setup_energy):
        """Test energy conservation validation result."""
        validator = setup_energy['validator']
        flow = setup_energy['flow']
    
        result = validator.validate_energy_conservation(flow)
    
        assert isinstance(result, FlowValidationResult), "Should return FlowValidationResult"
        assert result.data is not None, "Should have data"
>       assert 'energy' in result.data, "Should have energy metrics"
E       AssertionError: Should have energy metrics
E       assert 'energy' in {'error': 'Boolean value of Tensor with more than one value is ambiguous'}
E        +  where {'error': 'Boolean value of Tensor with more than one value is ambiguous'} = TilingFlowValidationResult(is_valid=False, message='Error validating energy conservation: Boolean value of Tensor with more than one value is ambiguous', data={'error': 'Boolean value of Tensor with more than one value is ambiguous'}).data

tests/test_validation/test_debug_flow.py:122: AssertionError
_____________ TestFlowProperties.test_flow_properties_construction _____________

self = <test_debug_flow.TestFlowProperties object at 0x705517427cb0>
setup_flow = {'flow': tensor([[[[[ 1.2059e-01,  2.9837e-02, -5.2708e-02,  ...,  1.2244e-01,
            -1.5087e-01,  1.2344e-01],
...3113e-01,  7.1285e-02]]]]]), 'validator': <src.validation.geometric.flow.TilingFlowValidator object at 0x7054fc21ab70>}

    def test_flow_properties_construction(self, setup_flow):
        """Test flow properties object construction."""
        validator = setup_flow['validator']
        flow = setup_flow['flow']
    
        result = validator.validate_flow(flow)
    
        assert isinstance(result, FlowValidationResult), "Should return FlowValidationResult"
        assert result.data is not None, "Should have data"
>       assert 'stability' in result.data, "Should have stability metrics"
E       AssertionError: Should have stability metrics
E       assert 'stability' in {'error': 'The size of tensor a (16) must match the size of tensor b (2) at non-singleton dimension 2'}
E        +  where {'error': 'The size of tensor a (16) must match the size of tensor b (2) at non-singleton dimension 2'} = TilingFlowValidationResult(is_valid=False, message='Error validating geometric flow: The size of tensor a (16) must match the size of tensor b (2) at non-singleton dimension 2', data={'error': 'The size of tensor a (16) must match the size of tensor b (2) at non-singleton dimension 2'}).data

tests/test_validation/test_debug_flow.py:159: AssertionError
_____________ TestFlowProperties.test_flow_properties_with_energy ______________

self = <test_debug_flow.TestFlowProperties object at 0x705517427da0>
setup_flow = {'flow': tensor([[[[[ 1.7822e-01, -8.1022e-02, -9.0994e-03,  ..., -6.3647e-02,
             1.4768e-01,  1.1463e-01],
...9666e-02,  2.1637e-01]]]]]), 'validator': <src.validation.geometric.flow.TilingFlowValidator object at 0x7054fc277020>}

    def test_flow_properties_with_energy(self, setup_flow):
        """Test flow properties with explicit energy metrics."""
        validator = setup_flow['validator']
        flow = setup_flow['flow']
    
        result = validator.validate_flow(flow)
    
        assert result.data is not None, "Should have data"
>       assert 'energy' in result.data, "Should have energy metrics"
E       AssertionError: Should have energy metrics
E       assert 'energy' in {'error': 'The size of tensor a (16) must match the size of tensor b (2) at non-singleton dimension 2'}
E        +  where {'error': 'The size of tensor a (16) must match the size of tensor b (2) at non-singleton dimension 2'} = TilingFlowValidationResult(is_valid=False, message='Error validating geometric flow: The size of tensor a (16) must match the size of tensor b (2) at non-singleton dimension 2', data={'error': 'The size of tensor a (16) must match the size of tensor b (2) at non-singleton dimension 2'}).data

tests/test_validation/test_debug_flow.py:170: AssertionError
_________________ TestFlowValidation.test_energy_conservation __________________

self = <test_flow_validation.TestFlowValidation object at 0x7055174505c0>

    def test_energy_conservation(self):
        """Test energy conservation validation."""
        def generate_flow(t):
            # Create a flow that conserves energy (exponential decay)
            t = t.reshape(-1, 1)  # Add batch dimension
            flow = torch.exp(-0.1 * t) * torch.ones((t.shape[0], self.dim))
            return flow.unsqueeze(0).repeat(self.batch_size, 1, 1)  # Add batch dimension
    
        # Generate test flow
        flow = generate_flow(self.t)
    
        # Validate energy conservation
        result = self.validator.validate_flow(flow)
    
        # Check results
        assert isinstance(result, FlowValidationResult)
        assert result.data is not None
>       assert 'energy' in result.data
E       AssertionError: assert 'energy' in {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}
E        +  where {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'} = TilingFlowValidationResult(is_valid=False, message='Error validating geometric flow: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2', data={'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}).data

tests/test_validation/test_flow_validation.py:57: AssertionError
__________________ TestFlowValidation.test_flow_monotonicity ___________________

self = <test_flow_validation.TestFlowValidation object at 0x7055174506e0>

    def test_flow_monotonicity(self):
        """Test flow monotonicity validation."""
        def generate_monotonic_flow(t):
            # Create a monotonic flow (exponential decay)
            t = t.reshape(-1, 1)  # Add batch dimension
            flow = torch.exp(-0.1 * t) * torch.ones((t.shape[0], self.dim))
            return flow.unsqueeze(0).repeat(self.batch_size, 1, 1)  # Add batch dimension
    
        # Generate test flow
        flow = generate_monotonic_flow(self.t)
    
        # Validate monotonicity
        result = self.validator.validate_flow(flow)
    
        # Check results
        assert isinstance(result, FlowValidationResult)
        assert result.data is not None
>       assert 'stability' in result.data
E       AssertionError: assert 'stability' in {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}
E        +  where {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'} = TilingFlowValidationResult(is_valid=False, message='Error validating geometric flow: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2', data={'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}).data

tests/test_validation/test_flow_validation.py:77: AssertionError
_________________ TestFlowValidation.test_long_time_existence __________________

self = <test_flow_validation.TestFlowValidation object at 0x705517450890>

    def test_long_time_existence(self):
        """Test long-time existence validation."""
        def generate_stable_flow(t):
            # Create a stable flow (tanh)
            t = t.reshape(-1, 1)  # Add batch dimension
            flow = torch.tanh(0.1 * t) * torch.ones((t.shape[0], self.dim))
            return flow.unsqueeze(0).repeat(self.batch_size, 1, 1)  # Add batch dimension
    
        # Generate test flow with longer time horizon
        t = torch.linspace(0, 100, 100)
        flow = generate_stable_flow(t)
    
        # Validate long-time existence
        result = self.validator.validate_flow(flow)
    
        # Check results
        assert isinstance(result, FlowValidationResult)
        assert result.data is not None
>       assert 'stability' in result.data
E       AssertionError: assert 'stability' in {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}
E        +  where {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'} = TilingFlowValidationResult(is_valid=False, message='Error validating geometric flow: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2', data={'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}).data

tests/test_validation/test_flow_validation.py:98: AssertionError
________________ TestFlowValidation.test_singularity_detection _________________

self = <test_flow_validation.TestFlowValidation object at 0x705517450a40>

    def test_singularity_detection(self):
        """Test singularity detection."""
        def generate_singular_flow(t):
            # Create a flow with singularity at t=0
            t = t.reshape(-1, 1)  # Add batch dimension
            x = t * torch.ones((t.shape[0], self.dim))
            flow = 1 / (x + 1e-6)  # Add small epsilon to avoid division by zero
            return flow.unsqueeze(0).repeat(self.batch_size, 1, 1)  # Add batch dimension
    
        # Generate test flow near singularity
        t = torch.linspace(-5, 5, 1000)
        flow = generate_singular_flow(t)
    
        # Detect singularities
        result = self.validator.validate_flow(flow)
    
        # Check results
        assert isinstance(result, FlowValidationResult)
        assert result.data is not None
>       assert 'metric' in result.data
E       AssertionError: assert 'metric' in {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}
E        +  where {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'} = TilingFlowValidationResult(is_valid=False, message='Error validating geometric flow: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2', data={'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}).data

tests/test_validation/test_flow_validation.py:120: AssertionError
________________ TestFlowValidation.test_validation_integration ________________

self = <test_flow_validation.TestFlowValidation object at 0x705517450bf0>

    def test_validation_integration(self):
        """Test integration of all validation methods."""
        # Generate random flow
        t = self.t.reshape(-1, 1)  # Add batch dimension
        flow = torch.exp(-0.1 * t) * torch.randn((t.shape[0], self.dim))
        flow = flow.unsqueeze(0).repeat(self.batch_size, 1, 1)  # Add batch dimension
    
        # Run all validations
        result = self.validator.validate_flow(flow)
    
        # Check results
        assert isinstance(result, FlowValidationResult)
        assert result.data is not None
>       assert 'metric' in result.data
E       AssertionError: assert 'metric' in {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}
E        +  where {'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'} = TilingFlowValidationResult(is_valid=False, message='Error validating geometric flow: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2', data={'error': 'The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2'}).data

tests/test_validation/test_flow_validation.py:136: AssertionError
______________ TestValidationFramework.test_geometric_validation _______________

self = <test_framework.TestValidationFramework object at 0x705517452960>
validation_framework = <src.validation.framework.ValidationFramework object at 0x70551608f890>
riemannian_framework = PatternRiemannianStructure(), batch_size = 16
manifold_dim = 16

    @pytest.mark.level0
    def test_geometric_validation(
        self,
        validation_framework: ValidationFramework,
        riemannian_framework: RiemannianFramework,
        batch_size: int,
        manifold_dim: int
    ) -> None:
        """Test geometric validation."""
        # Create test data
        points = torch.randn(batch_size, manifold_dim)
        metric = riemannian_framework.compute_metric(points)
    
        # Run validation
        result = validation_framework.validate_geometry(
            model=None,
            data=points,
            riemannian=riemannian_framework
        )
    
        # Check result structure
>       assert result.is_valid
E       AssertionError: assert False
E        +  where False = GeometricValidationResult(is_valid=False, message='Layer default not found in model', data={'layer_name': 'default'}).is_valid

tests/test_validation/test_framework.py:169: AssertionError
_______________ TestValidationFramework.test_quantum_validation ________________

self = <test_framework.TestValidationFramework object at 0x705517452c60>
validation_framework = <src.validation.framework.ValidationFramework object at 0x7054f55b43e0>
riemannian_framework = PatternRiemannianStructure(), batch_size = 16
manifold_dim = 16

    @pytest.mark.level0
    def test_quantum_validation(
        self,
        validation_framework: ValidationFramework,
        riemannian_framework: RiemannianFramework,
        batch_size: int,
        manifold_dim: int
    ) -> None:
        """Test quantum validation."""
        # Create test quantum state
        state = torch.randn(batch_size, manifold_dim, dtype=torch.complex64)
        state = state / torch.norm(state, dim=1, keepdim=True)
    
        # Create test measurements and bases
        measurements: List[Union[Tensor, QuantumState]] = [
            torch.randn(batch_size, manifold_dim, dtype=torch.complex64) / torch.sqrt(torch.tensor(manifold_dim))
            for _ in range(3)  # Create 3 test measurements
        ]
        bases = [f"|{i}⟩" for i in range(manifold_dim)]
    
        # Run validation
>       result = validation_framework.validate_quantum_state(
            state=state,
            measurements=measurements
        )

tests/test_validation/test_framework.py:206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/framework.py:635: in validate_quantum_state
    return self.quantum_validator.validate(
src/validation/quantum/state.py:843: in validate
    preparation = self.preparation_validator.validate_preparation(target, prepared)
src/validation/quantum/state.py:241: in validate_preparation
    fidelity = self._compute_fidelity(target, prepared)
src/validation/quantum/state.py:289: in _compute_fidelity
    if target.is_pure(self.tolerance) and prepared.is_pure(self.tolerance):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumState(amplitudes=tensor([[ 0.2176-0.1498j,  0.0526+0.2060j,  0.0955+0.2778j, -0.3758-0.2412j,
         -0.0565-....j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,
         0.+0.j, 0.+0.j]]))
tolerance = 1e-06

    def is_pure(self, tolerance: float = 1e-6) -> bool:
        """Check if the state is pure by computing the purity of its density matrix.
    
        A pure state has Tr(ρ²) = 1, while mixed states have Tr(ρ²) < 1.
    
        Args:
            tolerance: Numerical tolerance for comparison with 1.0
    
        Returns:
            bool: True if the state is pure, False otherwise
        """
        rho = self.density_matrix()
>       purity = torch.trace(torch.matmul(rho, rho)).real
E       RuntimeError: trace: expected a matrix, but got tensor with dim 3

src/core/quantum/types.py:148: RuntimeError
_______________ TestValidationFramework.test_pattern_validation ________________

self = <test_framework.TestValidationFramework object at 0x705517452f60>
validation_framework = <src.validation.framework.ValidationFramework object at 0x7054f55b57c0>
riemannian_framework = PatternRiemannianStructure(), batch_size = 16
manifold_dim = 16

    @pytest.mark.level0
    def test_pattern_validation(
        self,
        validation_framework: ValidationFramework,
        riemannian_framework: RiemannianFramework,
        batch_size: int,
        manifold_dim: int
    ):
        """Test pattern validation."""
        # Create test pattern
        pattern = torch.randn(batch_size, manifold_dim)
    
        # Create test dynamics
        dynamics = AttentionPatternDynamics(
            grid_size=manifold_dim,
            space_dim=2,
            hidden_dim=manifold_dim
        )
    
        # Run validation
>       result = validation_framework.validate_pattern_formation(
            pattern=pattern,
            dynamics=dynamics,
            time_steps=1000
        )

tests/test_validation/test_framework.py:250: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/framework.py:657: in validate_pattern_formation
    return pattern_validator.validate(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.patterns.formation.PatternFormationValidator object at 0x7054f55b5850>
dynamics = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054f55b5880>
initial = tensor([[ 0.6209,  0.6828,  0.4281,  1.0433,  0.1963, -0.3901,  0.5815,  0.2407,
         -0.3718, -0.6815,  0.7623,  ... -1.1607,  0.4075, -0.6748,  0.5607,
          1.7721,  0.8136, -0.1496, -0.0253, -1.0141,  1.0627, -1.0624,  0.8805]])
time_steps = 1000

    def validate(
        self,
        dynamics: Optional[PatternDynamics],
        initial: torch.Tensor,
        time_steps: int = 1000,
    ) -> ValidationResult:
        """Perform complete pattern formation validation.
    
        Args:
            dynamics: Pattern dynamics system (optional)
            initial: Initial pattern state
            time_steps: Number of time steps to simulate
    
        Returns:
            ValidationResult with is_valid=True if pattern formation is valid
        """
        # Initialize trajectory with initial state
        current = initial
        trajectory = [current]
    
        # Evolve system if dynamics provided
        if dynamics is not None:
            for _ in range(time_steps - 1):
                current = dynamics.step(current)
                trajectory.append(current)
        else:
            # If no dynamics, use initial state for validation
            trajectory = [initial] * time_steps
    
        # Convert to tensor
>       trajectory = torch.stack(trajectory)
E       RuntimeError: stack expects each tensor to be equal size, but got [16, 16] at entry 0 and [1, 1, 16, 16] at entry 1

src/validation/patterns/formation.py:1398: RuntimeError
______________ TestValidationFramework.test_integrated_validation ______________

self = <test_framework.TestValidationFramework object at 0x705517453260>
validation_framework = <src.validation.framework.ValidationFramework object at 0x7054fc39c920>
riemannian_framework = PatternRiemannianStructure(), batch_size = 16
manifold_dim = 16

    @pytest.mark.level1
    def test_integrated_validation(
        self,
        validation_framework: ValidationFramework,
        riemannian_framework: RiemannianFramework,
        batch_size: int,
        manifold_dim: int
    ):
        """Test integrated validation workflow."""
        # Create test data
        points = torch.randn(batch_size, manifold_dim)
        metric_tensor = riemannian_framework.compute_metric(points)
        metric = metric_tensor.values  # Extract raw tensor values
    
        # Run validation
>       result = validation_framework.validate_all(
            model=None,
            data=points,
            metric=metric,
            riemannian=riemannian_framework
        )

tests/test_validation/test_framework.py:288: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/framework.py:484: in validate_all
    quantum_result = self.validate_quantum_state(data)
src/validation/framework.py:635: in validate_quantum_state
    return self.quantum_validator.validate(
src/validation/quantum/state.py:843: in validate
    preparation = self.preparation_validator.validate_preparation(target, prepared)
src/validation/quantum/state.py:241: in validate_preparation
    fidelity = self._compute_fidelity(target, prepared)
src/validation/quantum/state.py:289: in _compute_fidelity
    if target.is_pure(self.tolerance) and prepared.is_pure(self.tolerance):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumState(amplitudes=tensor([[-0.3495+0.j,  0.2592+0.j, -0.3898+0.j,  0.5231+0.j,  0.4643+0.j, -0.0634+0.j,
       ....j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,
         0.+0.j, 0.+0.j]]))
tolerance = 1e-06

    def is_pure(self, tolerance: float = 1e-6) -> bool:
        """Check if the state is pure by computing the purity of its density matrix.
    
        A pure state has Tr(ρ²) = 1, while mixed states have Tr(ρ²) < 1.
    
        Args:
            tolerance: Numerical tolerance for comparison with 1.0
    
        Returns:
            bool: True if the state is pure, False otherwise
        """
        rho = self.density_matrix()
>       purity = torch.trace(torch.matmul(rho, rho)).real
E       RuntimeError: trace: expected a matrix, but got tensor with dim 3

src/core/quantum/types.py:148: RuntimeError
_________________ TestValidationFramework.test_error_handling __________________

self = <test_framework.TestValidationFramework object at 0x705517453560>
validation_framework = <src.validation.framework.ValidationFramework object at 0x7054fc29e0f0>
geometric_validator = <src.validation.geometric.model.ModelGeometricValidator object at 0x7054fc277440>

    def test_error_handling(
        self,
        validation_framework: ValidationFramework,
        geometric_validator: ModelGeometricValidator
    ):
        """Test error handling in validation framework."""
        # Test invalid metric shape
>       with pytest.raises(ValueError, match="Invalid metric shape"):
E       Failed: DID NOT RAISE <class 'ValueError'>

tests/test_validation/test_framework.py:316: Failed
_______________ TestValidationFramework.test_validation_metrics ________________

self = <test_framework.TestValidationFramework object at 0x705517453800>

    def test_validation_metrics(self):
        """Test validation metrics computation and aggregation."""
        metrics = ConcreteValidationResult(
            is_valid=True,
            message="Test validation metrics",
            data={
                "geometric": {"curvature": True, "energy": True},
                "quantum": {
                    "metrics": {
                        "normalization": True,
                        "unitarity": True,
                        "energy_conservation": True
                    },
                    "entanglement": {
                        "entanglement_entropy": 0.5,
                        "mutual_information": 0.3,
                        "relative_entropy": 0.2
                    },
                    "coherence": {
                        "coherence_length": 0.8,
                        "coherence_time": 0.7,
                        "decoherence_rate": 0.1
                    }
                },
                "pattern": {
                    "spatial_coherence": True,
                    "temporal_stability": True,
                    "translation_invariance": True,
                    "rotation_invariance": True,
                    "linear_stability": True,
                    "nonlinear_stability": True,
                    "bifurcation_points": [torch.tensor([1.0])],
                    "stability_eigenvalues": torch.tensor([0.1]),
                    "symmetry": True
                }
            },
            curvature_bounds=(-1.0, 1.0),
            energy_metrics={"total": 0.5},
            bifurcation_points=[torch.tensor([1.0])],
            stability_eigenvalues=torch.tensor([0.1]),
            framework_accuracy=0.95,
            framework_consistency=0.90,
            metrics={
                "geometric": 0.95,
                "quantum": 0.90,
                "pattern": 0.85
            }
        )
    
        # Test metric structure
        assert metrics.framework_accuracy == 0.95
        assert metrics.framework_consistency == 0.90
        assert len(metrics.component_scores) == 3
        assert all(0 <= score <= 1 for score in metrics.component_scores.values())
    
        # Test quantum metrics
        quantum = metrics.metrics["quantum"]
>       assert all(k in quantum["metrics"] for k in ["normalization", "unitarity", "energy_conservation"])

tests/test_validation/test_framework.py:407: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x70551758efb0>

>   assert all(k in quantum["metrics"] for k in ["normalization", "unitarity", "energy_conservation"])
E   TypeError: 'float' object is not subscriptable

tests/test_validation/test_framework.py:407: TypeError
________________ TestValidationFramework.test_full_integration _________________

self = <test_framework.TestValidationFramework object at 0x705517453980>
validation_framework = <src.validation.framework.ValidationFramework object at 0x7054f55b69f0>
batch_size = 16, dim = 8

    def test_full_integration(
        self, validation_framework: ValidationFramework, batch_size: int, dim: int
    ):
        """Test full integration of all validation components."""
        # Create test data
        metric = torch.randn(batch_size, dim, dim)
        metric = metric @ metric.transpose(-1, -2)  # Make symmetric positive definite
        state = torch.randn(batch_size, dim, dtype=torch.complex64)
        state = state / torch.norm(state, dim=1, keepdim=True)
    
        # Run validation
>       result = validation_framework.validate_all(
            model=None,
            data=state,
            metric=metric
        )

tests/test_validation/test_framework.py:430: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.framework.ValidationFramework object at 0x7054f55b69f0>
model = None
data = tensor([[-0.5383-0.0423j,  0.0309-0.0255j, -0.2019+0.1146j, -0.2518+0.2214j,
         -0.2507-0.3337j, -0.1562+0.3166j...2802j, -0.2746+0.1596j, -0.0485+0.1789j,
          0.0789+0.1092j, -0.0162-0.3318j, -0.1319+0.0817j, -0.3107-0.2316j]])
metric = tensor([[[ 2.9866e+00,  2.0196e-01, -1.9480e-01,  ...,  2.8109e-01,
          -2.9659e-01, -1.3156e+00],
         [ 2....2.4546e+00],
         [ 8.0657e-01, -1.7549e+00, -4.9612e-01,  ...,  2.1723e+00,
           2.4546e+00,  1.0590e+01]]])
riemannian = PatternRiemannianStructure()

    def validate_all(
        self,
        model: Optional[nn.Module],
        data: torch.Tensor,
        metric: Optional[torch.Tensor] = None,
        riemannian: Optional[RiemannianFramework] = None,
    ) -> FrameworkValidationResult:
        """Run complete validation on model and data."""
        # Initialize messages list
        messages = []
    
        # Run geometric validation
        geometric_result = None
        if metric is not None or riemannian is not None:
            if riemannian is None and metric is not None:
                # Create basic Riemannian framework from metric
                manifold_dim = metric.shape[-1]
                riemannian = PatternRiemannianStructure(
                    manifold_dim=manifold_dim,
                    pattern_dim=manifold_dim  # Using manifold_dim as pattern_dim for basic case
                )
                # Initialize metric factors from input metric
                with torch.no_grad():
>                   riemannian.metric_factors.copy_(metric.reshape(manifold_dim, -1))
E                   RuntimeError: The size of tensor a (8) must match the size of tensor b (128) at non-singleton dimension 1

src/validation/framework.py:479: RuntimeError
__________________ TestValidationFramework.test_validate_all ___________________

self = <test_framework.TestValidationFramework object at 0x705517453c80>
validation_framework = <src.validation.framework.ValidationFramework object at 0x7054f55b6720>
riemannian_framework = PatternRiemannianStructure(), batch_size = 16
manifold_dim = 16

    def test_validate_all(
        self,
        validation_framework: ValidationFramework,
        riemannian_framework: RiemannianFramework,
        batch_size: int,
        manifold_dim: int
    ):
        """Test full validation pipeline."""
        # Create test data
        points = torch.randn(batch_size, manifold_dim)
        metric_tensor = riemannian_framework.compute_metric(points)
        metric = metric_tensor.values  # Extract raw tensor values
    
        # Run validation
>       result = validation_framework.validate_all(
            model=None,
            data=points,
            metric=metric,
            riemannian=riemannian_framework
        )

tests/test_validation/test_framework.py:466: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/framework.py:484: in validate_all
    quantum_result = self.validate_quantum_state(data)
src/validation/framework.py:635: in validate_quantum_state
    return self.quantum_validator.validate(
src/validation/quantum/state.py:843: in validate
    preparation = self.preparation_validator.validate_preparation(target, prepared)
src/validation/quantum/state.py:241: in validate_preparation
    fidelity = self._compute_fidelity(target, prepared)
src/validation/quantum/state.py:289: in _compute_fidelity
    if target.is_pure(self.tolerance) and prepared.is_pure(self.tolerance):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumState(amplitudes=tensor([[ 0.1433+0.j, -0.2135+0.j, -0.1688+0.j,  0.1140+0.j, -0.3074+0.j, -0.2454+0.j,
       ....j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,
         0.+0.j, 0.+0.j]]))
tolerance = 1e-06

    def is_pure(self, tolerance: float = 1e-6) -> bool:
        """Check if the state is pure by computing the purity of its density matrix.
    
        A pure state has Tr(ρ²) = 1, while mixed states have Tr(ρ²) < 1.
    
        Args:
            tolerance: Numerical tolerance for comparison with 1.0
    
        Returns:
            bool: True if the state is pure, False otherwise
        """
        rho = self.density_matrix()
>       purity = torch.trace(torch.matmul(rho, rho)).real
E       RuntimeError: trace: expected a matrix, but got tensor with dim 3

src/core/quantum/types.py:148: RuntimeError
_________________ TestMetricValidation.test_fisher_rao_metric __________________

self = <test_metric_validation.TestMetricValidation object at 0x70551746e600>
validator = <src.validation.geometric.metric.MetricValidator object at 0x7054fc29c7d0>
batch_size = 16, dim = 8

    def test_fisher_rao_metric(self, validator: MetricValidator, batch_size: int, dim: int):
        """Test Fisher-Rao metric validation."""
        # Generate test points
        points = torch.randn(batch_size, dim)
    
        # Compute Fisher-Rao metric
        score = validator.compute_score_function(points)
        fisher_metric = torch.einsum('bi,bj->bij', score, score)
    
        # Test validation
>       assert validator.validate_fisher_rao(fisher_metric)
E       assert False
E        +  where False = validate_fisher_rao(tensor([[[ 1.0860e-01,  9.0966e-01,  3.9966e-01,  ...,  5.1454e-01,\n          -2.6577e-01, -1.2882e-01],\n         [ 9.0966e-01,  7.6196e+00,  3.3477e+00,  ...,  4.3100e+00,\n          -2.2262e+00, -1.0790e+00],\n         [ 3.9966e-01,  3.3477e+00,  1.4708e+00,  ...,  1.8936e+00,\n          -9.7808e-01, -4.7406e-01],\n         ...,\n         [ 5.1454e-01,  4.3100e+00,  1.8936e+00,  ...,  2.4379e+00,\n          -1.2592e+00, -6.1033e-01],\n         [-2.6577e-01, -2.2262e+00, -9.7808e-01,  ..., -1.2592e+00,\n           6.5041e-01,  3.1525e-01],\n         [-1.2882e-01, -1.0790e+00, -4.7406e-01,  ..., -6.1033e-01,\n           3.1525e-01,  1.5280e-01]],\n\n        [[ 5.7140e-04, -5.1296e-03,  1.9977e-02,  ...,  3.6268e-02,\n          -1.3020e-02,  1.5050e-02],\n         [-5.1296e-03,  4.6049e-02, -1.7934e-01,  ..., -3.2558e-01,\n           1.1688e-01, -1.3511e-01],\n         [ 1.9977e-02, -1.7934e-01,  6.9842e-01,  ...,  1.2680e+00,\n          -4.5519e-01,  5.2618e-01],\n         ...,\n         [ 3.6268e-02, -3.2558e-01,  1.2680e+00,  ...,  2.3020e+00,\n          -8.2639e-01,  9.5528e-01],\n         [-1.3020e-02,  1.1688e-01, -4.5519e-01,  ..., -8.2639e-01,\n           2.9666e-01, -3.4293e-01],\n         [ 1....1.9073e-01,  4.5346e-02,  2.7056e-01,  ..., -6.2694e-02,\n          -2.4385e-01, -1.7836e-01],\n         [ 1.1380e+00,  2.7056e-01,  1.6143e+00,  ..., -3.7407e-01,\n          -1.4549e+00, -1.0642e+00],\n         ...,\n         [-2.6369e-01, -6.2694e-02, -3.7407e-01,  ...,  8.6678e-02,\n           3.3714e-01,  2.4660e-01],\n         [-1.0256e+00, -2.4385e-01, -1.4549e+00,  ...,  3.3714e-01,\n           1.3113e+00,  9.5914e-01],\n         [-7.5020e-01, -1.7836e-01, -1.0642e+00,  ...,  2.4660e-01,\n           9.5914e-01,  7.0156e-01]],\n\n        [[ 7.0173e-03,  5.8723e-02, -5.4045e-03,  ...,  2.7288e-02,\n          -1.4432e-02, -5.1780e-02],\n         [ 5.8723e-02,  4.9141e-01, -4.5226e-02,  ...,  2.2835e-01,\n          -1.2077e-01, -4.3331e-01],\n         [-5.4045e-03, -4.5226e-02,  4.1623e-03,  ..., -2.1016e-02,\n           1.1115e-02,  3.9879e-02],\n         ...,\n         [ 2.7288e-02,  2.2835e-01, -2.1016e-02,  ...,  1.0611e-01,\n          -5.6119e-02, -2.0136e-01],\n         [-1.4432e-02, -1.2077e-01,  1.1115e-02,  ..., -5.6119e-02,\n           2.9679e-02,  1.0649e-01],\n         [-5.1780e-02, -4.3331e-01,  3.9879e-02,  ..., -2.0136e-01,\n           1.0649e-01,  3.8208e-01]]], grad_fn=<MulBackward0>))
E        +    where validate_fisher_rao = <src.validation.geometric.metric.MetricValidator object at 0x7054fc29c7d0>.validate_fisher_rao

tests/test_validation/test_metric_validation.py:103: AssertionError
________________ TestMetricValidation.test_curvature_validation ________________

self = <test_metric_validation.TestMetricValidation object at 0x70551746e7e0>
validator = <src.validation.geometric.metric.MetricValidator object at 0x7054fc39cb00>
batch_size = 16, dim = 8

    def test_curvature_validation(self, validator: MetricValidator, batch_size: int, dim: int):
        """Test curvature validation."""
        # Generate test metric
        matrix = torch.randn(batch_size, dim, dim)
        metric = matrix @ matrix.transpose(-1, -2)
        metric = metric / torch.norm(metric, dim=(-2, -1), keepdim=True)
    
        # Test curvature bounds
>       bounds = validator.validate_curvature_bounds(metric)

tests/test_validation/test_metric_validation.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/geometric/metric.py:940: in validate_curvature_bounds
    sectional = self.compute_sectional_curvature(metric)
src/validation/geometric/metric.py:826: in compute_sectional_curvature
    christoffel = self.compute_christoffel_symbols(metric)
src/validation/geometric/metric.py:800: in compute_christoffel_symbols
    grad = self.compute_metric_gradient(metric)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.geometric.metric.MetricValidator object at 0x7054fc39cb00>
metric = tensor([[[ 0.4309,  0.0233, -0.0140,  ...,  0.0116,  0.1501,  0.1007],
         [ 0.0233,  0.1375, -0.0932,  ..., -0.0...  0.2362,  0.1999],
         [-0.1361,  0.0737, -0.0524,  ..., -0.0631,  0.1999,  0.5046]]],
       requires_grad=True)

    def compute_metric_gradient(self, metric: Tensor) -> Tensor:
        """Compute metric gradient tensor.
    
        Args:
            metric: Metric tensor
    
        Returns:
            Metric gradient tensor
        """
        batch_size = metric.shape[0]
    
        # Compute gradient
        grad = torch.zeros(batch_size, self.manifold_dim, self.manifold_dim, self.manifold_dim)
        for i in range(self.manifold_dim):
            for j in range(self.manifold_dim):
                grad_ij = torch.autograd.grad(
                    metric[:, i, j].sum(),
                    metric,
                    create_graph=True
                )[0]
>               grad[:, i, j] = grad_ij
E               RuntimeError: expand(torch.FloatTensor{[16, 8, 8]}, size=[16, 8]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)

src/validation/geometric/metric.py:414: RuntimeError
_________________ TestMetricValidation.test_metric_properties __________________

self = <test_metric_validation.TestMetricValidation object at 0x70551746eba0>
validator = <src.validation.geometric.metric.MetricValidator object at 0x70551608ebd0>
batch_size = 16, dim = 8

    def test_metric_properties(self, validator: MetricValidator, batch_size: int, dim: int):
        """Test metric property computations."""
        # Generate test metric
        matrix = torch.randn(batch_size, dim, dim)
        metric = matrix @ matrix.transpose(-1, -2)
        metric = metric + torch.eye(dim)  # Ensure positive definiteness
    
        # Test property validation
>       properties = validator.validate_metric_properties(metric)

tests/test_validation/test_metric_validation.py:175: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/geometric/metric.py:903: in validate_metric_properties
    christoffel = self._compute_christoffel_symbols(metric)
src/validation/geometric/metric.py:282: in _compute_christoffel_symbols
    metric = self.compute_metric_values(points)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.geometric.metric.MetricValidator object at 0x70551608ebd0>
points = tensor([[[ 1.3389e+01, -8.4272e-01, -5.0781e+00,  ..., -2.9650e+00,
          -1.8336e+00,  2.9726e+00],
         [-8....3.7150e+00],
         [-3.0688e+00,  4.7578e+00, -4.8175e+00,  ...,  4.6848e+00,
           3.7150e+00,  1.0726e+01]]])

    def compute_metric_values(self, points: Tensor) -> Tensor:
        """Compute metric tensor values at points using Fisher-Rao structure.
    
        Args:
            points: Points tensor (batch_size x dim)
    
        Returns:
            Metric tensor values (batch_size x dim x dim)
        """
        batch_size = points.shape[0]
    
        # Compute Fisher-Rao metric components
        # g_ij = E[∂_i log p(x|θ) ∂_j log p(x|θ)]
        score_fn = self._compute_score_function(points)
    
        # Ensure score_fn has correct shape [batch_size, dim]
        if len(score_fn.shape) == 3:
            score_fn = score_fn.squeeze(1)
        elif len(score_fn.shape) == 1:
            score_fn = score_fn.unsqueeze(0)
    
        # Reshape if needed
        if len(score_fn.shape) != 2:
            score_fn = score_fn.view(batch_size, -1)
    
        metric = torch.einsum('bi,bj->bij', score_fn, score_fn)
    
        # Add regularization for numerical stability
>       metric = metric + self.eigenvalue_threshold * torch.eye(
            self.manifold_dim,
            device=points.device,
            dtype=points.dtype
        ).expand(batch_size, -1, -1)
E       RuntimeError: The size of tensor a (64) must match the size of tensor b (8) at non-singleton dimension 2

src/validation/geometric/metric.py:173: RuntimeError
___________ TestModelGeometricValidator.test_validate_layer_geometry ___________

self = <test_model_geometric.TestModelGeometricValidator object at 0x70551746ee70>
validator = <src.validation.geometric.model.ModelGeometricValidator object at 0x7054f55b7740>
batch_size = 16

    def test_validate_layer_geometry(
        self, validator: ModelGeometricValidator, batch_size: int
    ) -> None:
        """Test layer geometry validation."""
        # Generate random points
        points = torch.randn(batch_size, validator.model_geometry.manifold_dim)
    
        # Validate input layer
        result = validator.validate_layer_geometry('input', points)
>       assert result.is_valid, f"Layer validation failed: {result.message}"
E       AssertionError: Layer validation failed: Error validating layer input: 'MockModelGeometry' object has no attribute 'sectional_curvature'
E       assert False
E        +  where False = GeometricValidationResult(is_valid=False, message="Error validating layer input: 'MockModelGeometry' object has no attribute 'sectional_curvature'", data={'layer_name': 'input', 'error': "'MockModelGeometry' object has no attribute 'sectional_curvature'"}).is_valid

tests/test_validation/test_model_geometric.py:151: AssertionError
_________ TestModelGeometricValidator.test_validate_attention_geometry _________

self = <test_model_geometric.TestModelGeometricValidator object at 0x70551746e930>
validator = <src.validation.geometric.model.ModelGeometricValidator object at 0x7054f5509c10>
batch_size = 16

    def test_validate_attention_geometry(
        self, validator: ModelGeometricValidator, batch_size: int
    ) -> None:
        """Test attention geometry validation."""
        # Generate query and key points
        query_points = torch.randn(batch_size, validator.model_geometry.query_dim)
        key_points = torch.randn(batch_size, validator.model_geometry.key_dim)
    
        # Validate attention geometry
        result = validator.validate_attention_geometry(0, query_points, key_points)
        assert result.data is not None, "Validation data is None"
    
        # Print debug info
        print("\nDistance Statistics:")
        print("Query distances:")
>       assert 'query_metric' in result.data, "Query metric not found in validation data"
E       AssertionError: Query metric not found in validation data
E       assert 'query_metric' in {'error': "'MockAttentionHead' object has no attribute 'query_metric'", 'head_idx': 0}
E        +  where {'error': "'MockAttentionHead' object has no attribute 'query_metric'", 'head_idx': 0} = GeometricValidationResult(is_valid=False, message="Error validating attention head 0: 'MockAttentionHead' object has no attribute 'query_metric'", data={'head_idx': 0, 'error': "'MockAttentionHead' object has no attribute 'query_metric'"}).data

tests/test_validation/test_model_geometric.py:180: AssertionError
___________ TestModelGeometricValidator.test_geometric_preservation ____________

self = <test_model_geometric.TestModelGeometricValidator object at 0x70551746e300>
validator = <src.validation.geometric.model.ModelGeometricValidator object at 0x7054f5508fb0>
batch_size = 16

    def test_geometric_preservation(
        self, validator: ModelGeometricValidator, batch_size: int
    ) -> None:
        """Test geometric preservation check."""
        # Generate random points and perturbation
        points = torch.randn(batch_size, validator.model_geometry.manifold_dim)
        perturbation = torch.randn_like(points) * 0.1
    
        # Get base metric
        layer = validator.model_geometry.layers['hidden']
        base_metric = cast(Tensor, layer.metric_tensor.data)
    
        # Print metric properties
        print("\nBase Metric Properties:")
        print(f"Shape: {base_metric.shape}")
        print(f"Symmetric: {torch.allclose(cast(Tensor, base_metric), cast(Tensor, base_metric.transpose(-2, -1)))}")
        eigenvals = torch.linalg.eigvalsh(base_metric)
        print(f"Eigenvalue range: [{eigenvals.min():.6f}, {eigenvals.max():.6f}]\n")
    
        # Compute attention scores
        head = cast(AttentionHead, validator.model_geometry.attention_heads[0])
        scores = head.compute_attention(points, points + perturbation)
    
        # Print score properties
        print("Attention Scores Properties:")
        print(f"Shape: {scores.shape}")
        print(f"Range: [{scores.min():.6f}, {scores.max():.6f}]")
        print(f"Row sums: {scores.sum(dim=-1)}\n")
    
        # Check geometric preservation
        preserves = validator._check_geometric_preservation(points, base_metric, scores)
>       assert preserves, "Attention does not preserve geometric structure"
E       AssertionError: Attention does not preserve geometric structure
E       assert False

tests/test_validation/test_model_geometric.py:259: AssertionError
_________________________ test_pattern_flow_stability __________________________

setup_test_parameters = {'batch_size': 1, 'dt': 0.1, 'energy_threshold': 1e-06, 'grid_size': 16, ...}
pattern_validator = <src.validation.patterns.stability.PatternValidator object at 0x7054f5508320>
flow_validator = <src.validation.geometric.flow.TilingFlowValidator object at 0x7054f5509130>

    def test_pattern_flow_stability(setup_test_parameters, pattern_validator, flow_validator):
        """Test pattern stability under flow."""
        params = setup_test_parameters
    
        # Create pattern
        pattern = torch.randn(
            params['batch_size'],
            params['space_dim'],
            params['grid_size'],
            params['grid_size']
        ) * 0.1
    
        # Create pattern flow
        pattern_flow = PatternFlow(
            input_dim=pattern.shape[1],
            hidden_dim=128,
            manifold_dim=32
        )
    
        # Validate stability
>       stability_result = pattern_validator.validate(
            pattern_flow=pattern_flow,
            initial_state=pattern,
            time_steps=params['time_steps']
        )

tests/test_validation/test_pattern_flow.py:500: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/patterns/stability.py:148: in validate
    linear_result = self.linear_validator.validate_stability(pattern_flow, initial_state)
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:677: in jacobian
    outputs = func(*inputs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = PatternFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features=1...s=64, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1024, bias=True)
  )
)
x = tensor([[[[-2.8167e-02, -2.3648e-04,  9.6877e-02, -6.6180e-03, -1.6517e-01,
            1.6186e-01,  8.2864e-02, -2.72...        6.7530e-02,  2.8216e-02, -9.2439e-02,  6.2047e-02,  6.1513e-03,
            2.6426e-01]]]], requires_grad=True)
return_paths = False

    def forward(
        self, x: torch.Tensor, return_paths: bool = False
    ) -> Tuple[torch.Tensor, List[Dict]]:
        """
        Compute geometric flow and track paths.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_paths: Whether to return flow paths
    
        Returns:
            - Output tensor of shape (batch_size, seq_len, hidden_dim)
            - List of metrics dictionaries
        """
>       batch_size, seq_len, _ = x.shape
E       ValueError: too many values to unpack (expected 3)

src/core/tiling/geometric_flow.py:595: ValueError
___________________________ test_pattern_flow_energy ___________________________

setup_test_parameters = {'batch_size': 1, 'dt': 0.1, 'energy_threshold': 1e-06, 'grid_size': 16, ...}
flow_validator = <src.validation.geometric.flow.TilingFlowValidator object at 0x70551746e630>

    def test_pattern_flow_energy(setup_test_parameters, flow_validator):
        """Test energy conservation in pattern flow."""
        params = setup_test_parameters
    
        # Create pattern
        pattern = torch.randn(
            params['batch_size'],
            params['space_dim'],
            params['grid_size'],
            params['grid_size']
        ) * 0.1
    
        # Create and evolve pattern flow
        pattern_flow = PatternFlow(
            input_dim=pattern.shape[1],
            hidden_dim=128,
            manifold_dim=32
        )
>       output, metrics = pattern_flow(pattern, return_paths=True)

tests/test_validation/test_pattern_flow.py:556: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = PatternFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features=1...s=64, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1024, bias=True)
  )
)
x = tensor([[[[-3.1526e-02, -2.6413e-03,  8.6289e-02,  1.5131e-02, -6.5170e-02,
           -2.8070e-02, -7.3157e-02,  3.38...1,  1.2386e-01,
           -1.1229e-02, -1.2656e-01,  1.1123e-01, -1.4996e-02, -4.0750e-02,
           -8.7761e-03]]]])
return_paths = True

    def forward(
        self, x: torch.Tensor, return_paths: bool = False
    ) -> Tuple[torch.Tensor, List[Dict]]:
        """
        Compute geometric flow and track paths.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_paths: Whether to return flow paths
    
        Returns:
            - Output tensor of shape (batch_size, seq_len, hidden_dim)
            - List of metrics dictionaries
        """
>       batch_size, seq_len, _ = x.shape
E       ValueError: too many values to unpack (expected 3)

src/core/tiling/geometric_flow.py:595: ValueError
_________________ TestPatternFormation.test_pattern_emergence __________________

self = <test_pattern_formation.TestPatternFormation object at 0x7055174cd3d0>
validator = <src.validation.patterns.formation.PatternFormationValidator object at 0x7054fc3a54f0>
batch_size = 16, spatial_dim = 32

    def test_pattern_emergence(
        self, validator: PatternFormationValidator, batch_size: int, spatial_dim: int
    ):
        """Test pattern emergence validation."""
        # Create test pattern
        pattern = torch.zeros(batch_size, spatial_dim)
        for i in range(batch_size):
            x = torch.linspace(0, 2 * np.pi, spatial_dim)
            pattern[i] = torch.sin(x) + 0.1 * torch.randn_like(x)
    
        # Test emergence validation
>       result = validator.emergence_validator.validate_emergence(pattern)

tests/test_validation/test_pattern_formation.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/patterns/formation.py:667: in validate_emergence
    coherence = self._compute_coherence(final_state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.patterns.formation.EmergenceValidator object at 0x70551749a690>
state = tensor([-0.0494,  0.2335,  0.2554,  0.6462,  0.7424,  0.8449,  0.9521,  0.9698,
         1.0623,  0.9655,  0.8948,  0....2, -0.9233, -0.9949, -0.9179, -0.9827,
        -0.9393, -0.8350, -0.9193, -0.7316, -0.7929, -0.4108, -0.0198,  0.0750])

    def _compute_coherence(self, state: torch.Tensor) -> float:
        """Compute coherence of pattern.
    
        Args:
            state: Pattern state tensor
    
        Returns:
            float: Coherence score
        """
        # Ensure we have a 2D tensor by taking the first batch and channel if present
        if state.ndim > 2:
            state = state.squeeze()  # Remove singleton dimensions
            if state.ndim > 2:
                state = state[0]  # Take first batch if still > 2D
                if state.ndim > 2:
                    state = state[0]  # Take first channel if still > 2D
    
        padding = 2
>       height, width = state.shape
E       ValueError: not enough values to unpack (expected 2, got 1)

src/validation/patterns/formation.py:701: ValueError
________________ TestPatternFormation.test_spatial_organization ________________

self = <test_pattern_formation.TestPatternFormation object at 0x7055174cd520>
validator = <src.validation.patterns.formation.PatternFormationValidator object at 0x7054f55092b0>
batch_size = 16, spatial_dim = 32

    def test_spatial_organization(
        self, validator: PatternFormationValidator, batch_size: int, spatial_dim: int
    ):
        """Test spatial organization validation."""
        # Create test pattern
        pattern = torch.zeros(batch_size, spatial_dim)
        for i in range(batch_size):
            x = torch.linspace(0, 2 * np.pi, spatial_dim)
            pattern[i] = torch.sin(x) + 0.1 * torch.randn_like(x)
    
        # Test spatial validation
>       result = validator.spatial_validator.validate_spatial(pattern)

tests/test_validation/test_pattern_formation.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/patterns/formation.py:757: in validate_spatial
    wavelength = self._compute_wavelength(pattern)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.patterns.formation.SpatialValidator object at 0x7054f55096d0>
pattern = tensor([[-0.1505,  0.1698,  0.4282,  0.5789,  0.6228,  0.8616,  1.0163,  0.7889,
          1.0874,  0.9757,  0.8999,  ... -0.9019, -1.1602, -0.9981, -0.7899,
         -1.0694, -0.8264, -0.7734, -0.7087, -0.6610, -0.2203, -0.0847,  0.0583]])

    def _compute_wavelength(self, pattern: torch.Tensor) -> torch.Tensor:
        """Compute dominant wavelength from pattern.
    
        Args:
            pattern: Pattern tensor of shape (batch_size, channels, height, width)
    
        Returns:
            Wavelength tensor of shape (batch_size, channels)
        """
        # Get size and create frequency grid
        N = pattern.shape[-1]
        freqs = torch.fft.fftfreq(N, dtype=torch.float32, device=pattern.device)
    
        # Create 2D frequency grids for x and y separately
        freqs_x = freqs[None, :].expand(N, N)
        freqs_y = freqs[:, None].expand(N, N)
    
        # Compute power spectrum
        fft = torch.fft.fft2(pattern)
        power = torch.abs(fft).pow(2)
    
        # Create mask for valid frequencies (exclude DC and above Nyquist)
        nyquist = 0.5
        mask_x = (freqs_x.abs() > 0) & (freqs_x.abs() <= nyquist)  # Explicitly exclude DC
        mask_y = (freqs_y.abs() > 0) & (freqs_y.abs() <= nyquist)  # Explicitly exclude DC
        mask = mask_x | mask_y  # Use OR to capture peaks in either direction
    
        # Get valid frequencies and reshape power
        batch_shape = power.shape[:-2]  # (batch_size, channels)
>       power_valid = power.reshape(*batch_shape, -1)[..., mask.reshape(-1)]
E       IndexError: The shape of the mask [1024] at index 0 does not match the shape of the indexed tensor [512] at index 0

src/validation/patterns/formation.py:804: IndexError
_______________ TestPatternFormation.test_validation_integration _______________

self = <test_pattern_formation.TestPatternFormation object at 0x7055174cdc70>
validator = <src.validation.patterns.formation.PatternFormationValidator object at 0x7054f550bf50>
batch_size = 16, spatial_dim = 32

    def test_validation_integration(
        self, validator: PatternFormationValidator, batch_size: int, spatial_dim: int
    ):
        """Test integration of all validation components."""
        # Create test pattern sequence
        time_steps = 100
        trajectory = torch.zeros(time_steps, batch_size, spatial_dim)
        for t in range(time_steps):
            phase = 2 * np.pi * t / time_steps
            for i in range(batch_size):
                x = torch.linspace(0, 2 * np.pi, spatial_dim)
                trajectory[t, i] = torch.sin(x + phase) + 0.1 * torch.randn_like(x)
    
        # Test emergence validation
>       emergence = validator.emergence_validator.validate_emergence(trajectory[-1])

tests/test_validation/test_pattern_formation.py:178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/patterns/formation.py:667: in validate_emergence
    coherence = self._compute_coherence(final_state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.patterns.formation.EmergenceValidator object at 0x7054f5508560>
state = tensor([ 0.0052,  0.0294,  0.3973,  0.3711,  0.7759,  0.8124,  0.7361,  0.9127,
         0.7779,  1.0074,  0.9303,  0....1, -0.5921, -0.8132, -0.8448, -1.0862,
        -0.7832, -0.9203, -0.8207, -0.6775, -0.5745, -0.5398, -0.4137, -0.0776])

    def _compute_coherence(self, state: torch.Tensor) -> float:
        """Compute coherence of pattern.
    
        Args:
            state: Pattern state tensor
    
        Returns:
            float: Coherence score
        """
        # Ensure we have a 2D tensor by taking the first batch and channel if present
        if state.ndim > 2:
            state = state.squeeze()  # Remove singleton dimensions
            if state.ndim > 2:
                state = state[0]  # Take first batch if still > 2D
                if state.ndim > 2:
                    state = state[0]  # Take first channel if still > 2D
    
        padding = 2
>       height, width = state.shape
E       ValueError: not enough values to unpack (expected 2, got 1)

src/validation/patterns/formation.py:701: ValueError
_________________ TestPatternFormation.test_reaction_diffusion _________________

self = <test_pattern_formation.TestPatternFormation object at 0x7055174cde50>
validator = <src.validation.patterns.formation.PatternFormationValidator object at 0x7054f527e240>
batch_size = 16, spatial_dim = 32

    def test_reaction_diffusion(
        self, validator: PatternFormationValidator, batch_size: int, spatial_dim: int
    ):
        """Test reaction-diffusion pattern validation."""
        # Create test pattern sequence
        time_steps = 100
        trajectory = torch.zeros(time_steps, batch_size, spatial_dim)
        for t in range(time_steps):
            phase = 2 * np.pi * t / time_steps
            for i in range(batch_size):
                x = torch.linspace(0, 2 * np.pi, spatial_dim)
                # Simulate Turing pattern formation
                u = torch.sin(x + phase) + 0.1 * torch.randn_like(x)
                v = torch.cos(2 * x + phase) + 0.1 * torch.randn_like(x)
                trajectory[t, i] = u - v
    
        # Test emergence validation
>       emergence = validator.emergence_validator.validate_emergence(trajectory[-1])

tests/test_validation/test_pattern_formation.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/patterns/formation.py:667: in validate_emergence
    coherence = self._compute_coherence(final_state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.patterns.formation.EmergenceValidator object at 0x7054f527e270>
state = tensor([-1.1824, -0.9681, -0.2938,  0.0156,  0.7836,  1.1230,  1.7862,  1.8809,
         1.9221,  1.5921,  1.7492,  1....3, -0.5207, -0.4811,  0.1343, -0.0896,
         0.0326,  0.0157, -0.1583, -0.5866, -0.9486, -1.0994, -0.9860, -1.2768])

    def _compute_coherence(self, state: torch.Tensor) -> float:
        """Compute coherence of pattern.
    
        Args:
            state: Pattern state tensor
    
        Returns:
            float: Coherence score
        """
        # Ensure we have a 2D tensor by taking the first batch and channel if present
        if state.ndim > 2:
            state = state.squeeze()  # Remove singleton dimensions
            if state.ndim > 2:
                state = state[0]  # Take first batch if still > 2D
                if state.ndim > 2:
                    state = state[0]  # Take first channel if still > 2D
    
        padding = 2
>       height, width = state.shape
E       ValueError: not enough values to unpack (expected 2, got 1)

src/validation/patterns/formation.py:701: ValueError
_________________ TestPatternFormation.test_symmetry_breaking __________________

self = <test_pattern_formation.TestPatternFormation object at 0x7055174ce030>
validator = <src.validation.patterns.formation.PatternFormationValidator object at 0x7054f527f0e0>
batch_size = 16, spatial_dim = 32

    def test_symmetry_breaking(
        self, validator: PatternFormationValidator, batch_size: int, spatial_dim: int
    ):
        """Test symmetry breaking detection."""
        # Create test pattern
        pattern = torch.zeros(batch_size, spatial_dim)
        for i in range(batch_size):
            x = torch.linspace(0, 2 * np.pi, spatial_dim)
            pattern[i] = torch.sin(x) + 0.1 * torch.randn_like(x)
    
        # Test spatial validation
>       result = validator.spatial_validator.validate_spatial(pattern)

tests/test_validation/test_pattern_formation.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/patterns/formation.py:757: in validate_spatial
    wavelength = self._compute_wavelength(pattern)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.patterns.formation.SpatialValidator object at 0x7054f527f1d0>
pattern = tensor([[ 8.9575e-05,  2.0163e-01,  3.0660e-01,  5.9517e-01,  6.4815e-01,
          7.1193e-01,  9.5049e-01,  9.9125e-...940e-01,
         -1.1502e+00, -8.5704e-01, -5.4443e-01, -5.7714e-01, -3.8273e-01,
         -9.0915e-02, -1.6665e-02]])

    def _compute_wavelength(self, pattern: torch.Tensor) -> torch.Tensor:
        """Compute dominant wavelength from pattern.
    
        Args:
            pattern: Pattern tensor of shape (batch_size, channels, height, width)
    
        Returns:
            Wavelength tensor of shape (batch_size, channels)
        """
        # Get size and create frequency grid
        N = pattern.shape[-1]
        freqs = torch.fft.fftfreq(N, dtype=torch.float32, device=pattern.device)
    
        # Create 2D frequency grids for x and y separately
        freqs_x = freqs[None, :].expand(N, N)
        freqs_y = freqs[:, None].expand(N, N)
    
        # Compute power spectrum
        fft = torch.fft.fft2(pattern)
        power = torch.abs(fft).pow(2)
    
        # Create mask for valid frequencies (exclude DC and above Nyquist)
        nyquist = 0.5
        mask_x = (freqs_x.abs() > 0) & (freqs_x.abs() <= nyquist)  # Explicitly exclude DC
        mask_y = (freqs_y.abs() > 0) & (freqs_y.abs() <= nyquist)  # Explicitly exclude DC
        mask = mask_x | mask_y  # Use OR to capture peaks in either direction
    
        # Get valid frequencies and reshape power
        batch_shape = power.shape[:-2]  # (batch_size, channels)
>       power_valid = power.reshape(*batch_shape, -1)[..., mask.reshape(-1)]
E       IndexError: The shape of the mask [1024] at index 0 does not match the shape of the indexed tensor [512] at index 0

src/validation/patterns/formation.py:804: IndexError
_________________ TestPatternFormation.test_pattern_stability __________________

self = <test_pattern_formation.TestPatternFormation object at 0x7055174cd340>
validator = <src.validation.patterns.formation.PatternFormationValidator object at 0x7054f5214050>
batch_size = 16, spatial_dim = 32

    def test_pattern_stability(
        self, validator: PatternFormationValidator, batch_size: int, spatial_dim: int
    ):
        """Test pattern stability validation."""
        # Create test pattern sequence
        time_steps = 100
        trajectory = torch.zeros(time_steps, batch_size, spatial_dim)
        for t in range(time_steps):
            phase = 2 * np.pi * t / time_steps
            for i in range(batch_size):
                x = torch.linspace(0, 2 * np.pi, spatial_dim)
                trajectory[t, i] = torch.sin(x + phase) + 0.1 * torch.randn_like(x)
    
        # Test emergence validation
>       emergence = validator.emergence_validator.validate_emergence(trajectory[-1])

tests/test_validation/test_pattern_formation.py:268: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/patterns/formation.py:667: in validate_emergence
    coherence = self._compute_coherence(final_state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.patterns.formation.EmergenceValidator object at 0x7054f52140b0>
state = tensor([-0.1526,  0.2494,  0.3822,  0.4641,  0.5984,  0.5751,  1.0806,  0.9330,
         0.9890,  1.0151,  0.8917,  0....8, -0.8743, -0.9303, -0.9388, -0.9890,
        -1.1259, -0.9097, -0.9609, -0.6030, -0.7904, -0.5711, -0.1005, -0.1321])

    def _compute_coherence(self, state: torch.Tensor) -> float:
        """Compute coherence of pattern.
    
        Args:
            state: Pattern state tensor
    
        Returns:
            float: Coherence score
        """
        # Ensure we have a 2D tensor by taking the first batch and channel if present
        if state.ndim > 2:
            state = state.squeeze()  # Remove singleton dimensions
            if state.ndim > 2:
                state = state[0]  # Take first batch if still > 2D
                if state.ndim > 2:
                    state = state[0]  # Take first channel if still > 2D
    
        padding = 2
>       height, width = state.shape
E       ValueError: not enough values to unpack (expected 2, got 1)

src/validation/patterns/formation.py:701: ValueError
__________________ TestPatternStability.test_linear_stability __________________

self = <test_pattern_stability.TestPatternStability object at 0x7055174cee40>
flow = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
batch_size = 16, spatial_dim = 32

    def test_linear_stability(
        self, flow: GeometricFlow, batch_size: int, spatial_dim: int
    ):
        """Test linear stability analysis."""
        # Create linear stability analyzer
        analyzer = LinearStabilityValidator()
    
        # Generate test patterns with known stability
        def generate_stable_pattern():
            """Generate linearly stable pattern."""
            x = torch.linspace(0, 2 * np.pi, spatial_dim)
            return torch.sin(x) + 0.1 * torch.randn_like(x)
    
        def generate_unstable_pattern():
            """Generate linearly unstable pattern."""
            x = torch.linspace(0, 2 * np.pi, spatial_dim)
            return torch.exp(0.1 * x) * torch.sin(x)
    
        # Test stable patterns
        stable_patterns = torch.stack(
            [generate_stable_pattern() for _ in range(batch_size)]
        )
>       stable_result = analyzer.validate_stability(flow, stable_patterns)

tests/test_validation/test_pattern_stability.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:677: in jacobian
    outputs = func(*inputs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
x = tensor([[ 0.0991,  0.3818,  0.4199,  0.4370,  0.7723,  0.6845,  0.8578,  0.9559,
          0.9414,  0.8731,  0.7259,  ... -0.8977,
         -1.1414, -0.9907, -0.9019, -0.8056, -0.6022, -0.5629, -0.2488,  0.0185]],
       requires_grad=True)
return_path = False

    def forward(
        self, x: torch.Tensor, return_path: bool = False
    ) -> Tuple[torch.Tensor, Dict]:
        """Apply geometric flow with Ricci flow evolution.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_path: Whether to return the flow path
    
        Returns:
            Processed tensor and metrics dictionary
        """
>       batch_size, seq_len, _ = x.shape
E       ValueError: not enough values to unpack (expected 3, got 2)

src/core/tiling/geometric_flow.py:275: ValueError
________________ TestPatternStability.test_nonlinear_stability _________________

self = <test_pattern_stability.TestPatternStability object at 0x7055174cf020>
flow = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
batch_size = 16, spatial_dim = 32

    def test_nonlinear_stability(
        self, flow: GeometricFlow, batch_size: int, spatial_dim: int
    ):
        """Test nonlinear stability analysis."""
        # Create nonlinear stability analyzer
        analyzer = NonlinearStabilityValidator()
    
        # Generate test patterns with nonlinear dynamics
        def generate_pattern(stability: str) -> torch.Tensor:
            """Generate pattern with specified stability."""
            x = torch.linspace(0, 2 * np.pi, spatial_dim)
            if stability == "stable":
                return torch.tanh(torch.sin(x))
            return torch.sin(x) + 0.1 * torch.sin(2 * x)
    
        # Test stable patterns
        stable_patterns = torch.stack(
            [generate_pattern("stable") for _ in range(batch_size)]
        )
>       stable_result = analyzer.validate_stability(flow, stable_patterns)

tests/test_validation/test_pattern_stability.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:104: in validate_stability
    lyapunov = self._compute_lyapunov(flow, state)
src/validation/flow/stability.py:131: in _compute_lyapunov
    _, metrics = flow.forward(state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
x = tensor([[ 0.0000e+00,  1.9862e-01,  3.7511e-01,  5.1629e-01,  6.1987e-01,
          6.9036e-01,  7.3419e-01,  7.5671e-...671e-01,
         -7.3419e-01, -6.9036e-01, -6.1987e-01, -5.1629e-01, -3.7511e-01,
         -1.9862e-01,  1.7485e-07]])
return_path = False

    def forward(
        self, x: torch.Tensor, return_path: bool = False
    ) -> Tuple[torch.Tensor, Dict]:
        """Apply geometric flow with Ricci flow evolution.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_path: Whether to return the flow path
    
        Returns:
            Processed tensor and metrics dictionary
        """
>       batch_size, seq_len, _ = x.shape
E       ValueError: not enough values to unpack (expected 3, got 2)

src/core/tiling/geometric_flow.py:275: ValueError
_______________ TestPatternStability.test_perturbation_response ________________

self = <test_pattern_stability.TestPatternStability object at 0x7055174cf2f0>
batch_size = 16, spatial_dim = 32, time_steps = 100

    def test_perturbation_response(
        self,
        batch_size: int,
        spatial_dim: int,
        time_steps: int,
    ):
        """Test perturbation response analysis."""
        # Create perturbation analyzer
        analyzer = PerturbationAnalyzer()
    
        # Create pattern dynamics
        dynamics = PatternDynamics()
    
        # Generate test patterns and perturbations
        patterns = torch.stack(
            [
                torch.sin(torch.linspace(0, 2 * np.pi, spatial_dim))
                for _ in range(batch_size)
            ]
        )
        perturbation = 0.1 * torch.randn_like(patterns[0])
    
        # Test perturbation analysis
>       result = analyzer.analyze_perturbation(dynamics, patterns[0], perturbation)

tests/test_validation/test_pattern_stability.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/patterns/perturbation.py:67: in analyze_perturbation
    linear_response = self._compute_linear_response(
src/validation/patterns/perturbation.py:122: in _compute_linear_response
    jacobian = dynamics.compute_jacobian(pattern)
src/neural/attention/pattern/dynamics.py:164: in compute_jacobian
    return self.compute_stability_matrix(state, epsilon=1e-6, chunk_size=10)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.neural.attention.pattern.dynamics.PatternDynamics object at 0x7054fc39e000>
state = tensor([[ 0.0000e+00,  2.0130e-01,  3.9436e-01,  5.7127e-01,  7.2479e-01,
          8.4864e-01,  9.3775e-01,  9.8847e-...847e-01,
         -9.3775e-01, -8.4864e-01, -7.2479e-01, -5.7127e-01, -3.9436e-01,
         -2.0130e-01,  1.7485e-07]])
epsilon = 1e-06, chunk_size = 10

    def compute_stability_matrix(
        self,
        state: torch.Tensor,
        epsilon: float = 1e-6,
        chunk_size: int = 500
    ) -> torch.Tensor:
        """Compute stability matrix for current state.
    
        Args:
            state: Current state tensor
            epsilon: Small perturbation for numerical derivatives
            chunk_size: Size of chunks to process at once
    
        Returns:
            Stability matrix
        """
        # Get state shape and ensure proper dimensions
        if len(state.shape) == 4:  # [batch, channels, height, width]
            batch_size = state.shape[0]
            n = state.shape[1] * state.shape[2] * state.shape[3]
            state = state.view(batch_size, n)
        else:  # [channels, height, width]
            batch_size = 1
            n = state.numel()
            state = state.view(1, n)
    
        # Initialize Jacobian efficiently
        J = torch.zeros((n, n), dtype=state.dtype, device=state.device)
    
        # Use vectorized operations for efficiency
        for i in range(0, n, chunk_size):  # Process in chunks for memory efficiency
            end_idx = min(i + chunk_size, n)
            curr_chunk_size = end_idx - i
    
            # Create perturbations for this chunk
            perturb = torch.zeros((curr_chunk_size, n), dtype=state.dtype, device=state.device)
            perturb[range(curr_chunk_size), range(i, end_idx)] = epsilon
    
            # Forward differences (vectorized)
>           states_plus = (state + perturb).view(-1, self.dim, self.size, self.size)
E           RuntimeError: shape '[-1, 2, 32, 32]' is invalid for input of size 320

src/neural/attention/pattern/dynamics.py:205: RuntimeError
_________________ TestPatternStability.test_lyapunov_analysis __________________

self = <test_pattern_stability.TestPatternStability object at 0x7055174cf4d0>
flow = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
batch_size = 16, spatial_dim = 32

    def test_lyapunov_analysis(
        self, flow: GeometricFlow, batch_size: int, spatial_dim: int
    ):
        """Test Lyapunov stability analysis."""
        # Create nonlinear stability validator for Lyapunov analysis
        analyzer = NonlinearStabilityValidator()
    
        # Generate test trajectory
        def generate_trajectory() -> torch.Tensor:
            """Generate test trajectory."""
            t = torch.linspace(0, 10, 100)
            x = torch.linspace(0, 2 * np.pi, spatial_dim)
            X, T = torch.meshgrid(x, t, indexing="ij")
            return torch.sin(X - 0.3 * T) + 0.1 * torch.randn_like(X)
    
        trajectories = torch.stack([generate_trajectory() for _ in range(batch_size)])
    
        # Test nonlinear stability validation
>       result = analyzer.validate_stability(flow, trajectories)

tests/test_validation/test_pattern_stability.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:104: in validate_stability
    lyapunov = self._compute_lyapunov(flow, state)
src/validation/flow/stability.py:131: in _compute_lyapunov
    _, metrics = flow.forward(state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
x = tensor([[[-2.4052e-01, -1.2269e-01, -1.5789e-01,  ..., -2.8357e-01,
          -3.8953e-01, -1.2786e-01],
         [ 4....1.5924e-01],
         [-2.9152e-02,  4.8213e-03, -8.6018e-02,  ..., -2.2387e-01,
          -3.2151e-02, -1.6183e-01]]])
return_path = False

    def forward(
        self, x: torch.Tensor, return_path: bool = False
    ) -> Tuple[torch.Tensor, Dict]:
        """Apply geometric flow with Ricci flow evolution.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_path: Whether to return the flow path
    
        Returns:
            Processed tensor and metrics dictionary
        """
        batch_size, seq_len, _ = x.shape
    
        # Initial projection to manifold
        x_flat = x.reshape(-1, self.hidden_dim)
    
        # Apply flow field layers sequentially
        for layer in self.flow_field:
            x_flat = layer(x_flat)
    
        # Reshape back to original dimensions
>       x_manifold = x_flat.view(batch_size, seq_len, self.manifold_dim)
E       RuntimeError: shape '[16, 32, 16]' is invalid for input of size 12800

src/core/tiling/geometric_flow.py:285: RuntimeError
___________________ TestPatternStability.test_mode_stability ___________________

self = <test_pattern_stability.TestPatternStability object at 0x7055174cf710>
flow = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
batch_size = 16, spatial_dim = 32

    def test_mode_stability(
        self, flow: GeometricFlow, batch_size: int, spatial_dim: int
    ):
        """Test stability analysis of pattern modes."""
        # Create linear stability validator for mode analysis
        analyzer = LinearStabilityValidator()
    
        # Generate test patterns with multiple modes
        x = torch.linspace(0, 2 * np.pi, spatial_dim)
        patterns = []
        for _ in range(batch_size):
            # Combine multiple modes with different stabilities
            pattern = (
                torch.sin(x)  # Stable mode
                + 0.5 * torch.sin(2 * x)  # Less stable mode
                + 0.1 * torch.sin(3 * x)
            )  # Least stable mode
            patterns.append(pattern)
        patterns = torch.stack(patterns)
    
        # Test stability validation with mode decomposition
>       result = analyzer.validate_stability(flow, patterns)

tests/test_validation/test_pattern_stability.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:677: in jacobian
    outputs = func(*inputs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
x = tensor([[ 0.0000e+00,  4.5560e-01,  8.5053e-01,  1.1370e+00,  1.2893e+00,
          1.3077e+00,  1.2149e+00,  1.0484e+....2149e+00, -1.3077e+00, -1.2893e+00, -1.1370e+00, -8.5053e-01,
         -4.5560e-01,  3.5446e-07]], requires_grad=True)
return_path = False

    def forward(
        self, x: torch.Tensor, return_path: bool = False
    ) -> Tuple[torch.Tensor, Dict]:
        """Apply geometric flow with Ricci flow evolution.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_path: Whether to return the flow path
    
        Returns:
            Processed tensor and metrics dictionary
        """
>       batch_size, seq_len, _ = x.shape
E       ValueError: not enough values to unpack (expected 3, got 2)

src/core/tiling/geometric_flow.py:275: ValueError
_________________ TestPatternStability.test_stability_metrics __________________

self = <test_pattern_stability.TestPatternStability object at 0x7055174cf950>
flow = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
batch_size = 16, spatial_dim = 32

    def test_stability_metrics(
        self, flow: GeometricFlow, batch_size: int, spatial_dim: int
    ):
        """Test stability metrics computation and aggregation."""
        # Create both linear and nonlinear analyzers
        linear_analyzer = LinearStabilityValidator()
        nonlinear_analyzer = NonlinearStabilityValidator()
    
        # Generate test patterns
        patterns = torch.stack(
            [
                torch.sin(torch.linspace(0, 2 * np.pi, spatial_dim))
                for _ in range(batch_size)
            ]
        )
    
        # Test both types of stability analysis
>       linear_result = linear_analyzer.validate_stability(flow, patterns)

tests/test_validation/test_pattern_stability.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:677: in jacobian
    outputs = func(*inputs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
x = tensor([[ 0.0000e+00,  2.0130e-01,  3.9436e-01,  5.7127e-01,  7.2479e-01,
          8.4864e-01,  9.3775e-01,  9.8847e-....3775e-01, -8.4864e-01, -7.2479e-01, -5.7127e-01, -3.9436e-01,
         -2.0130e-01,  1.7485e-07]], requires_grad=True)
return_path = False

    def forward(
        self, x: torch.Tensor, return_path: bool = False
    ) -> Tuple[torch.Tensor, Dict]:
        """Apply geometric flow with Ricci flow evolution.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_path: Whether to return the flow path
    
        Returns:
            Processed tensor and metrics dictionary
        """
>       batch_size, seq_len, _ = x.shape
E       ValueError: not enough values to unpack (expected 3, got 2)

src/core/tiling/geometric_flow.py:275: ValueError
_______________ TestPatternStability.test_validation_integration _______________

self = <test_pattern_stability.TestPatternStability object at 0x7055174cfb90>
flow = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
batch_size = 16, spatial_dim = 32, time_steps = 100

    def test_validation_integration(
        self,
        flow: GeometricFlow,
        batch_size: int,
        spatial_dim: int,
        time_steps: int,
    ):
        """Test integrated stability validation."""
        # Create analyzers
        linear_analyzer = LinearStabilityValidator()
        nonlinear_analyzer = NonlinearStabilityValidator()
        perturbation_analyzer = PerturbationAnalyzer()
        dynamics = PatternDynamics()
    
        # Generate test pattern evolution
        time_series = []
        for _ in range(batch_size):
            t = torch.linspace(0, 10, time_steps)
            x = torch.linspace(0, 2 * np.pi, spatial_dim)
            X, T = torch.meshgrid(x, t, indexing="ij")
            pattern = torch.sin(X - 0.3 * T) + 0.1 * torch.randn_like(X)
            time_series.append(pattern)
        time_series = torch.stack(time_series)
    
        # Run stability validations
>       linear_result = linear_analyzer.validate_stability(flow, time_series[:, 0])

tests/test_validation/test_pattern_stability.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:677: in jacobian
    outputs = func(*inputs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
x = tensor([[ 0.1123, -0.1543, -0.0358,  ..., -0.3534, -0.2151, -0.1206],
        [-0.1001,  0.0489, -0.2734,  ..., -0.031...1, -0.1475, -0.2009],
        [-0.1294, -0.0112, -0.0728,  ..., -0.0764, -0.3184, -0.1528]],
       requires_grad=True)
return_path = False

    def forward(
        self, x: torch.Tensor, return_path: bool = False
    ) -> Tuple[torch.Tensor, Dict]:
        """Apply geometric flow with Ricci flow evolution.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_path: Whether to return the flow path
    
        Returns:
            Processed tensor and metrics dictionary
        """
>       batch_size, seq_len, _ = x.shape
E       ValueError: not enough values to unpack (expected 3, got 2)

src/core/tiling/geometric_flow.py:275: ValueError
__________________ TestPatternStability.test_dynamical_system __________________

self = <test_pattern_stability.TestPatternStability object at 0x7055174ec080>
flow = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
batch_size = 16, spatial_dim = 32

    def test_dynamical_system(
        self, flow: GeometricFlow, batch_size: int, spatial_dim: int
    ):
        """Test dynamical system properties."""
        # Create pattern dynamics
        dynamics = PatternDynamics()
    
        # Generate test pattern
        pattern = torch.sin(torch.linspace(0, 2 * np.pi, spatial_dim))
    
        # Test evolution
>       evolved = dynamics.evolve_pattern(pattern, steps=10)

tests/test_validation/test_pattern_stability.py:292: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/neural/attention/pattern/dynamics.py:149: in evolve_pattern
    reaction = self.reaction.reaction_term(current)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.neural.attention.pattern.reaction.ReactionSystem object at 0x7054f527da30>
state = tensor([ 0.0000e+00,  2.0130e-01,  3.9436e-01,  5.7127e-01,  7.2479e-01,
         8.4864e-01,  9.3775e-01,  9.8847e-01...9.8847e-01,
        -9.3775e-01, -8.4864e-01, -7.2479e-01, -5.7127e-01, -3.9436e-01,
        -2.0130e-01,  1.7485e-07])

    def reaction_term(self, state: torch.Tensor) -> torch.Tensor:
        """Default reaction term for pattern formation.
    
        This implements a simple activator-inhibitor system with:
            - Autocatalytic production of activator
            - Linear degradation of both species
            - Nonlinear inhibition
    
        Args:
            state: Input tensor [batch, channels, height, width]
    
        Returns:
            Reaction term tensor [batch, channels, height, width]
        """
        # Extract activator and inhibitor
>       activator = state[:,0:1]  # [batch, 1, height, width]
E       IndexError: too many indices for tensor of dimension 1

src/neural/attention/pattern/reaction.py:56: IndexError
_________________ TestPatternStability.test_bifurcation_theory _________________

self = <test_pattern_stability.TestPatternStability object at 0x7055174ec2c0>
flow = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
batch_size = 16, spatial_dim = 32

    def test_bifurcation_theory(
        self, flow: GeometricFlow, batch_size: int, spatial_dim: int
    ):
        """Test bifurcation theory properties."""
        # Create nonlinear stability validator
        analyzer = NonlinearStabilityValidator()
    
        # Generate test patterns at different parameter values
        patterns = []
        for param in torch.linspace(0, 1, batch_size):
            x = torch.linspace(0, 2 * np.pi, spatial_dim)
            pattern = torch.sin(x) + param * torch.sin(2 * x)
            patterns.append(pattern)
        patterns = torch.stack(patterns)
    
        # Test stability across parameter range
>       result = analyzer.validate_stability(flow, patterns)

tests/test_validation/test_pattern_stability.py:318: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:104: in validate_stability
    lyapunov = self._compute_lyapunov(flow, state)
src/validation/flow/stability.py:131: in _compute_lyapunov
    _, metrics = flow.forward(state)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
x = tensor([[ 0.0000e+00,  2.0130e-01,  3.9436e-01,  5.7127e-01,  7.2479e-01,
          8.4864e-01,  9.3775e-01,  9.8847e-...878e+00,
         -1.5891e+00, -1.7464e+00, -1.7235e+00, -1.5090e+00, -1.1191e+00,
         -5.9565e-01,  5.2454e-07]])
return_path = False

    def forward(
        self, x: torch.Tensor, return_path: bool = False
    ) -> Tuple[torch.Tensor, Dict]:
        """Apply geometric flow with Ricci flow evolution.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_path: Whether to return the flow path
    
        Returns:
            Processed tensor and metrics dictionary
        """
>       batch_size, seq_len, _ = x.shape
E       ValueError: not enough values to unpack (expected 3, got 2)

src/core/tiling/geometric_flow.py:275: ValueError
_________________ TestPatternStability.test_stability_analysis _________________

self = <test_pattern_stability.TestPatternStability object at 0x7055174ec500>
flow = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
batch_size = 16, spatial_dim = 32

    def test_stability_analysis(
        self, flow: GeometricFlow, batch_size: int, spatial_dim: int
    ):
        """Test stability analysis methods."""
        # Create analyzers
        linear_analyzer = LinearStabilityValidator()
        nonlinear_analyzer = NonlinearStabilityValidator()
        dynamics = PatternDynamics()
    
        # Generate test pattern
        pattern = torch.sin(torch.linspace(0, 2 * np.pi, spatial_dim))
    
        # Test linear stability
>       linear_result = linear_analyzer.validate_stability(flow, pattern)

tests/test_validation/test_pattern_stability.py:336: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/flow/stability.py:61: in validate_stability
    jacobian_tuple = torch.autograd.functional.jacobian(flow.forward, state)
venv/lib/python3.12/site-packages/torch/autograd/functional.py:677: in jacobian
    outputs = func(*inputs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GeometricFlow(
  (metric): RiemannianMetric()
  (arithmetic): ArithmeticDynamics(
    (height_map): Linear(in_features...atures=16, out_features=16, bias=True)
    (1): SiLU()
    (2): Linear(in_features=16, out_features=1, bias=True)
  )
)
x = tensor([ 0.0000e+00,  2.0130e-01,  3.9436e-01,  5.7127e-01,  7.2479e-01,
         8.4864e-01,  9.3775e-01,  9.8847e-01...-9.3775e-01, -8.4864e-01, -7.2479e-01, -5.7127e-01, -3.9436e-01,
        -2.0130e-01,  1.7485e-07], requires_grad=True)
return_path = False

    def forward(
        self, x: torch.Tensor, return_path: bool = False
    ) -> Tuple[torch.Tensor, Dict]:
        """Apply geometric flow with Ricci flow evolution.
    
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
            return_path: Whether to return the flow path
    
        Returns:
            Processed tensor and metrics dictionary
        """
>       batch_size, seq_len, _ = x.shape
E       ValueError: not enough values to unpack (expected 3, got 1)

src/core/tiling/geometric_flow.py:275: ValueError
__________________ TestStateValidation.test_state_preparation __________________

self = <test_state_validation.TestStateValidation object at 0x70551749a2a0>
validator = <src.validation.quantum.state.StatePreparationValidator object at 0x7054f5290740>
batch_size = 16, num_qubits = 4
basis_labels = ['|0000⟩', '|0001⟩', '|0010⟩', '|0011⟩', '|0100⟩', '|0101⟩', ...]

    def test_state_preparation(
        self, validator: StatePreparationValidator, batch_size: int, num_qubits: int, basis_labels: List[str]
    ):
        """Test quantum state preparation validation."""
        # Generate target state
        dim = 2**num_qubits
        amplitudes = torch.randn(batch_size, dim, dtype=torch.complex64)
        amplitudes = amplitudes / torch.norm(amplitudes, dim=1, keepdim=True)
        phase = torch.exp(1j * torch.rand(batch_size, 1))
        target = QuantumState(amplitudes=amplitudes, basis_labels=basis_labels, phase=phase)
    
        # Generate prepared state close to target
        prepared_amplitudes = amplitudes + 1e-3 * torch.randn_like(amplitudes)
        prepared_amplitudes = prepared_amplitudes / torch.norm(prepared_amplitudes, dim=1, keepdim=True)
        prepared = QuantumState(amplitudes=prepared_amplitudes, basis_labels=basis_labels, phase=phase)
    
        # Test preparation validation
>       result = validator.validate_preparation(target, prepared)

tests/test_validation/test_state_validation.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/quantum/state.py:241: in validate_preparation
    fidelity = self._compute_fidelity(target, prepared)
src/validation/quantum/state.py:289: in _compute_fidelity
    if target.is_pure(self.tolerance) and prepared.is_pure(self.tolerance):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumState(amplitudes=tensor([[-1.1243e-01+0.1929j,  2.9871e-01+0.2111j,  2.3351e-02+0.1208j,
          3.1066e-01-0...665+0.2568j],
        [0.7577+0.6526j],
        [0.8318+0.5551j],
        [0.8510+0.5251j],
        [0.9998+0.0188j]]))
tolerance = 1e-06

    def is_pure(self, tolerance: float = 1e-6) -> bool:
        """Check if the state is pure by computing the purity of its density matrix.
    
        A pure state has Tr(ρ²) = 1, while mixed states have Tr(ρ²) < 1.
    
        Args:
            tolerance: Numerical tolerance for comparison with 1.0
    
        Returns:
            bool: True if the state is pure, False otherwise
        """
        rho = self.density_matrix()
>       purity = torch.trace(torch.matmul(rho, rho)).real
E       RuntimeError: trace: expected a matrix, but got tensor with dim 3

src/core/quantum/types.py:148: RuntimeError
______________ TestStateValidation.test_density_matrix_properties ______________

self = <test_state_validation.TestStateValidation object at 0x7055174ec170>
validator = <src.validation.quantum.state.StatePreparationValidator object at 0x7055174ec380>
batch_size = 16, num_qubits = 4
basis_labels = ['|0000⟩', '|0001⟩', '|0010⟩', '|0011⟩', '|0100⟩', '|0101⟩', ...]

    def test_density_matrix_properties(
        self, validator: StatePreparationValidator, batch_size: int, num_qubits: int, basis_labels: List[str]
    ):
        """Test density matrix validation."""
        # Generate pure state
        dim = 2**num_qubits
        amplitudes = torch.randn(batch_size, dim, dtype=torch.complex64)
        amplitudes = amplitudes / torch.norm(amplitudes, dim=1, keepdim=True)
        phase = torch.exp(1j * torch.rand(batch_size, 1))
        state = QuantumState(amplitudes=amplitudes, basis_labels=basis_labels, phase=phase)
    
        # Get density matrix
        density = state.density_matrix()
    
        # Test density matrix properties
>       result = validator._validate_density_matrix(density)

tests/test_validation/test_state_validation.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.quantum.state.StatePreparationValidator object at 0x7055174ec380>
density = tensor([[[ 0.0409+0.0000e+00j, -0.0302-2.6241e-02j, -0.0294+2.4643e-02j,
           ..., -0.0202+1.5901e-02j,  0.0282+...e-03j, -0.0052-1.1335e-02j,
           ...,  0.0004-2.3082e-02j, -0.0131+4.4650e-02j,
           0.0133+0.0000e+00j]]])

    def _validate_density_matrix(self, density: torch.Tensor) -> DensityMatrixValidation:
        """Validate density matrix properties."""
        # Check Hermiticity
        hermitian = torch.allclose(density, density.conj().T, atol=self.tolerance)
    
        # Check trace normalization
>       trace = torch.trace(density)
E       RuntimeError: trace: expected a matrix, but got tensor with dim 3

src/validation/quantum/state.py:362: RuntimeError
__________________ TestStateValidation.test_state_tomography ___________________

self = <test_state_validation.TestStateValidation object at 0x7055174ecfb0>
validator = <src.validation.quantum.state.StatePreparationValidator object at 0x7054f5217f80>
batch_size = 16, num_qubits = 4
basis_labels = ['|0000⟩', '|0001⟩', '|0010⟩', '|0011⟩', '|0100⟩', '|0101⟩', ...]

    def test_state_tomography(
        self, validator: StatePreparationValidator, batch_size: int, num_qubits: int, basis_labels: List[str]
    ):
        """Test state tomography validation."""
        # Generate test state
        dim = 2**num_qubits
        amplitudes = torch.randn(batch_size, dim, dtype=torch.complex64)
        amplitudes = amplitudes / torch.norm(amplitudes, dim=1, keepdim=True)
        phase = torch.exp(1j * torch.rand(batch_size, 1))
        state = QuantumState(amplitudes=amplitudes, basis_labels=basis_labels, phase=phase)
    
        # Generate measurement operators
        def generate_projector(basis_state: int) -> torch.Tensor:
            """Generate projection operator."""
            proj = torch.zeros(dim, dim, dtype=torch.complex64)
            proj[basis_state, basis_state] = 1
            return proj
    
        projectors = [generate_projector(i) for i in range(dim)]
    
        # Test tomography validation
>       result = validator._validate_tomography(state, projectors)

tests/test_validation/test_state_validation.py:139: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/quantum/state.py:387: in _validate_tomography
    reconstructed, error = self._reconstruct_state(state, projectors)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.validation.quantum.state.StatePreparationValidator object at 0x7054f5217f80>
state = QuantumState(amplitudes=tensor([[-0.0695-5.3192e-02j,  0.2283-4.5775e-02j, -0.1717-1.2440e-01j,
         -0.0810+4.643...881+0.1539j],
        [0.6742+0.7385j],
        [0.7062+0.7080j],
        [0.6118+0.7910j],
        [0.7400+0.6726j]]))
projectors = [tensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,
         0.+....+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,
         0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]]), ...]

    def _reconstruct_state(
        self, state: QuantumState, projectors: List[torch.Tensor]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Perform quantum state reconstruction."""
        # Initialize estimated state
        dim = projectors[0].shape[0]
        rho = torch.eye(dim, dtype=torch.complex64) / dim
    
        # Get measurement probabilities from true state
        true_probs = torch.stack([
>           torch.trace(torch.matmul(state.density_matrix(), proj))
            for proj in projectors
        ])
E       RuntimeError: trace: expected a matrix, but got tensor with dim 3

src/validation/quantum/state.py:412: RuntimeError
_______________ TestStateValidation.test_validation_integration ________________

self = <test_state_validation.TestStateValidation object at 0x7055174ed1c0>
validator = <src.validation.quantum.state.StatePreparationValidator object at 0x7054f5292a80>
batch_size = 16, num_qubits = 4
basis_labels = ['|0000⟩', '|0001⟩', '|0010⟩', '|0011⟩', '|0100⟩', '|0101⟩', ...]

    def test_validation_integration(
        self, validator: StatePreparationValidator, batch_size: int, num_qubits: int, basis_labels: List[str]
    ):
        """Test integrated state validation."""
        # Generate test state
        dim = 2**num_qubits
        amplitudes = torch.randn(batch_size, dim, dtype=torch.complex64)
        amplitudes = amplitudes / torch.norm(amplitudes, dim=1, keepdim=True)
        phase = torch.exp(1j * torch.rand(batch_size, 1))
        target = QuantumState(amplitudes=amplitudes, basis_labels=basis_labels, phase=phase)
    
        # Test preparation
        prepared_amplitudes = amplitudes + 1e-3 * torch.randn_like(amplitudes)
        prepared_amplitudes = prepared_amplitudes / torch.norm(prepared_amplitudes, dim=1, keepdim=True)
        prepared = QuantumState(amplitudes=prepared_amplitudes, basis_labels=basis_labels, phase=phase)
>       prep_result = validator.validate_preparation(target, prepared)

tests/test_validation/test_state_validation.py:161: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/validation/quantum/state.py:241: in validate_preparation
    fidelity = self._compute_fidelity(target, prepared)
src/validation/quantum/state.py:289: in _compute_fidelity
    if target.is_pure(self.tolerance) and prepared.is_pure(self.tolerance):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = QuantumState(amplitudes=tensor([[-0.0540+0.1448j, -0.0798-0.3584j, -0.1679-0.2437j, -0.1463-0.0989j,
         -0.0557+...990+0.0441j],
        [0.8804+0.4743j],
        [0.6890+0.7247j],
        [0.8979+0.4402j],
        [0.9908+0.1350j]]))
tolerance = 1e-06

    def is_pure(self, tolerance: float = 1e-6) -> bool:
        """Check if the state is pure by computing the purity of its density matrix.
    
        A pure state has Tr(ρ²) = 1, while mixed states have Tr(ρ²) < 1.
    
        Args:
            tolerance: Numerical tolerance for comparison with 1.0
    
        Returns:
            bool: True if the state is pure, False otherwise
        """
        rho = self.density_matrix()
>       purity = torch.trace(torch.matmul(rho, rho)).real
E       RuntimeError: trace: expected a matrix, but got tensor with dim 3

src/core/quantum/types.py:148: RuntimeError
=============================== warnings summary ===============================
tests/core/tiling/test_strategies.py:24
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/core/tiling/test_strategies.py:24: PytestCollectionWarning: cannot collect test class 'TestAttentionTile' because it has a __init__ constructor (from: tests/core/tiling/test_strategies.py)
    class TestAttentionTile(nn.Module):

tests/performance/vulkan/gpu/test_memory_management.py:82
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/performance/vulkan/gpu/test_memory_management.py:82: PytestUnknownMarkWarning: Unknown pytest.mark.gpu - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.gpu

tests/performance/vulkan/gpu/test_memory_management.py:99
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/performance/vulkan/gpu/test_memory_management.py:99: PytestUnknownMarkWarning: Unknown pytest.mark.gpu - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.gpu

tests/performance/vulkan/gpu/test_memory_management.py:117
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/performance/vulkan/gpu/test_memory_management.py:117: PytestUnknownMarkWarning: Unknown pytest.mark.gpu - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.gpu

tests/performance/vulkan/gpu/test_memory_management.py:144
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/performance/vulkan/gpu/test_memory_management.py:144: PytestUnknownMarkWarning: Unknown pytest.mark.gpu - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.gpu

tests/performance/vulkan/gpu/test_memory_management.py:160
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/performance/vulkan/gpu/test_memory_management.py:160: PytestUnknownMarkWarning: Unknown pytest.mark.gpu - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.gpu

tests/test_neural/test_flow/test_geometric_flow.py:94
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:94: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency()

tests/test_neural/test_flow/test_geometric_flow.py:119
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:119: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency(depends=["TestGeometricFlow::test_metric_computation"])

tests/test_neural/test_flow/test_geometric_flow.py:126
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:126: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency(depends=["TestGeometricFlow::test_ricci_tensor"])

tests/test_neural/test_flow/test_geometric_flow.py:134
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:134: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency(depends=["TestGeometricFlow::test_flow_computation"])

tests/test_neural/test_flow/test_geometric_flow.py:143
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:143: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency(depends=["TestGeometricFlow::test_flow_step"])

tests/test_neural/test_flow/test_geometric_flow.py:151
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:151: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency(depends=["TestGeometricFlow::test_flow_step"])

tests/test_neural/test_flow/test_geometric_flow.py:160
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:160: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency(depends=["TestGeometricFlow::test_flow_normalization"])

tests/test_neural/test_flow/test_geometric_flow.py:176
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:176: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency(depends=["TestGeometricFlow::test_geometric_invariants"])

tests/test_neural/test_flow/test_geometric_flow.py:196
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:196: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency(depends=["TestGeometricFlow::test_geometric_invariants"])

tests/test_neural/test_flow/test_geometric_flow.py:213
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_neural/test_flow/test_geometric_flow.py:213: PytestUnknownMarkWarning: Unknown pytest.mark.dependency - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.dependency(depends=["TestGeometricFlow::test_flow_stability"])

tests/test_validation/test_framework.py:148
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_validation/test_framework.py:148: PytestUnknownMarkWarning: Unknown pytest.mark.level0 - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.level0

tests/test_validation/test_framework.py:185
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_validation/test_framework.py:185: PytestUnknownMarkWarning: Unknown pytest.mark.level0 - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.level0

tests/test_validation/test_framework.py:230
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_validation/test_framework.py:230: PytestUnknownMarkWarning: Unknown pytest.mark.level0 - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.level0

tests/test_validation/test_framework.py:273
  /home/d/Desktop/adaptive-attention-tiling-v2/tests/test_validation/test_framework.py:273: PytestUnknownMarkWarning: Unknown pytest.mark.level1 - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.level1

tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_state_preparation
  /home/d/Desktop/adaptive-attention-tiling-v2/src/core/quantum/state_space.py:86: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /home/d/Desktop/pytorch-vulkan-private/aten/src/ATen/native/Copy.cpp:308.)
    real_part = amplitudes[..., :self.dim].to(torch.float64)

tests/test_memory/test_memory_management.py::test_tensor_lifecycle
tests/test_memory/test_memory_management.py::test_operation_cleanup
tests/test_memory/test_memory_management.py::test_hyperbolic_operations
tests/test_memory/test_memory_management.py::test_nested_operations
tests/test_memory/test_memory_management.py::test_memory_stress
  /home/d/Desktop/adaptive-attention-tiling-v2/venv/lib/python3.12/site-packages/torch/__init__.py:1087: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead
    return isinstance(obj, torch.Tensor)

tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_controllability
  /home/d/Desktop/adaptive-attention-tiling-v2/venv/lib/python3.12/site-packages/_pytest/python.py:163: PytestReturnNotNoneWarning: Expected None, but tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_controllability returned tensor([[1., 0., 0., 0.],
          [0., 1., 0., 0.],
          [0., 0., 1., 0.],
          [0., 0., 0., 1.]]), which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/test_validation/test_pattern_flow.py::test_wavelength_computation_diagnostic
  /home/d/Desktop/adaptive-attention-tiling-v2/venv/lib/python3.12/site-packages/_pytest/python.py:163: PytestReturnNotNoneWarning: Expected None, but tests/test_validation/test_pattern_flow.py::test_wavelength_computation_diagnostic returned 8.0, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

tests/test_validation/test_state_validation.py::TestStateValidation::test_density_matrix_properties
  /home/d/Desktop/adaptive-attention-tiling-v2/src/validation/quantum/state.py:359: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /home/d/Desktop/pytorch-vulkan-private/aten/src/ATen/native/TensorShape.cpp:4409.)
    hermitian = torch.allclose(density, density.conj().T, atol=self.tolerance)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/core/attention/test_geometric.py::test_exp_log_inverse - assert ...
FAILED tests/core/tiling/test_strategies.py::test_performance_benchmarking - ...
FAILED tests/core/tiling/test_strategies.py::test_state_management - assert 3...
FAILED tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_initialization
FAILED tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_after_processing
FAILED tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_history
FAILED tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_with_resolution_change
FAILED tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_with_neighbors
FAILED tests/metrics/test_metrics_integration.py::TestMetricsIntegration::test_metrics_during_adaptation
FAILED tests/performance/benchmarks/test_core.py::TestCoreOperations::test_attention_computation
FAILED tests/performance/benchmarks/test_core.py::TestCoreOperations::test_pattern_formation
FAILED tests/performance/benchmarks/test_core.py::TestCoreOperations::test_flow_evolution
FAILED tests/performance/benchmarks/test_core.py::TestCoreOperations::test_memory_patterns
FAILED tests/performance/benchmarks/test_core.py::TestCoreOperations::test_scaling_characteristics
FAILED tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.1-matrix_size0]
FAILED tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.1-matrix_size1]
FAILED tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.5-matrix_size0]
FAILED tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.5-matrix_size1]
FAILED tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.9-matrix_size0]
FAILED tests/performance/cpu/test_algorithms.py::test_fast_path_optimization[0.9-matrix_size1]
FAILED tests/performance/cpu/test_algorithms.py::test_branch_prediction[matrix_size0-1]
FAILED tests/performance/cpu/test_algorithms.py::test_branch_prediction[matrix_size0-16]
FAILED tests/performance/cpu/test_algorithms.py::test_branch_prediction[matrix_size1-1]
FAILED tests/performance/cpu/test_algorithms.py::test_branch_prediction[matrix_size1-16]
FAILED tests/performance/cpu/test_algorithms.py::test_loop_optimization[O0]
FAILED tests/performance/cpu/test_algorithms.py::test_loop_optimization[O1]
FAILED tests/performance/cpu/test_algorithms.py::test_loop_optimization[O2]
FAILED tests/performance/cpu/test_algorithms.py::test_loop_optimization[O3]
FAILED tests/performance/cpu/test_algorithms.py::test_numerical_stability[matrix_size0]
FAILED tests/performance/cpu/test_algorithms.py::test_numerical_stability[matrix_size1]
FAILED tests/performance/cpu/test_algorithms.py::test_optimization_overhead
FAILED tests/performance/cpu/test_memory.py::test_memory_pool_efficiency[32-1024]
FAILED tests/performance/cpu/test_memory.py::test_memory_pool_efficiency[32-4096]
FAILED tests/performance/cpu/test_memory.py::test_memory_pool_efficiency[128-1024]
FAILED tests/performance/cpu/test_memory.py::test_memory_pool_efficiency[128-4096]
FAILED tests/performance/cpu/test_memory.py::test_resource_cleanup - IndexErr...
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_scale_connection
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_renormalization_flow
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_fixed_points
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_anomaly_polynomial
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_scale_invariants
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_callan_symanzik
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_operator_expansion
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_conformal_symmetry
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_holographic_scaling
FAILED tests/test_core/test_crystal/test_scale.py::TestScaleCohomology::test_entanglement_scaling
FAILED tests/test_core/test_patterns/test_fiber_bundle.py::TestConnectionFormHypothesis::test_linearity_property
FAILED tests/test_core/test_patterns/test_fiber_bundle.py::TestConnectionFormHypothesis::test_levi_civita_symmetry_property
FAILED tests/test_core/test_patterns/test_fiber_bundle.py::TestGeometricComponents::test_metric_derivatives
FAILED tests/test_core/test_patterns/test_riemannian.py::test_christoffel_properties
FAILED tests/test_core/test_patterns/test_riemannian.py::test_parallel_transport
FAILED tests/test_core/test_patterns/test_riemannian.py::test_curvature_identities
FAILED tests/test_core/test_patterns/test_riemannian.py::test_geodesic_equation
FAILED tests/test_core/test_patterns/test_riemannian.py::test_sectional_curvature
FAILED tests/test_core/test_patterns/test_riemannian.py::test_lie_derivative
FAILED tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_state_tomography
FAILED tests/test_core/test_quantum/test_state_space.py::TestHilbertSpace::test_geometric_phase
FAILED tests/test_infrastructure/test_infrastructure.py::TestInfrastructure::test_vulkan_integration
FAILED tests/test_infrastructure/test_infrastructure.py::TestInfrastructure::test_resource_allocation
FAILED tests/test_infrastructure/test_infrastructure.py::TestInfrastructure::test_infrastructure_integration
FAILED tests/test_integration/test_cross_validation.py::TestCrossValidation::test_pattern_quantum_interaction
FAILED tests/test_integration/test_cross_validation.py::TestCrossValidation::test_geometric_pattern_coupling
FAILED tests/test_integration/test_cross_validation.py::TestCrossValidation::test_infrastructure_framework
FAILED tests/test_integration/test_cross_validation.py::TestCrossValidation::test_end_to_end_validation
FAILED tests/test_integration/test_cross_validation.py::TestCrossValidation::test_validation_stability
FAILED tests/test_neural/test_attention/test_exponential.py::test_exponential_map
FAILED tests/test_neural/test_attention/test_logarithm.py::test_logarithm_map
FAILED tests/test_neural/test_attention/test_logarithm.py::test_logarithm_map_properties
FAILED tests/test_neural/test_attention/test_logarithm.py::test_exp_log_inverse
FAILED tests/test_neural/test_attention/test_minkowski.py::test_advanced_minkowski_properties
FAILED tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_bifurcation_analysis
FAILED tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_bifurcation_detection_threshold
FAILED tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_solution_branches
FAILED tests/test_neural/test_attention/test_pattern/test_control.py::test_pattern_control
FAILED tests/test_neural/test_attention/test_pattern/test_control.py::test_spatiotemporal_evolution
FAILED tests/test_neural/test_attention/test_pattern/test_diffusion_properties.py::test_convergence_to_steady_state
FAILED tests/test_neural/test_attention/test_pattern/test_quantum.py::TestQuantumPatterns::test_quantum_evolution
FAILED tests/test_neural/test_attention/test_pattern/test_quantum.py::TestQuantumPatterns::test_quantum_potential
FAILED tests/test_neural/test_attention/test_pattern/test_reaction_diffusion.py::test_reaction_diffusion
FAILED tests/test_neural/test_attention/test_pattern/test_stability.py::test_stability_analysis
FAILED tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_stability_analysis_basic
FAILED tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_stability_analysis_advanced
FAILED tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_pattern_formation
FAILED tests/test_neural/test_attention/test_pattern_dynamics.py::TestPatternDynamics::test_forward_pass
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_attention_state_preparation
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_attention_pattern_computation
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_geometric_attention_flow
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_quantum_classical_interface
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_multi_head_integration
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_geometric_phases
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_manifold_curvature
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_attention_entanglement
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_error_correction
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_topological_features
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_attention_patterns
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_geometric_structures
FAILED tests/test_neural/test_attention/test_quantum_geometric_attention.py::TestQuantumGeometricAttention::test_pattern_dynamics
FAILED tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_geometric_invariants
FAILED tests/test_neural/test_flow/test_geometric_flow.py::TestGeometricFlow::test_flow_convergence
FAILED tests/test_neural/test_flow/test_hamiltonian.py::TestHamiltonianSystem::test_hamiltonian_computation
FAILED tests/test_neural/test_flow/test_hamiltonian.py::TestHamiltonianSystem::test_evolution
FAILED tests/test_neural/test_flow/test_hamiltonian.py::TestHamiltonianSystem::test_canonical_transformations
FAILED tests/test_neural/test_flow/test_hamiltonian.py::TestHamiltonianSystem::test_symplectic_integration
FAILED tests/test_neural/test_flow/test_tensor_shapes.py::TestTensorShapes::test_validation_shapes
FAILED tests/test_neural/test_flow/test_tensor_shapes.py::TestTensorShapes::test_convergence_shapes
FAILED tests/test_utils/test_helpers.py::test_tensor_assertions - AssertionEr...
FAILED tests/test_utils/test_helpers.py::test_numerical_stability - TypeError...
FAILED tests/test_utils/test_helpers.py::test_performance_benchmark - Attribu...
FAILED tests/test_validation/test_debug_flow.py::TestStabilityValidation::test_validate_stability_return_type[is_valid]
FAILED tests/test_validation/test_debug_flow.py::TestStabilityValidation::test_validate_stability_return_type[message]
FAILED tests/test_validation/test_debug_flow.py::TestStabilityValidation::test_validate_stability_return_type[data]
FAILED tests/test_validation/test_debug_flow.py::TestStabilityValidation::test_stability_spectrum_computation
FAILED tests/test_validation/test_debug_flow.py::TestEnergyValidation::test_energy_computation[check0]
FAILED tests/test_validation/test_debug_flow.py::TestEnergyValidation::test_energy_computation[check1]
FAILED tests/test_validation/test_debug_flow.py::TestEnergyValidation::test_energy_computation[check2]
FAILED tests/test_validation/test_debug_flow.py::TestEnergyValidation::test_energy_conservation_validation
FAILED tests/test_validation/test_debug_flow.py::TestFlowProperties::test_flow_properties_construction
FAILED tests/test_validation/test_debug_flow.py::TestFlowProperties::test_flow_properties_with_energy
FAILED tests/test_validation/test_flow_validation.py::TestFlowValidation::test_energy_conservation
FAILED tests/test_validation/test_flow_validation.py::TestFlowValidation::test_flow_monotonicity
FAILED tests/test_validation/test_flow_validation.py::TestFlowValidation::test_long_time_existence
FAILED tests/test_validation/test_flow_validation.py::TestFlowValidation::test_singularity_detection
FAILED tests/test_validation/test_flow_validation.py::TestFlowValidation::test_validation_integration
FAILED tests/test_validation/test_framework.py::TestValidationFramework::test_geometric_validation
FAILED tests/test_validation/test_framework.py::TestValidationFramework::test_quantum_validation
FAILED tests/test_validation/test_framework.py::TestValidationFramework::test_pattern_validation
FAILED tests/test_validation/test_framework.py::TestValidationFramework::test_integrated_validation
FAILED tests/test_validation/test_framework.py::TestValidationFramework::test_error_handling
FAILED tests/test_validation/test_framework.py::TestValidationFramework::test_validation_metrics
FAILED tests/test_validation/test_framework.py::TestValidationFramework::test_full_integration
FAILED tests/test_validation/test_framework.py::TestValidationFramework::test_validate_all
FAILED tests/test_validation/test_metric_validation.py::TestMetricValidation::test_fisher_rao_metric
FAILED tests/test_validation/test_metric_validation.py::TestMetricValidation::test_curvature_validation
FAILED tests/test_validation/test_metric_validation.py::TestMetricValidation::test_metric_properties
FAILED tests/test_validation/test_model_geometric.py::TestModelGeometricValidator::test_validate_layer_geometry
FAILED tests/test_validation/test_model_geometric.py::TestModelGeometricValidator::test_validate_attention_geometry
FAILED tests/test_validation/test_model_geometric.py::TestModelGeometricValidator::test_geometric_preservation
FAILED tests/test_validation/test_pattern_flow.py::test_pattern_flow_stability
FAILED tests/test_validation/test_pattern_flow.py::test_pattern_flow_energy
FAILED tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_pattern_emergence
FAILED tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_spatial_organization
FAILED tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_validation_integration
FAILED tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_reaction_diffusion
FAILED tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_symmetry_breaking
FAILED tests/test_validation/test_pattern_formation.py::TestPatternFormation::test_pattern_stability
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_linear_stability
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_nonlinear_stability
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_perturbation_response
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_lyapunov_analysis
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_mode_stability
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_stability_metrics
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_validation_integration
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_dynamical_system
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_bifurcation_theory
FAILED tests/test_validation/test_pattern_stability.py::TestPatternStability::test_stability_analysis
FAILED tests/test_validation/test_state_validation.py::TestStateValidation::test_state_preparation
FAILED tests/test_validation/test_state_validation.py::TestStateValidation::test_density_matrix_properties
FAILED tests/test_validation/test_state_validation.py::TestStateValidation::test_state_tomography
FAILED tests/test_validation/test_state_validation.py::TestStateValidation::test_validation_integration
ERROR tests/performance/benchmarks/test_vulkan.py::TestVulkanBenchmarks::test_pool_efficiency
ERROR tests/performance/benchmarks/test_vulkan.py::TestVulkanBenchmarks::test_pool_efficiency
ERROR tests/performance/benchmarks/test_vulkan.py::TestVulkanBenchmarks::test_memory_operations
ERROR tests/performance/benchmarks/test_vulkan.py::TestVulkanBenchmarks::test_memory_operations
ERROR tests/performance/vulkan/gpu/test_memory_management.py::test_tensor_allocation
ERROR tests/performance/vulkan/gpu/test_memory_management.py::test_data_transfer
ERROR tests/performance/vulkan/gpu/test_memory_management.py::test_memory_tracking
ERROR tests/performance/vulkan/gpu/test_memory_management.py::test_error_handling
ERROR tests/performance/vulkan/gpu/test_memory_management.py::test_buffer_pool_cleanup
ERROR tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size0-pattern]
ERROR tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size0-flow]
ERROR tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size0-attention]
ERROR tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size1-pattern]
ERROR tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size1-flow]
ERROR tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size1-attention]
ERROR tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size2-pattern]
ERROR tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size2-flow]
ERROR tests/performance/vulkan/test_compute.py::test_shader_compilation[matrix_size2-attention]
ERROR tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size0-workgroup_size0]
ERROR tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size0-workgroup_size1]
ERROR tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size0-workgroup_size2]
ERROR tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size1-workgroup_size0]
ERROR tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size1-workgroup_size1]
ERROR tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size1-workgroup_size2]
ERROR tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size2-workgroup_size0]
ERROR tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size2-workgroup_size1]
ERROR tests/performance/vulkan/test_compute.py::test_workgroup_optimization[matrix_size2-workgroup_size2]
ERROR tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size0-1]
ERROR tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size0-8]
ERROR tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size0-32]
ERROR tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size1-1]
ERROR tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size1-8]
ERROR tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size1-32]
ERROR tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size2-1]
ERROR tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size2-8]
ERROR tests/performance/vulkan/test_compute.py::test_memory_transfer[matrix_size2-32]
ERROR tests/performance/vulkan/test_compute.py::test_resource_management[pattern]
ERROR tests/performance/vulkan/test_compute.py::test_resource_management[flow]
ERROR tests/performance/vulkan/test_compute.py::test_resource_management[attention]
ERROR tests/performance/vulkan/test_compute.py::test_descriptor_set_optimization
ERROR tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_pattern_evolution_performance
ERROR tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_flow_computation_performance
ERROR tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_workgroup_impact
ERROR tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_push_constant_performance
ERROR tests/performance/vulkan/test_shaders.py::TestVulkanShaders::test_batch_processing_efficiency
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_fence_performance
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_fence_performance
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_semaphore_performance
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_semaphore_performance
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_event_performance
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_event_performance
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_barrier_overhead
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_barrier_overhead
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_queue_sync
ERROR tests/performance/vulkan/test_sync.py::TestVulkanSync::test_queue_sync
ERROR tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_connection_vertical_preservation
ERROR tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_connection_horizontal_projection
ERROR tests/test_core/test_patterns/test_fiber_bundle.py::TestPatternFiberBundle::test_connection_levi_civita_compatibility
ERROR tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_stability_computation
ERROR tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_state_evolution
ERROR tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_bifurcation_detection_components
ERROR tests/test_neural/test_attention/test_pattern/test_bifurcation.py::test_convergence_at_bifurcation
===== 159 failed, 218 passed, 29 warnings, 62 errors in 200.13s (0:03:20) ======
