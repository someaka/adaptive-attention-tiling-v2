(.venv) d@d-AX16PRO:~/Desktop/AAT/adaptive-attention-tiling-v2$ python -m pytest tests/performance/cpu/test_vectorization.py tests/performance/cpu/test_memory.py tests/performance/cpu/test_algorithms.py -v --benchmark-only
============================================ test session starts ============================================
platform linux -- Python 3.12.7, pytest-8.3.4, pluggy-1.5.0 -- /home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/python
cachedir: .pytest_cache
benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests
configfile: pytest.ini
plugins: benchmark-5.1.0, hydra-core-1.3.2
collected 0 items / 3 errors                                                                                

================================================== ERRORS ===================================================
_____________________________________ ERROR collecting performance/cpu ______________________________________
.venv/lib/python3.12/site-packages/pluggy/_hooks.py:513: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
        firstresult = False
        kwargs     = {'file_path': PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'),
 'parent': <Dir cpu>,
 'path': local('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')}
        self       = <HookCaller 'pytest_collect_file'>
.venv/lib/python3.12/site-packages/pluggy/_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
        firstresult = False
        hook_name  = 'pytest_collect_file'
        kwargs     = {'file_path': PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'),
 'parent': <Dir cpu>,
 'path': local('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')}
        methods    = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/lib/python3.12/site-packages/_pytest/python.py'>>,
 <HookImpl plugin_name='doctest', plugin=<module '_pytest.doctest' from '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/lib/python3.12/site-packages/_pytest/doctest.py'>>,
 <HookImpl plugin_name='/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/conftest.py', plugin=<module 'conftest' from '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/conftest.py'>>]
        self       = <_pytest.config.PytestPluginManager object at 0x7cbc3aff1be0>
tests/conftest.py:117: in pytest_collect_file
    _run_ruff_commands(ruff_path, file_path)
        content    = ('"""Performance tests for CPU algorithm efficiency.\n'
 '\n'
 'This module tests the algorithmic efficiency of the\n'
 'Adaptive Attention Tiling system, focusing on:\n'
 '1. Fast path optimizations\n'
 '2. Branching efficiency\n'
 '3. Loop optimization\n'
 '4. Numerical stability\n'
 '"""\n'
 '\n'
 'import pytest\n'
 'import torch\n'
 'import numpy as np\n'
 'from typing import Dict, List, Tuple\n'
 'from src.core.performance.cpu.algorithms import AlgorithmOptimizer, '
 'OptimizationMetrics\n'
 '\n'
 '# Test configurations\n'
 'MATRIX_SIZES = [(64, 64), (256, 256), (1024, 1024)]\n'
 'BATCH_SIZES = [1, 16, 64]\n'
 'SPARSITY_LEVELS = [0.1, 0.5, 0.9]\n'
 'OPTIMIZATION_LEVELS = ["O0", "O1", "O2", "O3"]\n'
 '\n'
 '\n'
 '@pytest.fixture\n'
 'def algorithm_optimizer():\n'
 '    """Create an AlgorithmOptimizer instance for testing."""\n'
 '    return AlgorithmOptimizer(enable_profiling=True)\n'
 '\n'
 '\n'
 'def generate_sparse_matrix(size: Tuple[int, int], sparsity: float) -> '
 'torch.Tensor:\n'
 '    """Generate a sparse matrix with given sparsity level."""\n'
 '    matrix = torch.randn(size)\n'
 '    mask = torch.rand(size) > sparsity\n'
 '    return matrix * mask\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("matrix_size", MATRIX_SIZES)\n'
 '@pytest.mark.parametrize("sparsity", SPARSITY_LEVELS)\n'
 'def test_fast_path_optimization(\n'
 '    algorithm_optimizer: AlgorithmOptimizer, matrix_size: Tuple[int, int], '
 'sparsity: float\n'
 '):\n'
 '    """Test fast path optimizations for sparse operations."""\n'
 '    # Generate sparse matrices\n'
 '    matrix_a = generate_sparse_matrix(matrix_size, sparsity)\n'
 '    matrix_b = generate_sparse_matrix(matrix_size, sparsity)\n'
 '\n'
 '    # Baseline computation (dense)\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    baseline_result = torch.matmul(matrix_a, matrix_b)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    baseline_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Optimized computation (sparse)\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    optimized_result = algorithm_optimizer.fast_path_matmul(matrix_a, '
 'matrix_b)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    optimized_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Performance assertions\n'
 '    speedup = baseline_time / optimized_time\n'
 '    expected_speedup = 1 / (1 - sparsity)  # Theoretical speedup\n'
 '    assert speedup > expected_speedup * 0.5  # At least 50% of theoretical\n'
 '\n'
 '    # Accuracy verification\n'
 '    assert torch.allclose(baseline_result, optimized_result, rtol=1e-4)\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("batch_size", BATCH_SIZES)\n'
 '@pytest.mark.parametrize("matrix_size", MATRIX_SIZES)\n'
 'def test_branch_prediction(\n'
 '    algorithm_optimizer: AlgorithmOptimizer, batch_size: int, matrix_size: '
 'Tuple[int, int]\n'
 '):\n'
 '    """Test branch prediction efficiency."""\n'
 '    # Generate test data\n'
 '    matrices = [torch.randn(matrix_size) for _ in range(batch_size)]\n'
 '    thresholds = torch.linspace(-1, 1, batch_size)\n'
 '\n'
 '    def baseline_branch(matrices, thresholds):\n'
 '        results = []\n'
 '        for matrix, threshold in zip(matrices, thresholds):\n'
 '            if torch.mean(matrix) > threshold:\n'
 '                results.append(matrix * 2)\n'
 '            else:\n'
 '                results.append(matrix * 0.5)\n'
 '        return results\n'
 '\n'
 '    # Baseline branching\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    baseline_results = baseline_branch(matrices, thresholds)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    baseline_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Optimized branching\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    optimized_results = algorithm_optimizer.optimized_branch(matrices, '
 'thresholds)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    optimized_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Performance assertions\n'
 '    assert optimized_time < baseline_time * 0.8  # At least 20% faster\n'
 '\n'
 '    # Verify results\n'
 '    for baseline, optimized in zip(baseline_results, optimized_results):\n'
 '        assert torch.allclose(baseline, optimized, rtol=1e-4)\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("optimization_level", OPTIMIZATION_LEVELS)\n'
 'def test_loop_optimization(algorithm_optimizer: AlgorithmOptimizer, '
 'optimization_level: str):\n'
 '    """Test loop optimization strategies."""\n'
 '    size = 1024\n'
 '    iterations = 100\n'
 '    data = torch.randn(size, size)\n'
 '\n'
 '    # Configure optimization level\n'
 '    algorithm_optimizer.set_optimization_level(optimization_level)\n'
 '\n'
 '    # Run optimized computation\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    result = algorithm_optimizer.optimized_loop(data, iterations)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    execution_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    metrics = algorithm_optimizer.get_metrics()\n'
 '\n'
 '    # Performance assertions based on optimization level\n'
 '    if optimization_level == "O0":\n'
 '        assert metrics.loop_unroll_factor == 1\n'
 '    elif optimization_level == "O1":\n'
 '        assert metrics.loop_unroll_factor >= 2\n'
 '    elif optimization_level == "O2":\n'
 '        assert metrics.loop_unroll_factor >= 4\n'
 '        assert metrics.vectorization_enabled\n'
 '    else:  # O3\n'
 '        assert metrics.loop_unroll_factor >= 8\n'
 '        assert metrics.vectorization_enabled\n'
 '        assert metrics.cache_optimization_enabled\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("matrix_size", MATRIX_SIZES)\n'
 'def test_numerical_stability(algorithm_optimizer: AlgorithmOptimizer, '
 'matrix_size: Tuple[int, int]):\n'
 '    """Test numerical stability of optimized computations."""\n'
 '    # Generate test matrices\n'
 '    matrix_a = torch.randn(matrix_size) * 1e6  # Large values\n'
 '    matrix_b = torch.randn(matrix_size) * 1e-6  # Small values\n'
 '\n'
 '    # Standard computation\n'
 '    standard_result = torch.matmul(matrix_a, matrix_b)\n'
 '\n'
 '    # Optimized computation with stability checks\n'
 '    optimized_result = algorithm_optimizer.stable_matmul(matrix_a, '
 'matrix_b)\n'
 '\n'
 '    # Get stability metrics\n'
 '    metrics = algorithm_optimizer.get_metrics()\n'
 '\n'
 '    # Stability assertions\n'
 '    assert metrics.max_relative_error < 1e-5\n'
 '    assert metrics.mean_relative_error < 1e-6\n'
 '    assert not torch.isnan(optimized_result).any()\n'
 '    assert not torch.isinf(optimized_result).any()\n'
 '\n'
 '    # Compare results\n'
 '    assert torch.allclose(standard_result, optimized_result, rtol=1e-4)\n'
 '\n'
 '\n'
 'def test_optimization_overhead(algorithm_optimizer: AlgorithmOptimizer):\n'
 '    """Test overhead of optimization techniques."""\n'
 '    size = 256\n'
 '    data = torch.randn(size, size)\n'
 '\n'
 '    # Baseline computation\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    baseline_result = torch.matmul(data, data)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    baseline_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Optimized computation\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    optimized_result = algorithm_optimizer.optimized_matmul(data, data)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    optimization_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Get overhead metrics\n'
 '    metrics = algorithm_optimizer.get_metrics()\n'
 '\n'
 '    # Overhead assertions\n'
 '    assert metrics.optimization_overhead < baseline_time * 0.1  # Less than '
 '10%\n'
 '    assert optimization_time < baseline_time * 1.1  # Total time within 10%\n'
 '    assert torch.allclose(baseline_result, optimized_result, rtol=1e-4)\n')
        file_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')
        parent     = <Dir cpu>
        root_dir   = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2')
        ruff_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/ruff')
tests/conftest.py:92: in _run_ruff_commands
    pytest.fail(f"Ruff {command_name} failed for {file_path}:\n{result.stderr}")
E   Failed: Ruff check failed for /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py:
        args       = ['check', '--fix', '--unsafe-fixes']
        command_name = 'check'
        commands   = [['check', '--fix', '--unsafe-fixes'], ['format'], ['check']]
        file_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')
        file_str   = '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'
        result     = CompletedProcess(args=['/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/ruff', 'check', '--fix', '--unsafe-fixes', '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'], returncode=1, stdout='tests/performance/cpu/test_algorithms.py:88:9: ANN202 Missing return type annotation for private function `baseline_branch`\n   |\n86 |     thresholds = torch.linspace(-1, 1, batch_size)\n87 | \n88 |     def baseline_branch(matrices, thresholds):\n   |         ^^^^^^^^^^^^^^^ ANN202\n89 |         results = []\n90 |         for matrix, threshold in zip(matrices, thresholds):\n   |\n   = help: Add return type annotation\n\nFound 13 errors (12 fixed, 1 remaining).\n', stderr='')
        ruff_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/ruff')
_____________________________________ ERROR collecting performance/cpu ______________________________________
.venv/lib/python3.12/site-packages/pluggy/_hooks.py:513: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
        firstresult = False
        kwargs     = {'file_path': PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'),
 'parent': <Dir cpu>,
 'path': local('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')}
        self       = <HookCaller 'pytest_collect_file'>
.venv/lib/python3.12/site-packages/pluggy/_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
        firstresult = False
        hook_name  = 'pytest_collect_file'
        kwargs     = {'file_path': PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'),
 'parent': <Dir cpu>,
 'path': local('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')}
        methods    = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/lib/python3.12/site-packages/_pytest/python.py'>>,
 <HookImpl plugin_name='doctest', plugin=<module '_pytest.doctest' from '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/lib/python3.12/site-packages/_pytest/doctest.py'>>,
 <HookImpl plugin_name='/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/conftest.py', plugin=<module 'conftest' from '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/conftest.py'>>]
        self       = <_pytest.config.PytestPluginManager object at 0x7cbc3aff1be0>
tests/conftest.py:117: in pytest_collect_file
    _run_ruff_commands(ruff_path, file_path)
        content    = ('"""Performance tests for CPU algorithm efficiency.\n'
 '\n'
 'This module tests the algorithmic efficiency of the\n'
 'Adaptive Attention Tiling system, focusing on:\n'
 '1. Fast path optimizations\n'
 '2. Branching efficiency\n'
 '3. Loop optimization\n'
 '4. Numerical stability\n'
 '"""\n'
 '\n'
 'import pytest\n'
 'import torch\n'
 '\n'
 'from src.core.performance.cpu.algorithms import AlgorithmOptimizer\n'
 '\n'
 '# Test configurations\n'
 'MATRIX_SIZES = [(64, 64), (256, 256), (1024, 1024)]\n'
 'BATCH_SIZES = [1, 16, 64]\n'
 'SPARSITY_LEVELS = [0.1, 0.5, 0.9]\n'
 'OPTIMIZATION_LEVELS = ["O0", "O1", "O2", "O3"]\n'
 '\n'
 '\n'
 '@pytest.fixture\n'
 'def algorithm_optimizer():\n'
 '    """Create an AlgorithmOptimizer instance for testing."""\n'
 '    return AlgorithmOptimizer(enable_profiling=True)\n'
 '\n'
 '\n'
 'def generate_sparse_matrix(size: tuple[int, int], sparsity: float) -> '
 'torch.Tensor:\n'
 '    """Generate a sparse matrix with given sparsity level."""\n'
 '    matrix = torch.randn(size)\n'
 '    mask = torch.rand(size) > sparsity\n'
 '    return matrix * mask\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("matrix_size", MATRIX_SIZES)\n'
 '@pytest.mark.parametrize("sparsity", SPARSITY_LEVELS)\n'
 'def test_fast_path_optimization(\n'
 '    algorithm_optimizer: AlgorithmOptimizer, matrix_size: tuple[int, int], '
 'sparsity: float\n'
 '):\n'
 '    """Test fast path optimizations for sparse operations."""\n'
 '    # Generate sparse matrices\n'
 '    matrix_a = generate_sparse_matrix(matrix_size, sparsity)\n'
 '    matrix_b = generate_sparse_matrix(matrix_size, sparsity)\n'
 '\n'
 '    # Baseline computation (dense)\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    baseline_result = torch.matmul(matrix_a, matrix_b)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    baseline_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Optimized computation (sparse)\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    optimized_result = algorithm_optimizer.fast_path_matmul(matrix_a, '
 'matrix_b)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    optimized_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Performance assertions\n'
 '    speedup = baseline_time / optimized_time\n'
 '    expected_speedup = 1 / (1 - sparsity)  # Theoretical speedup\n'
 '    assert speedup > expected_speedup * 0.5  # At least 50% of theoretical\n'
 '\n'
 '    # Accuracy verification\n'
 '    assert torch.allclose(baseline_result, optimized_result, rtol=1e-4)\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("batch_size", BATCH_SIZES)\n'
 '@pytest.mark.parametrize("matrix_size", MATRIX_SIZES)\n'
 'def test_branch_prediction(\n'
 '    algorithm_optimizer: AlgorithmOptimizer, batch_size: int, matrix_size: '
 'tuple[int, int]\n'
 '):\n'
 '    """Test branch prediction efficiency."""\n'
 '    # Generate test data\n'
 '    matrices = [torch.randn(matrix_size) for _ in range(batch_size)]\n'
 '    thresholds = torch.linspace(-1, 1, batch_size)\n'
 '\n'
 '    def baseline_branch(matrices, thresholds):\n'
 '        results = []\n'
 '        for matrix, threshold in zip(matrices, thresholds):\n'
 '            if torch.mean(matrix) > threshold:\n'
 '                results.append(matrix * 2)\n'
 '            else:\n'
 '                results.append(matrix * 0.5)\n'
 '        return results\n'
 '\n'
 '    # Baseline branching\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    baseline_results = baseline_branch(matrices, thresholds)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    baseline_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Optimized branching\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    optimized_results = algorithm_optimizer.optimized_branch(matrices, '
 'thresholds)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    optimized_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Performance assertions\n'
 '    assert optimized_time < baseline_time * 0.8  # At least 20% faster\n'
 '\n'
 '    # Verify results\n'
 '    for baseline, optimized in zip(baseline_results, optimized_results):\n'
 '        assert torch.allclose(baseline, optimized, rtol=1e-4)\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("optimization_level", OPTIMIZATION_LEVELS)\n'
 'def test_loop_optimization(algorithm_optimizer: AlgorithmOptimizer, '
 'optimization_level: str):\n'
 '    """Test loop optimization strategies."""\n'
 '    size = 1024\n'
 '    iterations = 100\n'
 '    data = torch.randn(size, size)\n'
 '\n'
 '    # Configure optimization level\n'
 '    algorithm_optimizer.set_optimization_level(optimization_level)\n'
 '\n'
 '    # Run optimized computation\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    algorithm_optimizer.optimized_loop(data, iterations)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    start_time.elapsed_time(end_time)\n'
 '\n'
 '    metrics = algorithm_optimizer.get_metrics()\n'
 '\n'
 '    # Performance assertions based on optimization level\n'
 '    if optimization_level == "O0":\n'
 '        assert metrics.loop_unroll_factor == 1\n'
 '    elif optimization_level == "O1":\n'
 '        assert metrics.loop_unroll_factor >= 2\n'
 '    elif optimization_level == "O2":\n'
 '        assert metrics.loop_unroll_factor >= 4\n'
 '        assert metrics.vectorization_enabled\n'
 '    else:  # O3\n'
 '        assert metrics.loop_unroll_factor >= 8\n'
 '        assert metrics.vectorization_enabled\n'
 '        assert metrics.cache_optimization_enabled\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("matrix_size", MATRIX_SIZES)\n'
 'def test_numerical_stability(algorithm_optimizer: AlgorithmOptimizer, '
 'matrix_size: tuple[int, int]):\n'
 '    """Test numerical stability of optimized computations."""\n'
 '    # Generate test matrices\n'
 '    matrix_a = torch.randn(matrix_size) * 1e6  # Large values\n'
 '    matrix_b = torch.randn(matrix_size) * 1e-6  # Small values\n'
 '\n'
 '    # Standard computation\n'
 '    standard_result = torch.matmul(matrix_a, matrix_b)\n'
 '\n'
 '    # Optimized computation with stability checks\n'
 '    optimized_result = algorithm_optimizer.stable_matmul(matrix_a, '
 'matrix_b)\n'
 '\n'
 '    # Get stability metrics\n'
 '    metrics = algorithm_optimizer.get_metrics()\n'
 '\n'
 '    # Stability assertions\n'
 '    assert metrics.max_relative_error < 1e-5\n'
 '    assert metrics.mean_relative_error < 1e-6\n'
 '    assert not torch.isnan(optimized_result).any()\n'
 '    assert not torch.isinf(optimized_result).any()\n'
 '\n'
 '    # Compare results\n'
 '    assert torch.allclose(standard_result, optimized_result, rtol=1e-4)\n'
 '\n'
 '\n'
 'def test_optimization_overhead(algorithm_optimizer: AlgorithmOptimizer):\n'
 '    """Test overhead of optimization techniques."""\n'
 '    size = 256\n'
 '    data = torch.randn(size, size)\n'
 '\n'
 '    # Baseline computation\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    baseline_result = torch.matmul(data, data)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    baseline_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Optimized computation\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    optimized_result = algorithm_optimizer.optimized_matmul(data, data)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    optimization_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Get overhead metrics\n'
 '    metrics = algorithm_optimizer.get_metrics()\n'
 '\n'
 '    # Overhead assertions\n'
 '    assert metrics.optimization_overhead < baseline_time * 0.1  # Less than '
 '10%\n'
 '    assert optimization_time < baseline_time * 1.1  # Total time within 10%\n'
 '    assert torch.allclose(baseline_result, optimized_result, rtol=1e-4)\n')
        file_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')
        parent     = <Dir cpu>
        root_dir   = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2')
        ruff_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/ruff')
tests/conftest.py:92: in _run_ruff_commands
    pytest.fail(f"Ruff {command_name} failed for {file_path}:\n{result.stderr}")
E   Failed: Ruff check failed for /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py:
        args       = ['check', '--fix', '--unsafe-fixes']
        command_name = 'check'
        commands   = [['check', '--fix', '--unsafe-fixes'], ['format'], ['check']]
        file_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')
        file_str   = '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'
        result     = CompletedProcess(args=['/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/ruff', 'check', '--fix', '--unsafe-fixes', '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'], returncode=1, stdout='tests/performance/cpu/test_algorithms.py:87:9: ANN202 Missing return type annotation for private function `baseline_branch`\n   |\n85 |     thresholds = torch.linspace(-1, 1, batch_size)\n86 | \n87 |     def baseline_branch(matrices, thresholds):\n   |         ^^^^^^^^^^^^^^^ ANN202\n88 |         results = []\n89 |         for matrix, threshold in zip(matrices, thresholds):\n   |\n   = help: Add return type annotation\n\nFound 1 error.\n', stderr='')
        ruff_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/ruff')
_____________________________________ ERROR collecting performance/cpu ______________________________________
.venv/lib/python3.12/site-packages/pluggy/_hooks.py:513: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
        firstresult = False
        kwargs     = {'file_path': PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'),
 'parent': <Dir cpu>,
 'path': local('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')}
        self       = <HookCaller 'pytest_collect_file'>
.venv/lib/python3.12/site-packages/pluggy/_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
        firstresult = False
        hook_name  = 'pytest_collect_file'
        kwargs     = {'file_path': PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'),
 'parent': <Dir cpu>,
 'path': local('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')}
        methods    = [<HookImpl plugin_name='python', plugin=<module '_pytest.python' from '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/lib/python3.12/site-packages/_pytest/python.py'>>,
 <HookImpl plugin_name='doctest', plugin=<module '_pytest.doctest' from '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/lib/python3.12/site-packages/_pytest/doctest.py'>>,
 <HookImpl plugin_name='/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/conftest.py', plugin=<module 'conftest' from '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/conftest.py'>>]
        self       = <_pytest.config.PytestPluginManager object at 0x7cbc3aff1be0>
tests/conftest.py:117: in pytest_collect_file
    _run_ruff_commands(ruff_path, file_path)
        content    = ('"""Performance tests for CPU algorithm efficiency.\n'
 '\n'
 'This module tests the algorithmic efficiency of the\n'
 'Adaptive Attention Tiling system, focusing on:\n'
 '1. Fast path optimizations\n'
 '2. Branching efficiency\n'
 '3. Loop optimization\n'
 '4. Numerical stability\n'
 '"""\n'
 '\n'
 'import pytest\n'
 'import torch\n'
 '\n'
 'from src.core.performance.cpu.algorithms import AlgorithmOptimizer\n'
 '\n'
 '# Test configurations\n'
 'MATRIX_SIZES = [(64, 64), (256, 256), (1024, 1024)]\n'
 'BATCH_SIZES = [1, 16, 64]\n'
 'SPARSITY_LEVELS = [0.1, 0.5, 0.9]\n'
 'OPTIMIZATION_LEVELS = ["O0", "O1", "O2", "O3"]\n'
 '\n'
 '\n'
 '@pytest.fixture\n'
 'def algorithm_optimizer():\n'
 '    """Create an AlgorithmOptimizer instance for testing."""\n'
 '    return AlgorithmOptimizer(enable_profiling=True)\n'
 '\n'
 '\n'
 'def generate_sparse_matrix(size: tuple[int, int], sparsity: float) -> '
 'torch.Tensor:\n'
 '    """Generate a sparse matrix with given sparsity level."""\n'
 '    matrix = torch.randn(size)\n'
 '    mask = torch.rand(size) > sparsity\n'
 '    return matrix * mask\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("matrix_size", MATRIX_SIZES)\n'
 '@pytest.mark.parametrize("sparsity", SPARSITY_LEVELS)\n'
 'def test_fast_path_optimization(\n'
 '    algorithm_optimizer: AlgorithmOptimizer, matrix_size: tuple[int, int], '
 'sparsity: float\n'
 '):\n'
 '    """Test fast path optimizations for sparse operations."""\n'
 '    # Generate sparse matrices\n'
 '    matrix_a = generate_sparse_matrix(matrix_size, sparsity)\n'
 '    matrix_b = generate_sparse_matrix(matrix_size, sparsity)\n'
 '\n'
 '    # Baseline computation (dense)\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    baseline_result = torch.matmul(matrix_a, matrix_b)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    baseline_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Optimized computation (sparse)\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    optimized_result = algorithm_optimizer.fast_path_matmul(matrix_a, '
 'matrix_b)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    optimized_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Performance assertions\n'
 '    speedup = baseline_time / optimized_time\n'
 '    expected_speedup = 1 / (1 - sparsity)  # Theoretical speedup\n'
 '    assert speedup > expected_speedup * 0.5  # At least 50% of theoretical\n'
 '\n'
 '    # Accuracy verification\n'
 '    assert torch.allclose(baseline_result, optimized_result, rtol=1e-4)\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("batch_size", BATCH_SIZES)\n'
 '@pytest.mark.parametrize("matrix_size", MATRIX_SIZES)\n'
 'def test_branch_prediction(\n'
 '    algorithm_optimizer: AlgorithmOptimizer, batch_size: int, matrix_size: '
 'tuple[int, int]\n'
 '):\n'
 '    """Test branch prediction efficiency."""\n'
 '    # Generate test data\n'
 '    matrices = [torch.randn(matrix_size) for _ in range(batch_size)]\n'
 '    thresholds = torch.linspace(-1, 1, batch_size)\n'
 '\n'
 '    def baseline_branch(matrices, thresholds):\n'
 '        results = []\n'
 '        for matrix, threshold in zip(matrices, thresholds):\n'
 '            if torch.mean(matrix) > threshold:\n'
 '                results.append(matrix * 2)\n'
 '            else:\n'
 '                results.append(matrix * 0.5)\n'
 '        return results\n'
 '\n'
 '    # Baseline branching\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    baseline_results = baseline_branch(matrices, thresholds)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    baseline_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Optimized branching\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    optimized_results = algorithm_optimizer.optimized_branch(matrices, '
 'thresholds)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    optimized_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Performance assertions\n'
 '    assert optimized_time < baseline_time * 0.8  # At least 20% faster\n'
 '\n'
 '    # Verify results\n'
 '    for baseline, optimized in zip(baseline_results, optimized_results):\n'
 '        assert torch.allclose(baseline, optimized, rtol=1e-4)\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("optimization_level", OPTIMIZATION_LEVELS)\n'
 'def test_loop_optimization(algorithm_optimizer: AlgorithmOptimizer, '
 'optimization_level: str):\n'
 '    """Test loop optimization strategies."""\n'
 '    size = 1024\n'
 '    iterations = 100\n'
 '    data = torch.randn(size, size)\n'
 '\n'
 '    # Configure optimization level\n'
 '    algorithm_optimizer.set_optimization_level(optimization_level)\n'
 '\n'
 '    # Run optimized computation\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    algorithm_optimizer.optimized_loop(data, iterations)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    start_time.elapsed_time(end_time)\n'
 '\n'
 '    metrics = algorithm_optimizer.get_metrics()\n'
 '\n'
 '    # Performance assertions based on optimization level\n'
 '    if optimization_level == "O0":\n'
 '        assert metrics.loop_unroll_factor == 1\n'
 '    elif optimization_level == "O1":\n'
 '        assert metrics.loop_unroll_factor >= 2\n'
 '    elif optimization_level == "O2":\n'
 '        assert metrics.loop_unroll_factor >= 4\n'
 '        assert metrics.vectorization_enabled\n'
 '    else:  # O3\n'
 '        assert metrics.loop_unroll_factor >= 8\n'
 '        assert metrics.vectorization_enabled\n'
 '        assert metrics.cache_optimization_enabled\n'
 '\n'
 '\n'
 '@pytest.mark.parametrize("matrix_size", MATRIX_SIZES)\n'
 'def test_numerical_stability(algorithm_optimizer: AlgorithmOptimizer, '
 'matrix_size: tuple[int, int]):\n'
 '    """Test numerical stability of optimized computations."""\n'
 '    # Generate test matrices\n'
 '    matrix_a = torch.randn(matrix_size) * 1e6  # Large values\n'
 '    matrix_b = torch.randn(matrix_size) * 1e-6  # Small values\n'
 '\n'
 '    # Standard computation\n'
 '    standard_result = torch.matmul(matrix_a, matrix_b)\n'
 '\n'
 '    # Optimized computation with stability checks\n'
 '    optimized_result = algorithm_optimizer.stable_matmul(matrix_a, '
 'matrix_b)\n'
 '\n'
 '    # Get stability metrics\n'
 '    metrics = algorithm_optimizer.get_metrics()\n'
 '\n'
 '    # Stability assertions\n'
 '    assert metrics.max_relative_error < 1e-5\n'
 '    assert metrics.mean_relative_error < 1e-6\n'
 '    assert not torch.isnan(optimized_result).any()\n'
 '    assert not torch.isinf(optimized_result).any()\n'
 '\n'
 '    # Compare results\n'
 '    assert torch.allclose(standard_result, optimized_result, rtol=1e-4)\n'
 '\n'
 '\n'
 'def test_optimization_overhead(algorithm_optimizer: AlgorithmOptimizer):\n'
 '    """Test overhead of optimization techniques."""\n'
 '    size = 256\n'
 '    data = torch.randn(size, size)\n'
 '\n'
 '    # Baseline computation\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    baseline_result = torch.matmul(data, data)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    baseline_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Optimized computation\n'
 '    start_time = torch.cuda.Event(enable_timing=True)\n'
 '    end_time = torch.cuda.Event(enable_timing=True)\n'
 '\n'
 '    start_time.record()\n'
 '    optimized_result = algorithm_optimizer.optimized_matmul(data, data)\n'
 '    end_time.record()\n'
 '\n'
 '    torch.cuda.synchronize()\n'
 '    optimization_time = start_time.elapsed_time(end_time)\n'
 '\n'
 '    # Get overhead metrics\n'
 '    metrics = algorithm_optimizer.get_metrics()\n'
 '\n'
 '    # Overhead assertions\n'
 '    assert metrics.optimization_overhead < baseline_time * 0.1  # Less than '
 '10%\n'
 '    assert optimization_time < baseline_time * 1.1  # Total time within 10%\n'
 '    assert torch.allclose(baseline_result, optimized_result, rtol=1e-4)\n')
        file_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')
        parent     = <Dir cpu>
        root_dir   = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2')
        ruff_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/ruff')
tests/conftest.py:92: in _run_ruff_commands
    pytest.fail(f"Ruff {command_name} failed for {file_path}:\n{result.stderr}")
E   Failed: Ruff check failed for /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py:
        args       = ['check', '--fix', '--unsafe-fixes']
        command_name = 'check'
        commands   = [['check', '--fix', '--unsafe-fixes'], ['format'], ['check']]
        file_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py')
        file_str   = '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'
        result     = CompletedProcess(args=['/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/ruff', 'check', '--fix', '--unsafe-fixes', '/home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py'], returncode=1, stdout='tests/performance/cpu/test_algorithms.py:87:9: ANN202 Missing return type annotation for private function `baseline_branch`\n   |\n85 |     thresholds = torch.linspace(-1, 1, batch_size)\n86 | \n87 |     def baseline_branch(matrices, thresholds):\n   |         ^^^^^^^^^^^^^^^ ANN202\n88 |         results = []\n89 |         for matrix, threshold in zip(matrices, thresholds):\n   |\n   = help: Add return type annotation\n\nFound 1 error.\n', stderr='')
        ruff_path  = PosixPath('/home/d/Desktop/AAT/adaptive-attention-tiling-v2/.venv/bin/ruff')
========================================== short test summary info ==========================================
ERROR tests/performance/cpu - Failed: Ruff check failed for /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py:
ERROR tests/performance/cpu - Failed: Ruff check failed for /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py:
ERROR tests/performance/cpu - Failed: Ruff check failed for /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py:
============================================= 3 errors in 0.32s =============================================
ERROR: found no collectors for /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_vectorization.py

ERROR: found no collectors for /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_memory.py

ERROR: found no collectors for /home/d/Desktop/AAT/adaptive-attention-tiling-v2/tests/performance/cpu/test_algorithms.py

(.venv) d@d-AX16PRO:~/Desktop/AAT/adaptive-attention-tiling-v2$ 
